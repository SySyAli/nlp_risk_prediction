{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MaxAbsScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\syeda\\OneDrive\\Documents\\American Bureau of Shipping\\projects\\nlp_risk_prediction\\nlp_env\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "c:\\Users\\syeda\\OneDrive\\Documents\\American Bureau of Shipping\\projects\\nlp_risk_prediction\\nlp_env\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Import the excel spreadsheets in the data folder\n",
    "installer_df = pd.read_excel('../data/Installer.xlsx')\n",
    "involver_df = pd.read_excel('../data/Involver.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to merge using a subset of key columns\n",
    "merge_on_columns = ['Site', 'Vessel_Name', 'Wo_No']\n",
    "\n",
    "df = pd.concat([installer_df, involver_df], axis=0)\n",
    "\n",
    "# feature_columns = ['Object', 'Group', 'Symptom', 'Error_Cause', 'Cause_Details', 'Error_Class', 'Discovery', 'Completion_Note', 'Action_Taken', 'Work_Description', 'Directive']\n",
    "feature_columns = ['Object', 'Group', 'Object_Type','Completion_Note', 'Work_Description', 'Directive']\n",
    "# TODO: Change this to ESB1\n",
    "target_column = 'EBS1'\n",
    "\n",
    "# Filter the dataframe for the selected columns\n",
    "df = df[feature_columns + [target_column]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('int64'),\n",
       " array([24,  2, 13,  5, 25, 12, 10,  4, 20,  8, 21, 28, 29,  9, 15, 17,  1,\n",
       "         3, 18, 19, 26, 11,  0,  7, 22, 23, 14,  6, 27, 16]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the target column to ids \n",
    "# Encode the target column\n",
    "label_encoder = LabelEncoder()\n",
    "df[target_column] = label_encoder.fit_transform(df[target_column].astype(str))\n",
    "df[target_column].dtype, df[target_column].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:  (487, 6) Testing samples:  (122, 6)\n",
      "Training labels:  (487,) Testing labels:  (122,)\n",
      "Iteration 1, loss = 3.37101498\n",
      "Iteration 2, loss = 3.34394067\n",
      "Iteration 3, loss = 3.30235817\n",
      "Iteration 4, loss = 3.25058180\n",
      "Iteration 5, loss = 3.19119998\n",
      "Iteration 6, loss = 3.12854828\n",
      "Iteration 7, loss = 3.06195224\n",
      "Iteration 8, loss = 2.99437499\n",
      "Iteration 9, loss = 2.92637402\n",
      "Iteration 10, loss = 2.86053683\n",
      "Iteration 11, loss = 2.79281520\n",
      "Iteration 12, loss = 2.72809051\n",
      "Iteration 13, loss = 2.66570809\n",
      "Iteration 14, loss = 2.60490847\n",
      "Iteration 15, loss = 2.54703678\n",
      "Iteration 16, loss = 2.48991480\n",
      "Iteration 17, loss = 2.43810019\n",
      "Iteration 18, loss = 2.38844163\n",
      "Iteration 19, loss = 2.34179328\n",
      "Iteration 20, loss = 2.29824660\n",
      "Iteration 21, loss = 2.25764258\n",
      "Iteration 22, loss = 2.21908471\n",
      "Iteration 23, loss = 2.18342175\n",
      "Iteration 24, loss = 2.14919852\n",
      "Iteration 25, loss = 2.11675646\n",
      "Iteration 26, loss = 2.08573234\n",
      "Iteration 27, loss = 2.05562661\n",
      "Iteration 28, loss = 2.02680864\n",
      "Iteration 29, loss = 1.99810257\n",
      "Iteration 30, loss = 1.97076744\n",
      "Iteration 31, loss = 1.94390250\n",
      "Iteration 32, loss = 1.91746451\n",
      "Iteration 33, loss = 1.89156915\n",
      "Iteration 34, loss = 1.86596403\n",
      "Iteration 35, loss = 1.84160186\n",
      "Iteration 36, loss = 1.81696537\n",
      "Iteration 37, loss = 1.79314014\n",
      "Iteration 38, loss = 1.76987487\n",
      "Iteration 39, loss = 1.74671852\n",
      "Iteration 40, loss = 1.72436169\n",
      "Iteration 41, loss = 1.70167003\n",
      "Iteration 42, loss = 1.68028906\n",
      "Iteration 43, loss = 1.65856464\n",
      "Iteration 44, loss = 1.63754082\n",
      "Iteration 45, loss = 1.61685324\n",
      "Iteration 46, loss = 1.59706573\n",
      "Iteration 47, loss = 1.57753487\n",
      "Iteration 48, loss = 1.55805343\n",
      "Iteration 49, loss = 1.53968089\n",
      "Iteration 50, loss = 1.52170064\n",
      "Iteration 51, loss = 1.50410413\n",
      "Iteration 52, loss = 1.48710532\n",
      "Iteration 53, loss = 1.47014241\n",
      "Iteration 54, loss = 1.45385378\n",
      "Iteration 55, loss = 1.43809748\n",
      "Iteration 56, loss = 1.42269722\n",
      "Iteration 57, loss = 1.40740765\n",
      "Iteration 58, loss = 1.39301711\n",
      "Iteration 59, loss = 1.37884128\n",
      "Iteration 60, loss = 1.36493554\n",
      "Iteration 61, loss = 1.35146459\n",
      "Iteration 62, loss = 1.33833983\n",
      "Iteration 63, loss = 1.32547369\n",
      "Iteration 64, loss = 1.31308856\n",
      "Iteration 65, loss = 1.30096574\n",
      "Iteration 66, loss = 1.28927333\n",
      "Iteration 67, loss = 1.27780172\n",
      "Iteration 68, loss = 1.26645898\n",
      "Iteration 69, loss = 1.25571986\n",
      "Iteration 70, loss = 1.24503085\n",
      "Iteration 71, loss = 1.23474896\n",
      "Iteration 72, loss = 1.22451298\n",
      "Iteration 73, loss = 1.21475009\n",
      "Iteration 74, loss = 1.20525538\n",
      "Iteration 75, loss = 1.19552809\n",
      "Iteration 76, loss = 1.18645306\n",
      "Iteration 77, loss = 1.17758544\n",
      "Iteration 78, loss = 1.16878539\n",
      "Iteration 79, loss = 1.16005167\n",
      "Iteration 80, loss = 1.15188296\n",
      "Iteration 81, loss = 1.14345930\n",
      "Iteration 82, loss = 1.13549423\n",
      "Iteration 83, loss = 1.12754116\n",
      "Iteration 84, loss = 1.11994973\n",
      "Iteration 85, loss = 1.11222856\n",
      "Iteration 86, loss = 1.10480346\n",
      "Iteration 87, loss = 1.09743399\n",
      "Iteration 88, loss = 1.09024550\n",
      "Iteration 89, loss = 1.08314957\n",
      "Iteration 90, loss = 1.07613992\n",
      "Iteration 91, loss = 1.06935106\n",
      "Iteration 92, loss = 1.06270098\n",
      "Iteration 93, loss = 1.05606524\n",
      "Iteration 94, loss = 1.04957580\n",
      "Iteration 95, loss = 1.04314167\n",
      "Iteration 96, loss = 1.03686481\n",
      "Iteration 97, loss = 1.03055558\n",
      "Iteration 98, loss = 1.02444914\n",
      "Iteration 99, loss = 1.01837941\n",
      "Iteration 100, loss = 1.01255099\n",
      "Iteration 101, loss = 1.00656731\n",
      "Iteration 102, loss = 1.00079368\n",
      "Iteration 103, loss = 0.99518158\n",
      "Iteration 104, loss = 0.98952939\n",
      "Iteration 105, loss = 0.98394047\n",
      "Iteration 106, loss = 0.97843188\n",
      "Iteration 107, loss = 0.97297211\n",
      "Iteration 108, loss = 0.96764933\n",
      "Iteration 109, loss = 0.96234141\n",
      "Iteration 110, loss = 0.95712270\n",
      "Iteration 111, loss = 0.95191577\n",
      "Iteration 112, loss = 0.94695768\n",
      "Iteration 113, loss = 0.94182010\n",
      "Iteration 114, loss = 0.93691969\n",
      "Iteration 115, loss = 0.93196329\n",
      "Iteration 116, loss = 0.92715396\n",
      "Iteration 117, loss = 0.92239647\n",
      "Iteration 118, loss = 0.91769153\n",
      "Iteration 119, loss = 0.91299336\n",
      "Iteration 120, loss = 0.90842429\n",
      "Iteration 121, loss = 0.90380449\n",
      "Iteration 122, loss = 0.89937853\n",
      "Iteration 123, loss = 0.89477136\n",
      "Iteration 124, loss = 0.89051748\n",
      "Iteration 125, loss = 0.88602371\n",
      "Iteration 126, loss = 0.88170430\n",
      "Iteration 127, loss = 0.87755185\n",
      "Iteration 128, loss = 0.87330939\n",
      "Iteration 129, loss = 0.86915379\n",
      "Iteration 130, loss = 0.86513412\n",
      "Iteration 131, loss = 0.86096841\n",
      "Iteration 132, loss = 0.85702881\n",
      "Iteration 133, loss = 0.85305132\n",
      "Iteration 134, loss = 0.84917919\n",
      "Iteration 135, loss = 0.84526804\n",
      "Iteration 136, loss = 0.84143025\n",
      "Iteration 137, loss = 0.83764409\n",
      "Iteration 138, loss = 0.83389766\n",
      "Iteration 139, loss = 0.83009805\n",
      "Iteration 140, loss = 0.82643082\n",
      "Iteration 141, loss = 0.82276400\n",
      "Iteration 142, loss = 0.81916569\n",
      "Iteration 143, loss = 0.81552456\n",
      "Iteration 144, loss = 0.81199625\n",
      "Iteration 145, loss = 0.80852384\n",
      "Iteration 146, loss = 0.80488690\n",
      "Iteration 147, loss = 0.80146314\n",
      "Iteration 148, loss = 0.79801539\n",
      "Iteration 149, loss = 0.79464839\n",
      "Iteration 150, loss = 0.79129925\n",
      "Iteration 151, loss = 0.78794394\n",
      "Iteration 152, loss = 0.78473484\n",
      "Iteration 153, loss = 0.78134321\n",
      "Iteration 154, loss = 0.77821702\n",
      "Iteration 155, loss = 0.77495481\n",
      "Iteration 156, loss = 0.77180746\n",
      "Iteration 157, loss = 0.76857568\n",
      "Iteration 158, loss = 0.76549568\n",
      "Iteration 159, loss = 0.76244506\n",
      "Iteration 160, loss = 0.75927926\n",
      "Iteration 161, loss = 0.75618676\n",
      "Iteration 162, loss = 0.75314323\n",
      "Iteration 163, loss = 0.75016520\n",
      "Iteration 164, loss = 0.74709446\n",
      "Iteration 165, loss = 0.74416299\n",
      "Iteration 166, loss = 0.74122551\n",
      "Iteration 167, loss = 0.73826687\n",
      "Iteration 168, loss = 0.73534387\n",
      "Iteration 169, loss = 0.73249171\n",
      "Iteration 170, loss = 0.72954195\n",
      "Iteration 171, loss = 0.72676627\n",
      "Iteration 172, loss = 0.72386356\n",
      "Iteration 173, loss = 0.72112054\n",
      "Iteration 174, loss = 0.71835328\n",
      "Iteration 175, loss = 0.71559364\n",
      "Iteration 176, loss = 0.71287722\n",
      "Iteration 177, loss = 0.71022580\n",
      "Iteration 178, loss = 0.70747165\n",
      "Iteration 179, loss = 0.70480587\n",
      "Iteration 180, loss = 0.70213730\n",
      "Iteration 181, loss = 0.69945686\n",
      "Iteration 182, loss = 0.69684664\n",
      "Iteration 183, loss = 0.69419848\n",
      "Iteration 184, loss = 0.69155977\n",
      "Iteration 185, loss = 0.68901200\n",
      "Iteration 186, loss = 0.68642254\n",
      "Iteration 187, loss = 0.68389265\n",
      "Iteration 188, loss = 0.68134254\n",
      "Iteration 189, loss = 0.67876451\n",
      "Iteration 190, loss = 0.67623793\n",
      "Iteration 191, loss = 0.67374264\n",
      "Iteration 192, loss = 0.67123052\n",
      "Iteration 193, loss = 0.66870785\n",
      "Iteration 194, loss = 0.66620501\n",
      "Iteration 195, loss = 0.66380071\n",
      "Iteration 196, loss = 0.66131318\n",
      "Iteration 197, loss = 0.65891492\n",
      "Iteration 198, loss = 0.65646337\n",
      "Iteration 199, loss = 0.65404348\n",
      "Iteration 200, loss = 0.65168325\n",
      "Iteration 201, loss = 0.64927934\n",
      "Iteration 202, loss = 0.64690706\n",
      "Iteration 203, loss = 0.64455469\n",
      "Iteration 204, loss = 0.64218852\n",
      "Iteration 205, loss = 0.63992248\n",
      "Iteration 206, loss = 0.63758817\n",
      "Iteration 207, loss = 0.63535606\n",
      "Iteration 208, loss = 0.63303521\n",
      "Iteration 209, loss = 0.63083000\n",
      "Iteration 210, loss = 0.62853650\n",
      "Iteration 211, loss = 0.62634994\n",
      "Iteration 212, loss = 0.62407677\n",
      "Iteration 213, loss = 0.62185639\n",
      "Iteration 214, loss = 0.61966464\n",
      "Iteration 215, loss = 0.61745330\n",
      "Iteration 216, loss = 0.61528762\n",
      "Iteration 217, loss = 0.61312545\n",
      "Iteration 218, loss = 0.61095937\n",
      "Iteration 219, loss = 0.60881095\n",
      "Iteration 220, loss = 0.60666456\n",
      "Iteration 221, loss = 0.60450687\n",
      "Iteration 222, loss = 0.60242425\n",
      "Iteration 223, loss = 0.60031589\n",
      "Iteration 224, loss = 0.59821123\n",
      "Iteration 225, loss = 0.59610278\n",
      "Iteration 226, loss = 0.59411336\n",
      "Iteration 227, loss = 0.59199626\n",
      "Iteration 228, loss = 0.58994672\n",
      "Iteration 229, loss = 0.58794076\n",
      "Iteration 230, loss = 0.58594375\n",
      "Iteration 231, loss = 0.58386437\n",
      "Iteration 232, loss = 0.58188950\n",
      "Iteration 233, loss = 0.57988404\n",
      "Iteration 234, loss = 0.57787834\n",
      "Iteration 235, loss = 0.57590847\n",
      "Iteration 236, loss = 0.57396154\n",
      "Iteration 237, loss = 0.57198906\n",
      "Iteration 238, loss = 0.57005841\n",
      "Iteration 239, loss = 0.56813053\n",
      "Iteration 240, loss = 0.56614831\n",
      "Iteration 241, loss = 0.56430330\n",
      "Iteration 242, loss = 0.56239003\n",
      "Iteration 243, loss = 0.56048577\n",
      "Iteration 244, loss = 0.55865593\n",
      "Iteration 245, loss = 0.55678524\n",
      "Iteration 246, loss = 0.55492501\n",
      "Iteration 247, loss = 0.55310278\n",
      "Iteration 248, loss = 0.55125955\n",
      "Iteration 249, loss = 0.54944430\n",
      "Iteration 250, loss = 0.54758850\n",
      "Iteration 251, loss = 0.54580070\n",
      "Iteration 252, loss = 0.54402004\n",
      "Iteration 253, loss = 0.54218606\n",
      "Iteration 254, loss = 0.54038322\n",
      "Iteration 255, loss = 0.53864243\n",
      "Iteration 256, loss = 0.53683158\n",
      "Iteration 257, loss = 0.53508768\n",
      "Iteration 258, loss = 0.53336443\n",
      "Iteration 259, loss = 0.53159150\n",
      "Iteration 260, loss = 0.52986591\n",
      "Iteration 261, loss = 0.52809485\n",
      "Iteration 262, loss = 0.52641919\n",
      "Iteration 263, loss = 0.52467834\n",
      "Iteration 264, loss = 0.52297372\n",
      "Iteration 265, loss = 0.52124121\n",
      "Iteration 266, loss = 0.51956038\n",
      "Iteration 267, loss = 0.51789791\n",
      "Iteration 268, loss = 0.51620083\n",
      "Iteration 269, loss = 0.51447182\n",
      "Iteration 270, loss = 0.51277506\n",
      "Iteration 271, loss = 0.51109135\n",
      "Iteration 272, loss = 0.50943740\n",
      "Iteration 273, loss = 0.50771872\n",
      "Iteration 274, loss = 0.50607433\n",
      "Iteration 275, loss = 0.50437719\n",
      "Iteration 276, loss = 0.50278310\n",
      "Iteration 277, loss = 0.50111430\n",
      "Iteration 278, loss = 0.49948898\n",
      "Iteration 279, loss = 0.49788717\n",
      "Iteration 280, loss = 0.49629185\n",
      "Iteration 281, loss = 0.49467143\n",
      "Iteration 282, loss = 0.49308114\n",
      "Iteration 283, loss = 0.49148873\n",
      "Iteration 284, loss = 0.48995628\n",
      "Iteration 285, loss = 0.48833009\n",
      "Iteration 286, loss = 0.48678581\n",
      "Iteration 287, loss = 0.48526296\n",
      "Iteration 288, loss = 0.48372203\n",
      "Iteration 289, loss = 0.48217894\n",
      "Iteration 290, loss = 0.48068225\n",
      "Iteration 291, loss = 0.47913074\n",
      "Iteration 292, loss = 0.47769485\n",
      "Iteration 293, loss = 0.47616094\n",
      "Iteration 294, loss = 0.47466412\n",
      "Iteration 295, loss = 0.47323143\n",
      "Iteration 296, loss = 0.47176906\n",
      "Iteration 297, loss = 0.47027672\n",
      "Iteration 298, loss = 0.46881542\n",
      "Iteration 299, loss = 0.46735902\n",
      "Iteration 300, loss = 0.46592920\n",
      "Iteration 301, loss = 0.46443829\n",
      "Iteration 302, loss = 0.46297985\n",
      "Iteration 303, loss = 0.46159061\n",
      "Iteration 304, loss = 0.46011451\n",
      "Iteration 305, loss = 0.45874267\n",
      "Iteration 306, loss = 0.45733363\n",
      "Iteration 307, loss = 0.45591525\n",
      "Iteration 308, loss = 0.45458051\n",
      "Iteration 309, loss = 0.45319757\n",
      "Iteration 310, loss = 0.45182779\n",
      "Iteration 311, loss = 0.45051176\n",
      "Iteration 312, loss = 0.44910430\n",
      "Iteration 313, loss = 0.44775826\n",
      "Iteration 314, loss = 0.44644126\n",
      "Iteration 315, loss = 0.44505838\n",
      "Iteration 316, loss = 0.44376645\n",
      "Iteration 317, loss = 0.44244545\n",
      "Iteration 318, loss = 0.44110525\n",
      "Iteration 319, loss = 0.43978003\n",
      "Iteration 320, loss = 0.43846403\n",
      "Iteration 321, loss = 0.43717761\n",
      "Iteration 322, loss = 0.43588703\n",
      "Iteration 323, loss = 0.43455894\n",
      "Iteration 324, loss = 0.43325460\n",
      "Iteration 325, loss = 0.43199683\n",
      "Iteration 326, loss = 0.43071113\n",
      "Iteration 327, loss = 0.42940813\n",
      "Iteration 328, loss = 0.42810838\n",
      "Iteration 329, loss = 0.42686176\n",
      "Iteration 330, loss = 0.42556631\n",
      "Iteration 331, loss = 0.42429402\n",
      "Iteration 332, loss = 0.42304520\n",
      "Iteration 333, loss = 0.42179254\n",
      "Iteration 334, loss = 0.42053736\n",
      "Iteration 335, loss = 0.41934101\n",
      "Iteration 336, loss = 0.41808769\n",
      "Iteration 337, loss = 0.41686074\n",
      "Iteration 338, loss = 0.41564834\n",
      "Iteration 339, loss = 0.41441389\n",
      "Iteration 340, loss = 0.41319381\n",
      "Iteration 341, loss = 0.41199676\n",
      "Iteration 342, loss = 0.41080061\n",
      "Iteration 343, loss = 0.40960387\n",
      "Iteration 344, loss = 0.40840461\n",
      "Iteration 345, loss = 0.40720656\n",
      "Iteration 346, loss = 0.40602831\n",
      "Iteration 347, loss = 0.40487408\n",
      "Iteration 348, loss = 0.40367804\n",
      "Iteration 349, loss = 0.40253477\n",
      "Iteration 350, loss = 0.40134377\n",
      "Iteration 351, loss = 0.40019644\n",
      "Iteration 352, loss = 0.39908714\n",
      "Iteration 353, loss = 0.39792927\n",
      "Iteration 354, loss = 0.39677291\n",
      "Iteration 355, loss = 0.39565520\n",
      "Iteration 356, loss = 0.39454140\n",
      "Iteration 357, loss = 0.39336224\n",
      "Iteration 358, loss = 0.39226532\n",
      "Iteration 359, loss = 0.39114604\n",
      "Iteration 360, loss = 0.39002261\n",
      "Iteration 361, loss = 0.38890913\n",
      "Iteration 362, loss = 0.38780869\n",
      "Iteration 363, loss = 0.38669726\n",
      "Iteration 364, loss = 0.38563185\n",
      "Iteration 365, loss = 0.38456434\n",
      "Iteration 366, loss = 0.38344345\n",
      "Iteration 367, loss = 0.38240171\n",
      "Iteration 368, loss = 0.38133420\n",
      "Iteration 369, loss = 0.38023704\n",
      "Iteration 370, loss = 0.37917909\n",
      "Iteration 371, loss = 0.37812511\n",
      "Iteration 372, loss = 0.37706656\n",
      "Iteration 373, loss = 0.37602960\n",
      "Iteration 374, loss = 0.37497299\n",
      "Iteration 375, loss = 0.37390511\n",
      "Iteration 376, loss = 0.37288190\n",
      "Iteration 377, loss = 0.37182519\n",
      "Iteration 378, loss = 0.37079474\n",
      "Iteration 379, loss = 0.36977752\n",
      "Iteration 380, loss = 0.36878175\n",
      "Iteration 381, loss = 0.36773208\n",
      "Iteration 382, loss = 0.36673290\n",
      "Iteration 383, loss = 0.36572050\n",
      "Iteration 384, loss = 0.36471154\n",
      "Iteration 385, loss = 0.36370396\n",
      "Iteration 386, loss = 0.36270920\n",
      "Iteration 387, loss = 0.36172973\n",
      "Iteration 388, loss = 0.36074780\n",
      "Iteration 389, loss = 0.35975602\n",
      "Iteration 390, loss = 0.35880959\n",
      "Iteration 391, loss = 0.35782210\n",
      "Iteration 392, loss = 0.35686596\n",
      "Iteration 393, loss = 0.35589944\n",
      "Iteration 394, loss = 0.35493460\n",
      "Iteration 395, loss = 0.35399381\n",
      "Iteration 396, loss = 0.35304885\n",
      "Iteration 397, loss = 0.35206721\n",
      "Iteration 398, loss = 0.35113322\n",
      "Iteration 399, loss = 0.35021922\n",
      "Iteration 400, loss = 0.34929514\n",
      "Iteration 401, loss = 0.34834096\n",
      "Iteration 402, loss = 0.34738279\n",
      "Iteration 403, loss = 0.34650580\n",
      "Iteration 404, loss = 0.34552498\n",
      "Iteration 405, loss = 0.34460533\n",
      "Iteration 406, loss = 0.34373017\n",
      "Iteration 407, loss = 0.34281044\n",
      "Iteration 408, loss = 0.34192026\n",
      "Iteration 409, loss = 0.34101824\n",
      "Iteration 410, loss = 0.34009912\n",
      "Iteration 411, loss = 0.33925212\n",
      "Iteration 412, loss = 0.33832965\n",
      "Iteration 413, loss = 0.33745460\n",
      "Iteration 414, loss = 0.33656630\n",
      "Iteration 415, loss = 0.33569796\n",
      "Iteration 416, loss = 0.33478860\n",
      "Iteration 417, loss = 0.33393137\n",
      "Iteration 418, loss = 0.33306928\n",
      "Iteration 419, loss = 0.33217323\n",
      "Iteration 420, loss = 0.33130784\n",
      "Iteration 421, loss = 0.33045985\n",
      "Iteration 422, loss = 0.32961254\n",
      "Iteration 423, loss = 0.32875313\n",
      "Iteration 424, loss = 0.32790175\n",
      "Iteration 425, loss = 0.32705762\n",
      "Iteration 426, loss = 0.32621084\n",
      "Iteration 427, loss = 0.32538407\n",
      "Iteration 428, loss = 0.32452192\n",
      "Iteration 429, loss = 0.32369751\n",
      "Iteration 430, loss = 0.32286862\n",
      "Iteration 431, loss = 0.32201827\n",
      "Iteration 432, loss = 0.32122042\n",
      "Iteration 433, loss = 0.32039921\n",
      "Iteration 434, loss = 0.31957814\n",
      "Iteration 435, loss = 0.31880032\n",
      "Iteration 436, loss = 0.31797591\n",
      "Iteration 437, loss = 0.31718587\n",
      "Iteration 438, loss = 0.31636804\n",
      "Iteration 439, loss = 0.31559567\n",
      "Iteration 440, loss = 0.31480570\n",
      "Iteration 441, loss = 0.31401838\n",
      "Iteration 442, loss = 0.31327101\n",
      "Iteration 443, loss = 0.31248601\n",
      "Iteration 444, loss = 0.31167861\n",
      "Iteration 445, loss = 0.31090228\n",
      "Iteration 446, loss = 0.31014125\n",
      "Iteration 447, loss = 0.30939522\n",
      "Iteration 448, loss = 0.30860140\n",
      "Iteration 449, loss = 0.30784624\n",
      "Iteration 450, loss = 0.30706685\n",
      "Iteration 451, loss = 0.30632523\n",
      "Iteration 452, loss = 0.30555863\n",
      "Iteration 453, loss = 0.30479928\n",
      "Iteration 454, loss = 0.30404028\n",
      "Iteration 455, loss = 0.30327510\n",
      "Iteration 456, loss = 0.30254263\n",
      "Iteration 457, loss = 0.30179000\n",
      "Iteration 458, loss = 0.30106975\n",
      "Iteration 459, loss = 0.30032944\n",
      "Iteration 460, loss = 0.29961052\n",
      "Iteration 461, loss = 0.29888715\n",
      "Iteration 462, loss = 0.29816703\n",
      "Iteration 463, loss = 0.29746509\n",
      "Iteration 464, loss = 0.29672775\n",
      "Iteration 465, loss = 0.29603289\n",
      "Iteration 466, loss = 0.29529999\n",
      "Iteration 467, loss = 0.29458084\n",
      "Iteration 468, loss = 0.29389144\n",
      "Iteration 469, loss = 0.29316409\n",
      "Iteration 470, loss = 0.29246270\n",
      "Iteration 471, loss = 0.29175716\n",
      "Iteration 472, loss = 0.29106223\n",
      "Iteration 473, loss = 0.29035654\n",
      "Iteration 474, loss = 0.28964959\n",
      "Iteration 475, loss = 0.28897844\n",
      "Iteration 476, loss = 0.28828493\n",
      "Iteration 477, loss = 0.28756550\n",
      "Iteration 478, loss = 0.28689088\n",
      "Iteration 479, loss = 0.28620215\n",
      "Iteration 480, loss = 0.28549365\n",
      "Iteration 481, loss = 0.28484454\n",
      "Iteration 482, loss = 0.28411779\n",
      "Iteration 483, loss = 0.28345780\n",
      "Iteration 484, loss = 0.28279666\n",
      "Iteration 485, loss = 0.28212034\n",
      "Iteration 486, loss = 0.28146365\n",
      "Iteration 487, loss = 0.28079259\n",
      "Iteration 488, loss = 0.28012200\n",
      "Iteration 489, loss = 0.27946396\n",
      "Iteration 490, loss = 0.27880198\n",
      "Iteration 491, loss = 0.27813114\n",
      "Iteration 492, loss = 0.27749828\n",
      "Iteration 493, loss = 0.27682690\n",
      "Iteration 494, loss = 0.27618286\n",
      "Iteration 495, loss = 0.27553680\n",
      "Iteration 496, loss = 0.27489763\n",
      "Iteration 497, loss = 0.27427182\n",
      "Iteration 498, loss = 0.27362700\n",
      "Iteration 499, loss = 0.27299990\n",
      "Iteration 500, loss = 0.27235955\n",
      "Iteration 501, loss = 0.27173936\n",
      "Iteration 502, loss = 0.27112105\n",
      "Iteration 503, loss = 0.27047544\n",
      "Iteration 504, loss = 0.26984985\n",
      "Iteration 505, loss = 0.26922290\n",
      "Iteration 506, loss = 0.26860243\n",
      "Iteration 507, loss = 0.26798155\n",
      "Iteration 508, loss = 0.26736870\n",
      "Iteration 509, loss = 0.26675541\n",
      "Iteration 510, loss = 0.26615831\n",
      "Iteration 511, loss = 0.26554753\n",
      "Iteration 512, loss = 0.26497317\n",
      "Iteration 513, loss = 0.26435638\n",
      "Iteration 514, loss = 0.26375986\n",
      "Iteration 515, loss = 0.26316731\n",
      "Iteration 516, loss = 0.26258844\n",
      "Iteration 517, loss = 0.26198340\n",
      "Iteration 518, loss = 0.26140259\n",
      "Iteration 519, loss = 0.26080095\n",
      "Iteration 520, loss = 0.26024229\n",
      "Iteration 521, loss = 0.25963988\n",
      "Iteration 522, loss = 0.25906151\n",
      "Iteration 523, loss = 0.25845654\n",
      "Iteration 524, loss = 0.25787163\n",
      "Iteration 525, loss = 0.25727966\n",
      "Iteration 526, loss = 0.25669707\n",
      "Iteration 527, loss = 0.25612014\n",
      "Iteration 528, loss = 0.25553729\n",
      "Iteration 529, loss = 0.25496787\n",
      "Iteration 530, loss = 0.25439135\n",
      "Iteration 531, loss = 0.25381703\n",
      "Iteration 532, loss = 0.25325734\n",
      "Iteration 533, loss = 0.25269093\n",
      "Iteration 534, loss = 0.25211752\n",
      "Iteration 535, loss = 0.25156745\n",
      "Iteration 536, loss = 0.25098737\n",
      "Iteration 537, loss = 0.25042620\n",
      "Iteration 538, loss = 0.24988585\n",
      "Iteration 539, loss = 0.24931824\n",
      "Iteration 540, loss = 0.24879118\n",
      "Iteration 541, loss = 0.24822054\n",
      "Iteration 542, loss = 0.24767949\n",
      "Iteration 543, loss = 0.24714940\n",
      "Iteration 544, loss = 0.24660131\n",
      "Iteration 545, loss = 0.24607523\n",
      "Iteration 546, loss = 0.24552949\n",
      "Iteration 547, loss = 0.24498961\n",
      "Iteration 548, loss = 0.24447047\n",
      "Iteration 549, loss = 0.24391616\n",
      "Iteration 550, loss = 0.24338960\n",
      "Iteration 551, loss = 0.24282669\n",
      "Iteration 552, loss = 0.24231425\n",
      "Iteration 553, loss = 0.24176904\n",
      "Iteration 554, loss = 0.24125247\n",
      "Iteration 555, loss = 0.24072362\n",
      "Iteration 556, loss = 0.24019575\n",
      "Iteration 557, loss = 0.23966780\n",
      "Iteration 558, loss = 0.23913644\n",
      "Iteration 559, loss = 0.23862926\n",
      "Iteration 560, loss = 0.23809882\n",
      "Iteration 561, loss = 0.23757959\n",
      "Iteration 562, loss = 0.23709839\n",
      "Iteration 563, loss = 0.23656621\n",
      "Iteration 564, loss = 0.23607548\n",
      "Iteration 565, loss = 0.23557195\n",
      "Iteration 566, loss = 0.23505089\n",
      "Iteration 567, loss = 0.23456465\n",
      "Iteration 568, loss = 0.23407581\n",
      "Iteration 569, loss = 0.23357834\n",
      "Iteration 570, loss = 0.23311363\n",
      "Iteration 571, loss = 0.23260989\n",
      "Iteration 572, loss = 0.23212859\n",
      "Iteration 573, loss = 0.23162853\n",
      "Iteration 574, loss = 0.23113885\n",
      "Iteration 575, loss = 0.23066167\n",
      "Iteration 576, loss = 0.23016676\n",
      "Iteration 577, loss = 0.22968993\n",
      "Iteration 578, loss = 0.22919437\n",
      "Iteration 579, loss = 0.22872187\n",
      "Iteration 580, loss = 0.22825744\n",
      "Iteration 581, loss = 0.22775808\n",
      "Iteration 582, loss = 0.22729086\n",
      "Iteration 583, loss = 0.22680124\n",
      "Iteration 584, loss = 0.22631549\n",
      "Iteration 585, loss = 0.22582980\n",
      "Iteration 586, loss = 0.22535937\n",
      "Iteration 587, loss = 0.22488386\n",
      "Iteration 588, loss = 0.22439552\n",
      "Iteration 589, loss = 0.22393658\n",
      "Iteration 590, loss = 0.22345561\n",
      "Iteration 591, loss = 0.22299823\n",
      "Iteration 592, loss = 0.22254308\n",
      "Iteration 593, loss = 0.22206281\n",
      "Iteration 594, loss = 0.22161161\n",
      "Iteration 595, loss = 0.22114785\n",
      "Iteration 596, loss = 0.22068278\n",
      "Iteration 597, loss = 0.22023133\n",
      "Iteration 598, loss = 0.21977318\n",
      "Iteration 599, loss = 0.21931131\n",
      "Iteration 600, loss = 0.21887211\n",
      "Iteration 601, loss = 0.21841995\n",
      "Iteration 602, loss = 0.21797803\n",
      "Iteration 603, loss = 0.21752171\n",
      "Iteration 604, loss = 0.21707114\n",
      "Iteration 605, loss = 0.21663773\n",
      "Iteration 606, loss = 0.21617349\n",
      "Iteration 607, loss = 0.21576071\n",
      "Iteration 608, loss = 0.21528672\n",
      "Iteration 609, loss = 0.21484963\n",
      "Iteration 610, loss = 0.21439523\n",
      "Iteration 611, loss = 0.21394912\n",
      "Iteration 612, loss = 0.21350812\n",
      "Iteration 613, loss = 0.21305690\n",
      "Iteration 614, loss = 0.21261900\n",
      "Iteration 615, loss = 0.21216093\n",
      "Iteration 616, loss = 0.21174317\n",
      "Iteration 617, loss = 0.21129161\n",
      "Iteration 618, loss = 0.21086444\n",
      "Iteration 619, loss = 0.21042748\n",
      "Iteration 620, loss = 0.20998975\n",
      "Iteration 621, loss = 0.20955956\n",
      "Iteration 622, loss = 0.20912168\n",
      "Iteration 623, loss = 0.20870584\n",
      "Iteration 624, loss = 0.20826705\n",
      "Iteration 625, loss = 0.20784437\n",
      "Iteration 626, loss = 0.20742027\n",
      "Iteration 627, loss = 0.20699771\n",
      "Iteration 628, loss = 0.20658193\n",
      "Iteration 629, loss = 0.20614656\n",
      "Iteration 630, loss = 0.20574414\n",
      "Iteration 631, loss = 0.20533697\n",
      "Iteration 632, loss = 0.20492776\n",
      "Iteration 633, loss = 0.20451579\n",
      "Iteration 634, loss = 0.20410766\n",
      "Iteration 635, loss = 0.20369604\n",
      "Iteration 636, loss = 0.20328214\n",
      "Iteration 637, loss = 0.20287789\n",
      "Iteration 638, loss = 0.20247303\n",
      "Iteration 639, loss = 0.20205494\n",
      "Iteration 640, loss = 0.20164221\n",
      "Iteration 641, loss = 0.20124295\n",
      "Iteration 642, loss = 0.20086857\n",
      "Iteration 643, loss = 0.20045119\n",
      "Iteration 644, loss = 0.20006495\n",
      "Iteration 645, loss = 0.19965566\n",
      "Iteration 646, loss = 0.19926637\n",
      "Iteration 647, loss = 0.19886843\n",
      "Iteration 648, loss = 0.19847756\n",
      "Iteration 649, loss = 0.19808034\n",
      "Iteration 650, loss = 0.19768315\n",
      "Iteration 651, loss = 0.19729761\n",
      "Iteration 652, loss = 0.19691028\n",
      "Iteration 653, loss = 0.19652066\n",
      "Iteration 654, loss = 0.19612264\n",
      "Iteration 655, loss = 0.19573950\n",
      "Iteration 656, loss = 0.19536114\n",
      "Iteration 657, loss = 0.19496388\n",
      "Iteration 658, loss = 0.19458509\n",
      "Iteration 659, loss = 0.19419533\n",
      "Iteration 660, loss = 0.19382284\n",
      "Iteration 661, loss = 0.19345056\n",
      "Iteration 662, loss = 0.19307426\n",
      "Iteration 663, loss = 0.19267518\n",
      "Iteration 664, loss = 0.19230701\n",
      "Iteration 665, loss = 0.19190823\n",
      "Iteration 666, loss = 0.19152694\n",
      "Iteration 667, loss = 0.19112640\n",
      "Iteration 668, loss = 0.19075121\n",
      "Iteration 669, loss = 0.19036702\n",
      "Iteration 670, loss = 0.18998892\n",
      "Iteration 671, loss = 0.18961486\n",
      "Iteration 672, loss = 0.18922291\n",
      "Iteration 673, loss = 0.18887232\n",
      "Iteration 674, loss = 0.18849974\n",
      "Iteration 675, loss = 0.18813768\n",
      "Iteration 676, loss = 0.18776968\n",
      "Iteration 677, loss = 0.18740954\n",
      "Iteration 678, loss = 0.18703642\n",
      "Iteration 679, loss = 0.18667935\n",
      "Iteration 680, loss = 0.18632878\n",
      "Iteration 681, loss = 0.18596113\n",
      "Iteration 682, loss = 0.18560032\n",
      "Iteration 683, loss = 0.18525902\n",
      "Iteration 684, loss = 0.18490061\n",
      "Iteration 685, loss = 0.18455629\n",
      "Iteration 686, loss = 0.18420358\n",
      "Iteration 687, loss = 0.18385918\n",
      "Iteration 688, loss = 0.18351856\n",
      "Iteration 689, loss = 0.18317361\n",
      "Iteration 690, loss = 0.18282148\n",
      "Iteration 691, loss = 0.18248606\n",
      "Iteration 692, loss = 0.18213248\n",
      "Iteration 693, loss = 0.18179729\n",
      "Iteration 694, loss = 0.18144405\n",
      "Iteration 695, loss = 0.18110075\n",
      "Iteration 696, loss = 0.18074865\n",
      "Iteration 697, loss = 0.18040256\n",
      "Iteration 698, loss = 0.18006112\n",
      "Iteration 699, loss = 0.17971629\n",
      "Iteration 700, loss = 0.17938162\n",
      "Iteration 701, loss = 0.17903713\n",
      "Iteration 702, loss = 0.17870385\n",
      "Iteration 703, loss = 0.17836194\n",
      "Iteration 704, loss = 0.17802057\n",
      "Iteration 705, loss = 0.17769473\n",
      "Iteration 706, loss = 0.17737055\n",
      "Iteration 707, loss = 0.17702677\n",
      "Iteration 708, loss = 0.17669450\n",
      "Iteration 709, loss = 0.17636857\n",
      "Iteration 710, loss = 0.17603614\n",
      "Iteration 711, loss = 0.17571155\n",
      "Iteration 712, loss = 0.17538570\n",
      "Iteration 713, loss = 0.17505912\n",
      "Iteration 714, loss = 0.17474853\n",
      "Iteration 715, loss = 0.17439495\n",
      "Iteration 716, loss = 0.17406775\n",
      "Iteration 717, loss = 0.17374685\n",
      "Iteration 718, loss = 0.17341618\n",
      "Iteration 719, loss = 0.17309601\n",
      "Iteration 720, loss = 0.17277904\n",
      "Iteration 721, loss = 0.17244630\n",
      "Iteration 722, loss = 0.17211982\n",
      "Iteration 723, loss = 0.17180313\n",
      "Iteration 724, loss = 0.17147369\n",
      "Iteration 725, loss = 0.17115979\n",
      "Iteration 726, loss = 0.17084187\n",
      "Iteration 727, loss = 0.17051259\n",
      "Iteration 728, loss = 0.17021436\n",
      "Iteration 729, loss = 0.16988940\n",
      "Iteration 730, loss = 0.16956998\n",
      "Iteration 731, loss = 0.16925395\n",
      "Iteration 732, loss = 0.16893680\n",
      "Iteration 733, loss = 0.16861383\n",
      "Iteration 734, loss = 0.16830393\n",
      "Iteration 735, loss = 0.16799378\n",
      "Iteration 736, loss = 0.16767555\n",
      "Iteration 737, loss = 0.16736290\n",
      "Iteration 738, loss = 0.16705780\n",
      "Iteration 739, loss = 0.16673542\n",
      "Iteration 740, loss = 0.16642325\n",
      "Iteration 741, loss = 0.16610671\n",
      "Iteration 742, loss = 0.16579575\n",
      "Iteration 743, loss = 0.16547958\n",
      "Iteration 744, loss = 0.16517183\n",
      "Iteration 745, loss = 0.16485856\n",
      "Iteration 746, loss = 0.16455641\n",
      "Iteration 747, loss = 0.16424549\n",
      "Iteration 748, loss = 0.16395171\n",
      "Iteration 749, loss = 0.16365141\n",
      "Iteration 750, loss = 0.16335176\n",
      "Iteration 751, loss = 0.16306102\n",
      "Iteration 752, loss = 0.16275933\n",
      "Iteration 753, loss = 0.16246986\n",
      "Iteration 754, loss = 0.16217102\n",
      "Iteration 755, loss = 0.16188636\n",
      "Iteration 756, loss = 0.16157367\n",
      "Iteration 757, loss = 0.16127161\n",
      "Iteration 758, loss = 0.16098000\n",
      "Iteration 759, loss = 0.16067515\n",
      "Iteration 760, loss = 0.16038323\n",
      "Iteration 761, loss = 0.16009086\n",
      "Iteration 762, loss = 0.15979654\n",
      "Iteration 763, loss = 0.15949994\n",
      "Iteration 764, loss = 0.15919250\n",
      "Iteration 765, loss = 0.15889737\n",
      "Iteration 766, loss = 0.15860765\n",
      "Iteration 767, loss = 0.15830834\n",
      "Iteration 768, loss = 0.15801687\n",
      "Iteration 769, loss = 0.15773627\n",
      "Iteration 770, loss = 0.15743936\n",
      "Iteration 771, loss = 0.15713939\n",
      "Iteration 772, loss = 0.15684845\n",
      "Iteration 773, loss = 0.15655986\n",
      "Iteration 774, loss = 0.15626515\n",
      "Iteration 775, loss = 0.15597227\n",
      "Iteration 776, loss = 0.15568254\n",
      "Iteration 777, loss = 0.15540261\n",
      "Iteration 778, loss = 0.15511921\n",
      "Iteration 779, loss = 0.15484436\n",
      "Iteration 780, loss = 0.15454591\n",
      "Iteration 781, loss = 0.15426964\n",
      "Iteration 782, loss = 0.15399787\n",
      "Iteration 783, loss = 0.15369462\n",
      "Iteration 784, loss = 0.15344281\n",
      "Iteration 785, loss = 0.15316041\n",
      "Iteration 786, loss = 0.15288576\n",
      "Iteration 787, loss = 0.15260586\n",
      "Iteration 788, loss = 0.15234784\n",
      "Iteration 789, loss = 0.15207848\n",
      "Iteration 790, loss = 0.15181245\n",
      "Iteration 791, loss = 0.15154197\n",
      "Iteration 792, loss = 0.15127007\n",
      "Iteration 793, loss = 0.15101014\n",
      "Iteration 794, loss = 0.15073791\n",
      "Iteration 795, loss = 0.15047638\n",
      "Iteration 796, loss = 0.15020191\n",
      "Iteration 797, loss = 0.14993903\n",
      "Iteration 798, loss = 0.14967006\n",
      "Iteration 799, loss = 0.14939394\n",
      "Iteration 800, loss = 0.14911736\n",
      "Iteration 801, loss = 0.14885473\n",
      "Iteration 802, loss = 0.14858824\n",
      "Iteration 803, loss = 0.14833448\n",
      "Iteration 804, loss = 0.14805549\n",
      "Iteration 805, loss = 0.14780039\n",
      "Iteration 806, loss = 0.14753810\n",
      "Iteration 807, loss = 0.14728022\n",
      "Iteration 808, loss = 0.14701684\n",
      "Iteration 809, loss = 0.14675447\n",
      "Iteration 810, loss = 0.14648940\n",
      "Iteration 811, loss = 0.14622914\n",
      "Iteration 812, loss = 0.14596517\n",
      "Iteration 813, loss = 0.14571395\n",
      "Iteration 814, loss = 0.14545085\n",
      "Iteration 815, loss = 0.14520602\n",
      "Iteration 816, loss = 0.14495973\n",
      "Iteration 817, loss = 0.14469552\n",
      "Iteration 818, loss = 0.14444424\n",
      "Iteration 819, loss = 0.14419512\n",
      "Iteration 820, loss = 0.14394319\n",
      "Iteration 821, loss = 0.14371332\n",
      "Iteration 822, loss = 0.14344678\n",
      "Iteration 823, loss = 0.14319640\n",
      "Iteration 824, loss = 0.14295093\n",
      "Iteration 825, loss = 0.14269595\n",
      "Iteration 826, loss = 0.14243682\n",
      "Iteration 827, loss = 0.14219241\n",
      "Iteration 828, loss = 0.14195215\n",
      "Iteration 829, loss = 0.14169638\n",
      "Iteration 830, loss = 0.14145141\n",
      "Iteration 831, loss = 0.14120907\n",
      "Iteration 832, loss = 0.14096157\n",
      "Iteration 833, loss = 0.14072764\n",
      "Iteration 834, loss = 0.14047170\n",
      "Iteration 835, loss = 0.14024616\n",
      "Iteration 836, loss = 0.14001174\n",
      "Iteration 837, loss = 0.13976721\n",
      "Iteration 838, loss = 0.13953959\n",
      "Iteration 839, loss = 0.13928958\n",
      "Iteration 840, loss = 0.13905472\n",
      "Iteration 841, loss = 0.13880277\n",
      "Iteration 842, loss = 0.13857813\n",
      "Iteration 843, loss = 0.13833512\n",
      "Iteration 844, loss = 0.13809026\n",
      "Iteration 845, loss = 0.13784861\n",
      "Iteration 846, loss = 0.13761160\n",
      "Iteration 847, loss = 0.13737061\n",
      "Iteration 848, loss = 0.13713207\n",
      "Iteration 849, loss = 0.13689873\n",
      "Iteration 850, loss = 0.13665688\n",
      "Iteration 851, loss = 0.13642417\n",
      "Iteration 852, loss = 0.13618829\n",
      "Iteration 853, loss = 0.13594380\n",
      "Iteration 854, loss = 0.13571485\n",
      "Iteration 855, loss = 0.13548492\n",
      "Iteration 856, loss = 0.13525181\n",
      "Iteration 857, loss = 0.13501169\n",
      "Iteration 858, loss = 0.13478223\n",
      "Iteration 859, loss = 0.13454907\n",
      "Iteration 860, loss = 0.13431907\n",
      "Iteration 861, loss = 0.13406919\n",
      "Iteration 862, loss = 0.13385259\n",
      "Iteration 863, loss = 0.13361245\n",
      "Iteration 864, loss = 0.13337671\n",
      "Iteration 865, loss = 0.13316433\n",
      "Iteration 866, loss = 0.13292324\n",
      "Iteration 867, loss = 0.13270117\n",
      "Iteration 868, loss = 0.13246733\n",
      "Iteration 869, loss = 0.13225550\n",
      "Iteration 870, loss = 0.13201907\n",
      "Iteration 871, loss = 0.13178883\n",
      "Iteration 872, loss = 0.13156093\n",
      "Iteration 873, loss = 0.13133591\n",
      "Iteration 874, loss = 0.13111254\n",
      "Iteration 875, loss = 0.13089556\n",
      "Iteration 876, loss = 0.13066489\n",
      "Iteration 877, loss = 0.13043753\n",
      "Iteration 878, loss = 0.13022515\n",
      "Iteration 879, loss = 0.12998852\n",
      "Iteration 880, loss = 0.12978531\n",
      "Iteration 881, loss = 0.12955285\n",
      "Iteration 882, loss = 0.12934091\n",
      "Iteration 883, loss = 0.12912031\n",
      "Iteration 884, loss = 0.12890204\n",
      "Iteration 885, loss = 0.12868458\n",
      "Iteration 886, loss = 0.12846340\n",
      "Iteration 887, loss = 0.12825031\n",
      "Iteration 888, loss = 0.12801923\n",
      "Iteration 889, loss = 0.12780207\n",
      "Iteration 890, loss = 0.12758427\n",
      "Iteration 891, loss = 0.12736120\n",
      "Iteration 892, loss = 0.12714790\n",
      "Iteration 893, loss = 0.12694170\n",
      "Iteration 894, loss = 0.12670843\n",
      "Iteration 895, loss = 0.12650277\n",
      "Iteration 896, loss = 0.12628674\n",
      "Iteration 897, loss = 0.12607961\n",
      "Iteration 898, loss = 0.12586634\n",
      "Iteration 899, loss = 0.12565193\n",
      "Iteration 900, loss = 0.12544500\n",
      "Iteration 901, loss = 0.12523700\n",
      "Iteration 902, loss = 0.12503382\n",
      "Iteration 903, loss = 0.12482277\n",
      "Iteration 904, loss = 0.12463019\n",
      "Iteration 905, loss = 0.12441557\n",
      "Iteration 906, loss = 0.12421590\n",
      "Iteration 907, loss = 0.12401382\n",
      "Iteration 908, loss = 0.12379828\n",
      "Iteration 909, loss = 0.12359468\n",
      "Iteration 910, loss = 0.12339443\n",
      "Iteration 911, loss = 0.12319344\n",
      "Iteration 912, loss = 0.12298920\n",
      "Iteration 913, loss = 0.12278421\n",
      "Iteration 914, loss = 0.12257813\n",
      "Iteration 915, loss = 0.12238114\n",
      "Iteration 916, loss = 0.12218039\n",
      "Iteration 917, loss = 0.12197697\n",
      "Iteration 918, loss = 0.12177996\n",
      "Iteration 919, loss = 0.12158196\n",
      "Iteration 920, loss = 0.12138855\n",
      "Iteration 921, loss = 0.12119322\n",
      "Iteration 922, loss = 0.12099331\n",
      "Iteration 923, loss = 0.12079696\n",
      "Iteration 924, loss = 0.12059548\n",
      "Iteration 925, loss = 0.12039929\n",
      "Iteration 926, loss = 0.12020948\n",
      "Iteration 927, loss = 0.12001633\n",
      "Iteration 928, loss = 0.11980940\n",
      "Iteration 929, loss = 0.11961844\n",
      "Iteration 930, loss = 0.11941376\n",
      "Iteration 931, loss = 0.11923931\n",
      "Iteration 932, loss = 0.11902426\n",
      "Iteration 933, loss = 0.11883908\n",
      "Iteration 934, loss = 0.11864844\n",
      "Iteration 935, loss = 0.11845782\n",
      "Iteration 936, loss = 0.11825621\n",
      "Iteration 937, loss = 0.11806216\n",
      "Iteration 938, loss = 0.11787242\n",
      "Iteration 939, loss = 0.11768314\n",
      "Iteration 940, loss = 0.11749283\n",
      "Iteration 941, loss = 0.11729179\n",
      "Iteration 942, loss = 0.11710441\n",
      "Iteration 943, loss = 0.11691564\n",
      "Iteration 944, loss = 0.11671978\n",
      "Iteration 945, loss = 0.11652764\n",
      "Iteration 946, loss = 0.11633332\n",
      "Iteration 947, loss = 0.11614505\n",
      "Iteration 948, loss = 0.11594331\n",
      "Iteration 949, loss = 0.11575636\n",
      "Iteration 950, loss = 0.11556374\n",
      "Iteration 951, loss = 0.11536798\n",
      "Iteration 952, loss = 0.11519579\n",
      "Iteration 953, loss = 0.11499243\n",
      "Iteration 954, loss = 0.11480506\n",
      "Iteration 955, loss = 0.11461578\n",
      "Iteration 956, loss = 0.11442422\n",
      "Iteration 957, loss = 0.11424196\n",
      "Iteration 958, loss = 0.11404258\n",
      "Iteration 959, loss = 0.11386238\n",
      "Iteration 960, loss = 0.11366870\n",
      "Iteration 961, loss = 0.11348047\n",
      "Iteration 962, loss = 0.11330458\n",
      "Iteration 963, loss = 0.11311003\n",
      "Iteration 964, loss = 0.11292859\n",
      "Iteration 965, loss = 0.11274844\n",
      "Iteration 966, loss = 0.11257006\n",
      "Iteration 967, loss = 0.11238157\n",
      "Iteration 968, loss = 0.11219303\n",
      "Iteration 969, loss = 0.11201917\n",
      "Iteration 970, loss = 0.11183472\n",
      "Iteration 971, loss = 0.11165169\n",
      "Iteration 972, loss = 0.11147599\n",
      "Iteration 973, loss = 0.11130017\n",
      "Iteration 974, loss = 0.11112468\n",
      "Iteration 975, loss = 0.11094703\n",
      "Iteration 976, loss = 0.11076617\n",
      "Iteration 977, loss = 0.11058247\n",
      "Iteration 978, loss = 0.11039956\n",
      "Iteration 979, loss = 0.11021258\n",
      "Iteration 980, loss = 0.11004380\n",
      "Iteration 981, loss = 0.10985481\n",
      "Iteration 982, loss = 0.10967405\n",
      "Iteration 983, loss = 0.10950226\n",
      "Iteration 984, loss = 0.10931894\n",
      "Iteration 985, loss = 0.10916141\n",
      "Iteration 986, loss = 0.10898035\n",
      "Iteration 987, loss = 0.10881003\n",
      "Iteration 988, loss = 0.10863158\n",
      "Iteration 989, loss = 0.10845560\n",
      "Iteration 990, loss = 0.10828299\n",
      "Iteration 991, loss = 0.10810243\n",
      "Iteration 992, loss = 0.10792836\n",
      "Iteration 993, loss = 0.10774995\n",
      "Iteration 994, loss = 0.10757515\n",
      "Iteration 995, loss = 0.10739712\n",
      "Iteration 996, loss = 0.10721240\n",
      "Iteration 997, loss = 0.10705388\n",
      "Iteration 998, loss = 0.10687762\n",
      "Iteration 999, loss = 0.10669795\n",
      "Iteration 1000, loss = 0.10652386\n",
      "Iteration 1001, loss = 0.10636405\n",
      "Iteration 1002, loss = 0.10618704\n",
      "Iteration 1003, loss = 0.10601536\n",
      "Iteration 1004, loss = 0.10584503\n",
      "Iteration 1005, loss = 0.10568152\n",
      "Iteration 1006, loss = 0.10551209\n",
      "Iteration 1007, loss = 0.10535115\n",
      "Iteration 1008, loss = 0.10518096\n",
      "Iteration 1009, loss = 0.10502001\n",
      "Iteration 1010, loss = 0.10484898\n",
      "Iteration 1011, loss = 0.10469279\n",
      "Iteration 1012, loss = 0.10452939\n",
      "Iteration 1013, loss = 0.10437304\n",
      "Iteration 1014, loss = 0.10421388\n",
      "Iteration 1015, loss = 0.10405668\n",
      "Iteration 1016, loss = 0.10389574\n",
      "Iteration 1017, loss = 0.10373508\n",
      "Iteration 1018, loss = 0.10356403\n",
      "Iteration 1019, loss = 0.10340757\n",
      "Iteration 1020, loss = 0.10324108\n",
      "Iteration 1021, loss = 0.10307575\n",
      "Iteration 1022, loss = 0.10290896\n",
      "Iteration 1023, loss = 0.10275015\n",
      "Iteration 1024, loss = 0.10257766\n",
      "Iteration 1025, loss = 0.10242222\n",
      "Iteration 1026, loss = 0.10225933\n",
      "Iteration 1027, loss = 0.10209820\n",
      "Iteration 1028, loss = 0.10192968\n",
      "Iteration 1029, loss = 0.10177366\n",
      "Iteration 1030, loss = 0.10162193\n",
      "Iteration 1031, loss = 0.10145624\n",
      "Iteration 1032, loss = 0.10130446\n",
      "Iteration 1033, loss = 0.10113912\n",
      "Iteration 1034, loss = 0.10098948\n",
      "Iteration 1035, loss = 0.10083063\n",
      "Iteration 1036, loss = 0.10067398\n",
      "Iteration 1037, loss = 0.10051959\n",
      "Iteration 1038, loss = 0.10036055\n",
      "Iteration 1039, loss = 0.10020659\n",
      "Iteration 1040, loss = 0.10004351\n",
      "Iteration 1041, loss = 0.09988760\n",
      "Iteration 1042, loss = 0.09973757\n",
      "Iteration 1043, loss = 0.09957357\n",
      "Iteration 1044, loss = 0.09941469\n",
      "Iteration 1045, loss = 0.09925884\n",
      "Iteration 1046, loss = 0.09911469\n",
      "Iteration 1047, loss = 0.09895416\n",
      "Iteration 1048, loss = 0.09880127\n",
      "Iteration 1049, loss = 0.09865286\n",
      "Iteration 1050, loss = 0.09849812\n",
      "Iteration 1051, loss = 0.09835583\n",
      "Iteration 1052, loss = 0.09820125\n",
      "Iteration 1053, loss = 0.09805200\n",
      "Iteration 1054, loss = 0.09789923\n",
      "Iteration 1055, loss = 0.09775526\n",
      "Iteration 1056, loss = 0.09759954\n",
      "Iteration 1057, loss = 0.09745266\n",
      "Iteration 1058, loss = 0.09731036\n",
      "Iteration 1059, loss = 0.09716192\n",
      "Iteration 1060, loss = 0.09701905\n",
      "Iteration 1061, loss = 0.09686290\n",
      "Iteration 1062, loss = 0.09671831\n",
      "Iteration 1063, loss = 0.09656846\n",
      "Iteration 1064, loss = 0.09642491\n",
      "Iteration 1065, loss = 0.09627865\n",
      "Iteration 1066, loss = 0.09612488\n",
      "Iteration 1067, loss = 0.09598124\n",
      "Iteration 1068, loss = 0.09583704\n",
      "Iteration 1069, loss = 0.09569242\n",
      "Iteration 1070, loss = 0.09554636\n",
      "Iteration 1071, loss = 0.09540305\n",
      "Iteration 1072, loss = 0.09525643\n",
      "Iteration 1073, loss = 0.09510761\n",
      "Iteration 1074, loss = 0.09497144\n",
      "Iteration 1075, loss = 0.09482024\n",
      "Iteration 1076, loss = 0.09467824\n",
      "Iteration 1077, loss = 0.09453675\n",
      "Iteration 1078, loss = 0.09438453\n",
      "Iteration 1079, loss = 0.09424858\n",
      "Iteration 1080, loss = 0.09410024\n",
      "Iteration 1081, loss = 0.09395572\n",
      "Iteration 1082, loss = 0.09381612\n",
      "Iteration 1083, loss = 0.09366640\n",
      "Iteration 1084, loss = 0.09351533\n",
      "Iteration 1085, loss = 0.09336865\n",
      "Iteration 1086, loss = 0.09322979\n",
      "Iteration 1087, loss = 0.09307933\n",
      "Iteration 1088, loss = 0.09294048\n",
      "Iteration 1089, loss = 0.09279579\n",
      "Iteration 1090, loss = 0.09266428\n",
      "Iteration 1091, loss = 0.09252123\n",
      "Iteration 1092, loss = 0.09237471\n",
      "Iteration 1093, loss = 0.09223834\n",
      "Iteration 1094, loss = 0.09209631\n",
      "Iteration 1095, loss = 0.09195842\n",
      "Iteration 1096, loss = 0.09181815\n",
      "Iteration 1097, loss = 0.09168575\n",
      "Iteration 1098, loss = 0.09153615\n",
      "Iteration 1099, loss = 0.09140463\n",
      "Iteration 1100, loss = 0.09127033\n",
      "Iteration 1101, loss = 0.09113200\n",
      "Iteration 1102, loss = 0.09099819\n",
      "Iteration 1103, loss = 0.09085944\n",
      "Iteration 1104, loss = 0.09072484\n",
      "Iteration 1105, loss = 0.09059424\n",
      "Iteration 1106, loss = 0.09045596\n",
      "Iteration 1107, loss = 0.09031793\n",
      "Iteration 1108, loss = 0.09017907\n",
      "Iteration 1109, loss = 0.09004015\n",
      "Iteration 1110, loss = 0.08989511\n",
      "Iteration 1111, loss = 0.08976179\n",
      "Iteration 1112, loss = 0.08962514\n",
      "Iteration 1113, loss = 0.08948618\n",
      "Iteration 1114, loss = 0.08934710\n",
      "Iteration 1115, loss = 0.08921588\n",
      "Iteration 1116, loss = 0.08908237\n",
      "Iteration 1117, loss = 0.08895056\n",
      "Iteration 1118, loss = 0.08880948\n",
      "Iteration 1119, loss = 0.08867653\n",
      "Iteration 1120, loss = 0.08855183\n",
      "Iteration 1121, loss = 0.08842425\n",
      "Iteration 1122, loss = 0.08829109\n",
      "Iteration 1123, loss = 0.08816340\n",
      "Iteration 1124, loss = 0.08803466\n",
      "Iteration 1125, loss = 0.08790084\n",
      "Iteration 1126, loss = 0.08778795\n",
      "Iteration 1127, loss = 0.08765386\n",
      "Iteration 1128, loss = 0.08752844\n",
      "Iteration 1129, loss = 0.08739855\n",
      "Iteration 1130, loss = 0.08726743\n",
      "Iteration 1131, loss = 0.08714062\n",
      "Iteration 1132, loss = 0.08701365\n",
      "Iteration 1133, loss = 0.08688275\n",
      "Iteration 1134, loss = 0.08675739\n",
      "Iteration 1135, loss = 0.08663122\n",
      "Iteration 1136, loss = 0.08650137\n",
      "Iteration 1137, loss = 0.08638251\n",
      "Iteration 1138, loss = 0.08625028\n",
      "Iteration 1139, loss = 0.08613070\n",
      "Iteration 1140, loss = 0.08600076\n",
      "Iteration 1141, loss = 0.08588287\n",
      "Iteration 1142, loss = 0.08576100\n",
      "Iteration 1143, loss = 0.08563361\n",
      "Iteration 1144, loss = 0.08551350\n",
      "Iteration 1145, loss = 0.08539174\n",
      "Iteration 1146, loss = 0.08526783\n",
      "Iteration 1147, loss = 0.08514274\n",
      "Iteration 1148, loss = 0.08502431\n",
      "Iteration 1149, loss = 0.08490251\n",
      "Iteration 1150, loss = 0.08478208\n",
      "Iteration 1151, loss = 0.08465427\n",
      "Iteration 1152, loss = 0.08453279\n",
      "Iteration 1153, loss = 0.08441345\n",
      "Iteration 1154, loss = 0.08428750\n",
      "Iteration 1155, loss = 0.08416357\n",
      "Iteration 1156, loss = 0.08404334\n",
      "Iteration 1157, loss = 0.08391713\n",
      "Iteration 1158, loss = 0.08379673\n",
      "Iteration 1159, loss = 0.08367498\n",
      "Iteration 1160, loss = 0.08354506\n",
      "Iteration 1161, loss = 0.08342859\n",
      "Iteration 1162, loss = 0.08331005\n",
      "Iteration 1163, loss = 0.08318411\n",
      "Iteration 1164, loss = 0.08306674\n",
      "Iteration 1165, loss = 0.08295027\n",
      "Iteration 1166, loss = 0.08283359\n",
      "Iteration 1167, loss = 0.08271583\n",
      "Iteration 1168, loss = 0.08259799\n",
      "Iteration 1169, loss = 0.08248810\n",
      "Iteration 1170, loss = 0.08236699\n",
      "Iteration 1171, loss = 0.08225187\n",
      "Iteration 1172, loss = 0.08214026\n",
      "Iteration 1173, loss = 0.08201831\n",
      "Iteration 1174, loss = 0.08189928\n",
      "Iteration 1175, loss = 0.08178569\n",
      "Iteration 1176, loss = 0.08166825\n",
      "Iteration 1177, loss = 0.08155348\n",
      "Iteration 1178, loss = 0.08143345\n",
      "Iteration 1179, loss = 0.08132244\n",
      "Iteration 1180, loss = 0.08120978\n",
      "Iteration 1181, loss = 0.08109164\n",
      "Iteration 1182, loss = 0.08098256\n",
      "Iteration 1183, loss = 0.08087461\n",
      "Iteration 1184, loss = 0.08075791\n",
      "Iteration 1185, loss = 0.08064198\n",
      "Iteration 1186, loss = 0.08053604\n",
      "Iteration 1187, loss = 0.08042387\n",
      "Iteration 1188, loss = 0.08031031\n",
      "Iteration 1189, loss = 0.08019521\n",
      "Iteration 1190, loss = 0.08008520\n",
      "Iteration 1191, loss = 0.07997494\n",
      "Iteration 1192, loss = 0.07986456\n",
      "Iteration 1193, loss = 0.07975309\n",
      "Iteration 1194, loss = 0.07963518\n",
      "Iteration 1195, loss = 0.07952765\n",
      "Iteration 1196, loss = 0.07941658\n",
      "Iteration 1197, loss = 0.07930774\n",
      "Iteration 1198, loss = 0.07919395\n",
      "Iteration 1199, loss = 0.07908395\n",
      "Iteration 1200, loss = 0.07897822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\syeda\\OneDrive\\Documents\\American Bureau of Shipping\\projects\\nlp_risk_prediction\\nlp_env\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-5 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-5 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-5 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-5 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-5 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-5 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-5 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-5 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-5 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-5 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-5 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;Object&#x27;, TfidfVectorizer(),\n",
       "                                                  &#x27;Object&#x27;),\n",
       "                                                 (&#x27;Group&#x27;, TfidfVectorizer(),\n",
       "                                                  &#x27;Group&#x27;),\n",
       "                                                 (&#x27;Object_Type&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;Object_Type&#x27;),\n",
       "                                                 (&#x27;Completion_Note&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;Completion_Note&#x27;),\n",
       "                                                 (&#x27;Work_Description&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;Work_Description&#x27;),\n",
       "                                                 (&#x27;Directive&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;Directive&#x27;)])),\n",
       "                (&#x27;scaler&#x27;, MaxAbsScaler()),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 MLPClassifier(activation=&#x27;tanh&#x27;, hidden_layer_sizes=(100, 100),\n",
       "                               learning_rate=&#x27;adaptive&#x27;, max_iter=1200,\n",
       "                               solver=&#x27;sgd&#x27;, verbose=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-91\" type=\"checkbox\" ><label for=\"sk-estimator-id-91\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;Object&#x27;, TfidfVectorizer(),\n",
       "                                                  &#x27;Object&#x27;),\n",
       "                                                 (&#x27;Group&#x27;, TfidfVectorizer(),\n",
       "                                                  &#x27;Group&#x27;),\n",
       "                                                 (&#x27;Object_Type&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;Object_Type&#x27;),\n",
       "                                                 (&#x27;Completion_Note&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;Completion_Note&#x27;),\n",
       "                                                 (&#x27;Work_Description&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;Work_Description&#x27;),\n",
       "                                                 (&#x27;Directive&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;Directive&#x27;)])),\n",
       "                (&#x27;scaler&#x27;, MaxAbsScaler()),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 MLPClassifier(activation=&#x27;tanh&#x27;, hidden_layer_sizes=(100, 100),\n",
       "                               learning_rate=&#x27;adaptive&#x27;, max_iter=1200,\n",
       "                               solver=&#x27;sgd&#x27;, verbose=True))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-92\" type=\"checkbox\" ><label for=\"sk-estimator-id-92\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;preprocessor: ColumnTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for preprocessor: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>ColumnTransformer(transformers=[(&#x27;Object&#x27;, TfidfVectorizer(), &#x27;Object&#x27;),\n",
       "                                (&#x27;Group&#x27;, TfidfVectorizer(), &#x27;Group&#x27;),\n",
       "                                (&#x27;Object_Type&#x27;, TfidfVectorizer(),\n",
       "                                 &#x27;Object_Type&#x27;),\n",
       "                                (&#x27;Completion_Note&#x27;, TfidfVectorizer(),\n",
       "                                 &#x27;Completion_Note&#x27;),\n",
       "                                (&#x27;Work_Description&#x27;, TfidfVectorizer(),\n",
       "                                 &#x27;Work_Description&#x27;),\n",
       "                                (&#x27;Directive&#x27;, TfidfVectorizer(), &#x27;Directive&#x27;)])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-93\" type=\"checkbox\" ><label for=\"sk-estimator-id-93\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Object</label><div class=\"sk-toggleable__content fitted\"><pre>Object</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-94\" type=\"checkbox\" ><label for=\"sk-estimator-id-94\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer()</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-95\" type=\"checkbox\" ><label for=\"sk-estimator-id-95\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Group</label><div class=\"sk-toggleable__content fitted\"><pre>Group</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-96\" type=\"checkbox\" ><label for=\"sk-estimator-id-96\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer()</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-97\" type=\"checkbox\" ><label for=\"sk-estimator-id-97\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Object_Type</label><div class=\"sk-toggleable__content fitted\"><pre>Object_Type</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-98\" type=\"checkbox\" ><label for=\"sk-estimator-id-98\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer()</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-99\" type=\"checkbox\" ><label for=\"sk-estimator-id-99\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Completion_Note</label><div class=\"sk-toggleable__content fitted\"><pre>Completion_Note</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-100\" type=\"checkbox\" ><label for=\"sk-estimator-id-100\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer()</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-101\" type=\"checkbox\" ><label for=\"sk-estimator-id-101\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Work_Description</label><div class=\"sk-toggleable__content fitted\"><pre>Work_Description</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-102\" type=\"checkbox\" ><label for=\"sk-estimator-id-102\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer()</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-103\" type=\"checkbox\" ><label for=\"sk-estimator-id-103\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Directive</label><div class=\"sk-toggleable__content fitted\"><pre>Directive</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-104\" type=\"checkbox\" ><label for=\"sk-estimator-id-104\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-105\" type=\"checkbox\" ><label for=\"sk-estimator-id-105\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MaxAbsScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.MaxAbsScaler.html\">?<span>Documentation for MaxAbsScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MaxAbsScaler()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-106\" type=\"checkbox\" ><label for=\"sk-estimator-id-106\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, hidden_layer_sizes=(100, 100),\n",
       "              learning_rate=&#x27;adaptive&#x27;, max_iter=1200, solver=&#x27;sgd&#x27;,\n",
       "              verbose=True)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('Object', TfidfVectorizer(),\n",
       "                                                  'Object'),\n",
       "                                                 ('Group', TfidfVectorizer(),\n",
       "                                                  'Group'),\n",
       "                                                 ('Object_Type',\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  'Object_Type'),\n",
       "                                                 ('Completion_Note',\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  'Completion_Note'),\n",
       "                                                 ('Work_Description',\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  'Work_Description'),\n",
       "                                                 ('Directive',\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  'Directive')])),\n",
       "                ('scaler', MaxAbsScaler()),\n",
       "                ('classifier',\n",
       "                 MLPClassifier(activation='tanh', hidden_layer_sizes=(100, 100),\n",
       "                               learning_rate='adaptive', max_iter=1200,\n",
       "                               solver='sgd', verbose=True))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the TF-IDF vectorizer for text columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (col, TfidfVectorizer(), col) for col in feature_columns\n",
    "    ], remainder='drop'  # Dropping non-specified columns, though all columns are specified here\n",
    ")\n",
    "\n",
    "params = {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'max_iter': 1200, 'solver': 'sgd', 'verbose': True}\n",
    "\n",
    "# Create a pipeline with preprocessor, scaler, and MLPClassifier with fixed hyperparameters\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('scaler', MaxAbsScaler()),  # Apply scaler after TF-IDF transformation\n",
    "    ('classifier', MLPClassifier(**params))\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df[feature_columns]\n",
    "y = df[target_column]\n",
    "\n",
    "# Check if the length of X and y are consistent\n",
    "assert len(X) == len(y), \"Mismatch in the number of samples between X and y\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training samples: \", X_train.shape, \"Testing samples: \", X_test.shape)\n",
    "print(\"Training labels: \", y_train.shape, \"Testing labels: \", y_test.shape)\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.8770491803278688\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       1.00      1.00      1.00        25\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.67      0.67      0.67         3\n",
      "           5       0.83      1.00      0.91        25\n",
      "           7       0.50      1.00      0.67         1\n",
      "           8       1.00      0.50      0.67         2\n",
      "           9       0.75      1.00      0.86         3\n",
      "          10       0.75      1.00      0.86         3\n",
      "          12       1.00      1.00      1.00         2\n",
      "          13       0.96      0.93      0.95        29\n",
      "          14       1.00      1.00      1.00         1\n",
      "          16       0.00      0.00      0.00         1\n",
      "          17       1.00      0.40      0.57         5\n",
      "          20       0.50      1.00      0.67         1\n",
      "          21       1.00      1.00      1.00         4\n",
      "          22       0.33      1.00      0.50         1\n",
      "          23       0.00      0.00      0.00         0\n",
      "          25       1.00      0.50      0.67         2\n",
      "          26       1.00      1.00      1.00         4\n",
      "          27       1.00      1.00      1.00         2\n",
      "          28       0.67      1.00      0.80         2\n",
      "          29       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.88       122\n",
      "   macro avg       0.62      0.67      0.62       122\n",
      "weighted avg       0.86      0.88      0.85       122\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\syeda\\OneDrive\\Documents\\American Bureau of Shipping\\projects\\nlp_risk_prediction\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\syeda\\OneDrive\\Documents\\American Bureau of Shipping\\projects\\nlp_risk_prediction\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\syeda\\OneDrive\\Documents\\American Bureau of Shipping\\projects\\nlp_risk_prediction\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\syeda\\OneDrive\\Documents\\American Bureau of Shipping\\projects\\nlp_risk_prediction\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\syeda\\OneDrive\\Documents\\American Bureau of Shipping\\projects\\nlp_risk_prediction\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\syeda\\OneDrive\\Documents\\American Bureau of Shipping\\projects\\nlp_risk_prediction\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test accuracy: \", test_accuracy)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot the loss curve\n",
    "classifier = pipeline.named_steps['classifier']\n",
    "plt.plot(classifier.loss_curve_)\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
