{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MaxAbsScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\syeda\\OneDrive\\Documents\\American Bureau of Shipping\\projects\\nlp_risk_prediction\\nlp_env\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "c:\\Users\\syeda\\OneDrive\\Documents\\American Bureau of Shipping\\projects\\nlp_risk_prediction\\nlp_env\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Import the excel spreadsheets in the data folder\n",
    "installer_df = pd.read_excel('../data/Installer.xlsx')\n",
    "involver_df = pd.read_excel('../data/Involver.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to merge using a subset of key columns\n",
    "merge_on_columns = ['Site', 'Vessel_Name', 'Wo_No']\n",
    "\n",
    "df = pd.concat([installer_df, involver_df], axis=0)\n",
    "\n",
    "feature_columns = ['Group', 'Symptom', 'Error_Cause', 'Cause_Details', 'Error_Class', 'Discovery', 'Completion_Note', 'Action_Taken', 'Work_Description', 'Directive']\n",
    "# TODO: Change this to ESB1\n",
    "target_column = 'EBS1'\n",
    "\n",
    "# Filter the dataframe for the selected columns\n",
    "df = df[feature_columns + [target_column]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('int64'),\n",
       " array([ 4, 25, 28,  7,  3, 19,  9, 29, 18, 17, 10, 22, 12, 26, 21, 11, 15,\n",
       "        33,  2,  6, 32, 23, 24, 30, 16,  1, 14, 27,  5,  0, 20,  8, 13, 31]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the target column to ids \n",
    "# Encode the target column\n",
    "label_encoder = LabelEncoder()\n",
    "df[target_column] = label_encoder.fit_transform(df[target_column].astype(str))\n",
    "df[target_column].dtype, df[target_column].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:  (545, 10) Testing samples:  (137, 10)\n",
      "Training labels:  (545,) Testing labels:  (137,)\n",
      "Iteration 1, loss = 3.45405356\n",
      "Iteration 2, loss = 3.43986240\n",
      "Iteration 3, loss = 3.41800125\n",
      "Iteration 4, loss = 3.39084951\n",
      "Iteration 5, loss = 3.35988155\n",
      "Iteration 6, loss = 3.32660063\n",
      "Iteration 7, loss = 3.29139586\n",
      "Iteration 8, loss = 3.25462892\n",
      "Iteration 9, loss = 3.21863071\n",
      "Iteration 10, loss = 3.18116470\n",
      "Iteration 11, loss = 3.14345298\n",
      "Iteration 12, loss = 3.10642271\n",
      "Iteration 13, loss = 3.06930664\n",
      "Iteration 14, loss = 3.03240815\n",
      "Iteration 15, loss = 2.99561090\n",
      "Iteration 16, loss = 2.95930815\n",
      "Iteration 17, loss = 2.92275975\n",
      "Iteration 18, loss = 2.88676853\n",
      "Iteration 19, loss = 2.85175843\n",
      "Iteration 20, loss = 2.81642164\n",
      "Iteration 21, loss = 2.78139219\n",
      "Iteration 22, loss = 2.74663940\n",
      "Iteration 23, loss = 2.71262696\n",
      "Iteration 24, loss = 2.67943205\n",
      "Iteration 25, loss = 2.64576007\n",
      "Iteration 26, loss = 2.61266208\n",
      "Iteration 27, loss = 2.58126872\n",
      "Iteration 28, loss = 2.54966310\n",
      "Iteration 29, loss = 2.51899312\n",
      "Iteration 30, loss = 2.48951248\n",
      "Iteration 31, loss = 2.45912777\n",
      "Iteration 32, loss = 2.43060367\n",
      "Iteration 33, loss = 2.40312655\n",
      "Iteration 34, loss = 2.37531623\n",
      "Iteration 35, loss = 2.34896166\n",
      "Iteration 36, loss = 2.32298665\n",
      "Iteration 37, loss = 2.29810423\n",
      "Iteration 38, loss = 2.27375508\n",
      "Iteration 39, loss = 2.25006020\n",
      "Iteration 40, loss = 2.22715337\n",
      "Iteration 41, loss = 2.20495917\n",
      "Iteration 42, loss = 2.18334785\n",
      "Iteration 43, loss = 2.16213329\n",
      "Iteration 44, loss = 2.14175978\n",
      "Iteration 45, loss = 2.12198163\n",
      "Iteration 46, loss = 2.10230379\n",
      "Iteration 47, loss = 2.08355559\n",
      "Iteration 48, loss = 2.06509110\n",
      "Iteration 49, loss = 2.04673234\n",
      "Iteration 50, loss = 2.02914008\n",
      "Iteration 51, loss = 2.01210572\n",
      "Iteration 52, loss = 1.99477934\n",
      "Iteration 53, loss = 1.97844845\n",
      "Iteration 54, loss = 1.96222392\n",
      "Iteration 55, loss = 1.94582592\n",
      "Iteration 56, loss = 1.93023867\n",
      "Iteration 57, loss = 1.91495067\n",
      "Iteration 58, loss = 1.89960184\n",
      "Iteration 59, loss = 1.88455914\n",
      "Iteration 60, loss = 1.86996812\n",
      "Iteration 61, loss = 1.85519283\n",
      "Iteration 62, loss = 1.84102210\n",
      "Iteration 63, loss = 1.82677756\n",
      "Iteration 64, loss = 1.81271306\n",
      "Iteration 65, loss = 1.79897125\n",
      "Iteration 66, loss = 1.78538766\n",
      "Iteration 67, loss = 1.77187068\n",
      "Iteration 68, loss = 1.75850731\n",
      "Iteration 69, loss = 1.74535245\n",
      "Iteration 70, loss = 1.73243433\n",
      "Iteration 71, loss = 1.71939981\n",
      "Iteration 72, loss = 1.70685384\n",
      "Iteration 73, loss = 1.69422800\n",
      "Iteration 74, loss = 1.68173166\n",
      "Iteration 75, loss = 1.66934186\n",
      "Iteration 76, loss = 1.65724480\n",
      "Iteration 77, loss = 1.64527177\n",
      "Iteration 78, loss = 1.63340962\n",
      "Iteration 79, loss = 1.62139763\n",
      "Iteration 80, loss = 1.60981526\n",
      "Iteration 81, loss = 1.59823035\n",
      "Iteration 82, loss = 1.58700354\n",
      "Iteration 83, loss = 1.57560274\n",
      "Iteration 84, loss = 1.56454922\n",
      "Iteration 85, loss = 1.55357874\n",
      "Iteration 86, loss = 1.54268222\n",
      "Iteration 87, loss = 1.53199018\n",
      "Iteration 88, loss = 1.52130892\n",
      "Iteration 89, loss = 1.51088465\n",
      "Iteration 90, loss = 1.50046351\n",
      "Iteration 91, loss = 1.49035175\n",
      "Iteration 92, loss = 1.48017490\n",
      "Iteration 93, loss = 1.47016432\n",
      "Iteration 94, loss = 1.46032280\n",
      "Iteration 95, loss = 1.45060473\n",
      "Iteration 96, loss = 1.44100874\n",
      "Iteration 97, loss = 1.43162272\n",
      "Iteration 98, loss = 1.42216391\n",
      "Iteration 99, loss = 1.41283705\n",
      "Iteration 100, loss = 1.40378899\n",
      "Iteration 101, loss = 1.39484134\n",
      "Iteration 102, loss = 1.38586745\n",
      "Iteration 103, loss = 1.37715204\n",
      "Iteration 104, loss = 1.36845946\n",
      "Iteration 105, loss = 1.35999335\n",
      "Iteration 106, loss = 1.35132088\n",
      "Iteration 107, loss = 1.34301669\n",
      "Iteration 108, loss = 1.33494389\n",
      "Iteration 109, loss = 1.32671220\n",
      "Iteration 110, loss = 1.31871628\n",
      "Iteration 111, loss = 1.31068232\n",
      "Iteration 112, loss = 1.30285779\n",
      "Iteration 113, loss = 1.29523809\n",
      "Iteration 114, loss = 1.28754085\n",
      "Iteration 115, loss = 1.28004993\n",
      "Iteration 116, loss = 1.27262407\n",
      "Iteration 117, loss = 1.26523095\n",
      "Iteration 118, loss = 1.25792805\n",
      "Iteration 119, loss = 1.25086594\n",
      "Iteration 120, loss = 1.24376824\n",
      "Iteration 121, loss = 1.23672889\n",
      "Iteration 122, loss = 1.22988710\n",
      "Iteration 123, loss = 1.22309436\n",
      "Iteration 124, loss = 1.21640849\n",
      "Iteration 125, loss = 1.20970493\n",
      "Iteration 126, loss = 1.20316838\n",
      "Iteration 127, loss = 1.19659681\n",
      "Iteration 128, loss = 1.19024048\n",
      "Iteration 129, loss = 1.18396844\n",
      "Iteration 130, loss = 1.17764345\n",
      "Iteration 131, loss = 1.17154649\n",
      "Iteration 132, loss = 1.16531119\n",
      "Iteration 133, loss = 1.15931069\n",
      "Iteration 134, loss = 1.15338509\n",
      "Iteration 135, loss = 1.14749715\n",
      "Iteration 136, loss = 1.14164812\n",
      "Iteration 137, loss = 1.13590272\n",
      "Iteration 138, loss = 1.13016641\n",
      "Iteration 139, loss = 1.12457589\n",
      "Iteration 140, loss = 1.11904065\n",
      "Iteration 141, loss = 1.11353041\n",
      "Iteration 142, loss = 1.10804971\n",
      "Iteration 143, loss = 1.10277924\n",
      "Iteration 144, loss = 1.09739940\n",
      "Iteration 145, loss = 1.09205540\n",
      "Iteration 146, loss = 1.08688045\n",
      "Iteration 147, loss = 1.08176993\n",
      "Iteration 148, loss = 1.07667161\n",
      "Iteration 149, loss = 1.07161269\n",
      "Iteration 150, loss = 1.06669568\n",
      "Iteration 151, loss = 1.06172121\n",
      "Iteration 152, loss = 1.05684957\n",
      "Iteration 153, loss = 1.05203068\n",
      "Iteration 154, loss = 1.04731505\n",
      "Iteration 155, loss = 1.04250043\n",
      "Iteration 156, loss = 1.03782538\n",
      "Iteration 157, loss = 1.03317748\n",
      "Iteration 158, loss = 1.02868043\n",
      "Iteration 159, loss = 1.02411906\n",
      "Iteration 160, loss = 1.01955343\n",
      "Iteration 161, loss = 1.01515559\n",
      "Iteration 162, loss = 1.01072369\n",
      "Iteration 163, loss = 1.00626187\n",
      "Iteration 164, loss = 1.00200343\n",
      "Iteration 165, loss = 0.99769859\n",
      "Iteration 166, loss = 0.99343858\n",
      "Iteration 167, loss = 0.98931465\n",
      "Iteration 168, loss = 0.98516805\n",
      "Iteration 169, loss = 0.98087607\n",
      "Iteration 170, loss = 0.97687549\n",
      "Iteration 171, loss = 0.97279190\n",
      "Iteration 172, loss = 0.96874074\n",
      "Iteration 173, loss = 0.96473325\n",
      "Iteration 174, loss = 0.96084168\n",
      "Iteration 175, loss = 0.95690561\n",
      "Iteration 176, loss = 0.95300825\n",
      "Iteration 177, loss = 0.94910593\n",
      "Iteration 178, loss = 0.94533015\n",
      "Iteration 179, loss = 0.94154421\n",
      "Iteration 180, loss = 0.93776388\n",
      "Iteration 181, loss = 0.93398508\n",
      "Iteration 182, loss = 0.93034640\n",
      "Iteration 183, loss = 0.92668113\n",
      "Iteration 184, loss = 0.92304152\n",
      "Iteration 185, loss = 0.91943868\n",
      "Iteration 186, loss = 0.91587672\n",
      "Iteration 187, loss = 0.91232913\n",
      "Iteration 188, loss = 0.90884361\n",
      "Iteration 189, loss = 0.90540755\n",
      "Iteration 190, loss = 0.90183223\n",
      "Iteration 191, loss = 0.89841472\n",
      "Iteration 192, loss = 0.89506538\n",
      "Iteration 193, loss = 0.89167153\n",
      "Iteration 194, loss = 0.88830136\n",
      "Iteration 195, loss = 0.88501114\n",
      "Iteration 196, loss = 0.88169659\n",
      "Iteration 197, loss = 0.87837838\n",
      "Iteration 198, loss = 0.87520583\n",
      "Iteration 199, loss = 0.87187381\n",
      "Iteration 200, loss = 0.86873305\n",
      "Iteration 201, loss = 0.86557981\n",
      "Iteration 202, loss = 0.86239009\n",
      "Iteration 203, loss = 0.85924275\n",
      "Iteration 204, loss = 0.85606775\n",
      "Iteration 205, loss = 0.85306014\n",
      "Iteration 206, loss = 0.84998707\n",
      "Iteration 207, loss = 0.84687868\n",
      "Iteration 208, loss = 0.84383298\n",
      "Iteration 209, loss = 0.84086446\n",
      "Iteration 210, loss = 0.83782323\n",
      "Iteration 211, loss = 0.83487368\n",
      "Iteration 212, loss = 0.83193405\n",
      "Iteration 213, loss = 0.82897028\n",
      "Iteration 214, loss = 0.82609361\n",
      "Iteration 215, loss = 0.82314665\n",
      "Iteration 216, loss = 0.82031463\n",
      "Iteration 217, loss = 0.81742214\n",
      "Iteration 218, loss = 0.81459395\n",
      "Iteration 219, loss = 0.81178109\n",
      "Iteration 220, loss = 0.80895033\n",
      "Iteration 221, loss = 0.80625429\n",
      "Iteration 222, loss = 0.80343670\n",
      "Iteration 223, loss = 0.80068150\n",
      "Iteration 224, loss = 0.79796199\n",
      "Iteration 225, loss = 0.79522025\n",
      "Iteration 226, loss = 0.79252343\n",
      "Iteration 227, loss = 0.78985815\n",
      "Iteration 228, loss = 0.78726233\n",
      "Iteration 229, loss = 0.78458397\n",
      "Iteration 230, loss = 0.78196690\n",
      "Iteration 231, loss = 0.77928971\n",
      "Iteration 232, loss = 0.77672738\n",
      "Iteration 233, loss = 0.77412813\n",
      "Iteration 234, loss = 0.77157725\n",
      "Iteration 235, loss = 0.76900756\n",
      "Iteration 236, loss = 0.76651282\n",
      "Iteration 237, loss = 0.76398748\n",
      "Iteration 238, loss = 0.76145912\n",
      "Iteration 239, loss = 0.75904207\n",
      "Iteration 240, loss = 0.75650688\n",
      "Iteration 241, loss = 0.75406922\n",
      "Iteration 242, loss = 0.75157616\n",
      "Iteration 243, loss = 0.74917648\n",
      "Iteration 244, loss = 0.74676272\n",
      "Iteration 245, loss = 0.74432623\n",
      "Iteration 246, loss = 0.74194371\n",
      "Iteration 247, loss = 0.73956472\n",
      "Iteration 248, loss = 0.73720838\n",
      "Iteration 249, loss = 0.73479644\n",
      "Iteration 250, loss = 0.73250365\n",
      "Iteration 251, loss = 0.73013542\n",
      "Iteration 252, loss = 0.72781484\n",
      "Iteration 253, loss = 0.72552927\n",
      "Iteration 254, loss = 0.72319565\n",
      "Iteration 255, loss = 0.72092472\n",
      "Iteration 256, loss = 0.71863354\n",
      "Iteration 257, loss = 0.71634390\n",
      "Iteration 258, loss = 0.71409250\n",
      "Iteration 259, loss = 0.71189257\n",
      "Iteration 260, loss = 0.70968022\n",
      "Iteration 261, loss = 0.70740617\n",
      "Iteration 262, loss = 0.70524211\n",
      "Iteration 263, loss = 0.70304317\n",
      "Iteration 264, loss = 0.70085712\n",
      "Iteration 265, loss = 0.69870463\n",
      "Iteration 266, loss = 0.69654837\n",
      "Iteration 267, loss = 0.69438434\n",
      "Iteration 268, loss = 0.69224988\n",
      "Iteration 269, loss = 0.69011518\n",
      "Iteration 270, loss = 0.68801854\n",
      "Iteration 271, loss = 0.68589991\n",
      "Iteration 272, loss = 0.68382804\n",
      "Iteration 273, loss = 0.68170258\n",
      "Iteration 274, loss = 0.67966889\n",
      "Iteration 275, loss = 0.67758060\n",
      "Iteration 276, loss = 0.67553470\n",
      "Iteration 277, loss = 0.67350983\n",
      "Iteration 278, loss = 0.67152878\n",
      "Iteration 279, loss = 0.66947841\n",
      "Iteration 280, loss = 0.66747929\n",
      "Iteration 281, loss = 0.66548818\n",
      "Iteration 282, loss = 0.66346900\n",
      "Iteration 283, loss = 0.66148683\n",
      "Iteration 284, loss = 0.65955639\n",
      "Iteration 285, loss = 0.65758806\n",
      "Iteration 286, loss = 0.65562421\n",
      "Iteration 287, loss = 0.65367623\n",
      "Iteration 288, loss = 0.65176159\n",
      "Iteration 289, loss = 0.64982261\n",
      "Iteration 290, loss = 0.64788626\n",
      "Iteration 291, loss = 0.64599357\n",
      "Iteration 292, loss = 0.64410503\n",
      "Iteration 293, loss = 0.64221591\n",
      "Iteration 294, loss = 0.64038887\n",
      "Iteration 295, loss = 0.63841456\n",
      "Iteration 296, loss = 0.63658220\n",
      "Iteration 297, loss = 0.63473080\n",
      "Iteration 298, loss = 0.63289425\n",
      "Iteration 299, loss = 0.63103343\n",
      "Iteration 300, loss = 0.62924279\n",
      "Iteration 301, loss = 0.62743939\n",
      "Iteration 302, loss = 0.62557178\n",
      "Iteration 303, loss = 0.62377393\n",
      "Iteration 304, loss = 0.62199466\n",
      "Iteration 305, loss = 0.62022686\n",
      "Iteration 306, loss = 0.61841206\n",
      "Iteration 307, loss = 0.61667128\n",
      "Iteration 308, loss = 0.61489886\n",
      "Iteration 309, loss = 0.61312620\n",
      "Iteration 310, loss = 0.61136154\n",
      "Iteration 311, loss = 0.60963228\n",
      "Iteration 312, loss = 0.60791228\n",
      "Iteration 313, loss = 0.60615800\n",
      "Iteration 314, loss = 0.60445002\n",
      "Iteration 315, loss = 0.60274060\n",
      "Iteration 316, loss = 0.60103688\n",
      "Iteration 317, loss = 0.59934126\n",
      "Iteration 318, loss = 0.59764766\n",
      "Iteration 319, loss = 0.59592793\n",
      "Iteration 320, loss = 0.59422808\n",
      "Iteration 321, loss = 0.59260720\n",
      "Iteration 322, loss = 0.59089370\n",
      "Iteration 323, loss = 0.58929661\n",
      "Iteration 324, loss = 0.58757733\n",
      "Iteration 325, loss = 0.58597057\n",
      "Iteration 326, loss = 0.58433159\n",
      "Iteration 327, loss = 0.58269979\n",
      "Iteration 328, loss = 0.58107937\n",
      "Iteration 329, loss = 0.57944674\n",
      "Iteration 330, loss = 0.57782455\n",
      "Iteration 331, loss = 0.57625269\n",
      "Iteration 332, loss = 0.57462337\n",
      "Iteration 333, loss = 0.57305703\n",
      "Iteration 334, loss = 0.57142976\n",
      "Iteration 335, loss = 0.56987966\n",
      "Iteration 336, loss = 0.56828766\n",
      "Iteration 337, loss = 0.56675272\n",
      "Iteration 338, loss = 0.56514479\n",
      "Iteration 339, loss = 0.56363142\n",
      "Iteration 340, loss = 0.56206445\n",
      "Iteration 341, loss = 0.56051791\n",
      "Iteration 342, loss = 0.55898453\n",
      "Iteration 343, loss = 0.55746101\n",
      "Iteration 344, loss = 0.55593632\n",
      "Iteration 345, loss = 0.55440889\n",
      "Iteration 346, loss = 0.55289995\n",
      "Iteration 347, loss = 0.55139931\n",
      "Iteration 348, loss = 0.54989338\n",
      "Iteration 349, loss = 0.54841053\n",
      "Iteration 350, loss = 0.54690059\n",
      "Iteration 351, loss = 0.54543256\n",
      "Iteration 352, loss = 0.54394908\n",
      "Iteration 353, loss = 0.54248971\n",
      "Iteration 354, loss = 0.54103154\n",
      "Iteration 355, loss = 0.53959755\n",
      "Iteration 356, loss = 0.53807859\n",
      "Iteration 357, loss = 0.53664375\n",
      "Iteration 358, loss = 0.53522061\n",
      "Iteration 359, loss = 0.53378092\n",
      "Iteration 360, loss = 0.53232986\n",
      "Iteration 361, loss = 0.53090979\n",
      "Iteration 362, loss = 0.52945247\n",
      "Iteration 363, loss = 0.52801785\n",
      "Iteration 364, loss = 0.52665859\n",
      "Iteration 365, loss = 0.52518671\n",
      "Iteration 366, loss = 0.52377745\n",
      "Iteration 367, loss = 0.52241076\n",
      "Iteration 368, loss = 0.52099277\n",
      "Iteration 369, loss = 0.51957898\n",
      "Iteration 370, loss = 0.51819403\n",
      "Iteration 371, loss = 0.51682196\n",
      "Iteration 372, loss = 0.51544537\n",
      "Iteration 373, loss = 0.51408558\n",
      "Iteration 374, loss = 0.51270551\n",
      "Iteration 375, loss = 0.51134873\n",
      "Iteration 376, loss = 0.50997980\n",
      "Iteration 377, loss = 0.50862148\n",
      "Iteration 378, loss = 0.50729386\n",
      "Iteration 379, loss = 0.50592779\n",
      "Iteration 380, loss = 0.50458901\n",
      "Iteration 381, loss = 0.50328009\n",
      "Iteration 382, loss = 0.50191849\n",
      "Iteration 383, loss = 0.50061634\n",
      "Iteration 384, loss = 0.49928126\n",
      "Iteration 385, loss = 0.49796400\n",
      "Iteration 386, loss = 0.49662529\n",
      "Iteration 387, loss = 0.49533404\n",
      "Iteration 388, loss = 0.49405268\n",
      "Iteration 389, loss = 0.49271732\n",
      "Iteration 390, loss = 0.49144687\n",
      "Iteration 391, loss = 0.49014030\n",
      "Iteration 392, loss = 0.48882519\n",
      "Iteration 393, loss = 0.48755921\n",
      "Iteration 394, loss = 0.48628918\n",
      "Iteration 395, loss = 0.48497798\n",
      "Iteration 396, loss = 0.48373256\n",
      "Iteration 397, loss = 0.48245514\n",
      "Iteration 398, loss = 0.48119850\n",
      "Iteration 399, loss = 0.47994900\n",
      "Iteration 400, loss = 0.47867420\n",
      "Iteration 401, loss = 0.47742873\n",
      "Iteration 402, loss = 0.47619631\n",
      "Iteration 403, loss = 0.47496251\n",
      "Iteration 404, loss = 0.47369372\n",
      "Iteration 405, loss = 0.47245220\n",
      "Iteration 406, loss = 0.47124340\n",
      "Iteration 407, loss = 0.47001026\n",
      "Iteration 408, loss = 0.46878672\n",
      "Iteration 409, loss = 0.46758312\n",
      "Iteration 410, loss = 0.46633839\n",
      "Iteration 411, loss = 0.46512996\n",
      "Iteration 412, loss = 0.46394576\n",
      "Iteration 413, loss = 0.46277236\n",
      "Iteration 414, loss = 0.46152785\n",
      "Iteration 415, loss = 0.46036103\n",
      "Iteration 416, loss = 0.45916918\n",
      "Iteration 417, loss = 0.45798912\n",
      "Iteration 418, loss = 0.45679244\n",
      "Iteration 419, loss = 0.45563605\n",
      "Iteration 420, loss = 0.45446253\n",
      "Iteration 421, loss = 0.45327970\n",
      "Iteration 422, loss = 0.45211696\n",
      "Iteration 423, loss = 0.45097318\n",
      "Iteration 424, loss = 0.44981580\n",
      "Iteration 425, loss = 0.44866481\n",
      "Iteration 426, loss = 0.44749484\n",
      "Iteration 427, loss = 0.44636198\n",
      "Iteration 428, loss = 0.44520666\n",
      "Iteration 429, loss = 0.44411214\n",
      "Iteration 430, loss = 0.44296225\n",
      "Iteration 431, loss = 0.44179463\n",
      "Iteration 432, loss = 0.44068429\n",
      "Iteration 433, loss = 0.43954056\n",
      "Iteration 434, loss = 0.43845298\n",
      "Iteration 435, loss = 0.43731081\n",
      "Iteration 436, loss = 0.43618724\n",
      "Iteration 437, loss = 0.43507975\n",
      "Iteration 438, loss = 0.43395653\n",
      "Iteration 439, loss = 0.43287604\n",
      "Iteration 440, loss = 0.43176696\n",
      "Iteration 441, loss = 0.43066018\n",
      "Iteration 442, loss = 0.42956092\n",
      "Iteration 443, loss = 0.42845850\n",
      "Iteration 444, loss = 0.42738208\n",
      "Iteration 445, loss = 0.42628492\n",
      "Iteration 446, loss = 0.42520909\n",
      "Iteration 447, loss = 0.42412449\n",
      "Iteration 448, loss = 0.42306266\n",
      "Iteration 449, loss = 0.42197220\n",
      "Iteration 450, loss = 0.42090884\n",
      "Iteration 451, loss = 0.41983120\n",
      "Iteration 452, loss = 0.41879570\n",
      "Iteration 453, loss = 0.41772829\n",
      "Iteration 454, loss = 0.41664755\n",
      "Iteration 455, loss = 0.41560557\n",
      "Iteration 456, loss = 0.41456515\n",
      "Iteration 457, loss = 0.41352380\n",
      "Iteration 458, loss = 0.41248765\n",
      "Iteration 459, loss = 0.41142274\n",
      "Iteration 460, loss = 0.41039569\n",
      "Iteration 461, loss = 0.40935765\n",
      "Iteration 462, loss = 0.40831997\n",
      "Iteration 463, loss = 0.40731072\n",
      "Iteration 464, loss = 0.40625992\n",
      "Iteration 465, loss = 0.40526051\n",
      "Iteration 466, loss = 0.40424686\n",
      "Iteration 467, loss = 0.40323602\n",
      "Iteration 468, loss = 0.40220996\n",
      "Iteration 469, loss = 0.40121716\n",
      "Iteration 470, loss = 0.40020988\n",
      "Iteration 471, loss = 0.39917723\n",
      "Iteration 472, loss = 0.39819047\n",
      "Iteration 473, loss = 0.39720897\n",
      "Iteration 474, loss = 0.39622990\n",
      "Iteration 475, loss = 0.39520303\n",
      "Iteration 476, loss = 0.39422223\n",
      "Iteration 477, loss = 0.39323772\n",
      "Iteration 478, loss = 0.39225667\n",
      "Iteration 479, loss = 0.39129324\n",
      "Iteration 480, loss = 0.39027240\n",
      "Iteration 481, loss = 0.38933112\n",
      "Iteration 482, loss = 0.38834681\n",
      "Iteration 483, loss = 0.38735433\n",
      "Iteration 484, loss = 0.38640107\n",
      "Iteration 485, loss = 0.38545380\n",
      "Iteration 486, loss = 0.38447022\n",
      "Iteration 487, loss = 0.38352194\n",
      "Iteration 488, loss = 0.38256257\n",
      "Iteration 489, loss = 0.38163050\n",
      "Iteration 490, loss = 0.38066736\n",
      "Iteration 491, loss = 0.37972696\n",
      "Iteration 492, loss = 0.37878225\n",
      "Iteration 493, loss = 0.37784867\n",
      "Iteration 494, loss = 0.37692540\n",
      "Iteration 495, loss = 0.37598953\n",
      "Iteration 496, loss = 0.37503653\n",
      "Iteration 497, loss = 0.37412097\n",
      "Iteration 498, loss = 0.37319992\n",
      "Iteration 499, loss = 0.37228574\n",
      "Iteration 500, loss = 0.37138590\n",
      "Iteration 501, loss = 0.37045305\n",
      "Iteration 502, loss = 0.36954779\n",
      "Iteration 503, loss = 0.36863187\n",
      "Iteration 504, loss = 0.36773005\n",
      "Iteration 505, loss = 0.36682394\n",
      "Iteration 506, loss = 0.36592164\n",
      "Iteration 507, loss = 0.36505209\n",
      "Iteration 508, loss = 0.36415156\n",
      "Iteration 509, loss = 0.36324072\n",
      "Iteration 510, loss = 0.36235096\n",
      "Iteration 511, loss = 0.36149585\n",
      "Iteration 512, loss = 0.36059807\n",
      "Iteration 513, loss = 0.35972832\n",
      "Iteration 514, loss = 0.35883696\n",
      "Iteration 515, loss = 0.35798529\n",
      "Iteration 516, loss = 0.35707368\n",
      "Iteration 517, loss = 0.35622876\n",
      "Iteration 518, loss = 0.35534662\n",
      "Iteration 519, loss = 0.35448567\n",
      "Iteration 520, loss = 0.35361141\n",
      "Iteration 521, loss = 0.35277061\n",
      "Iteration 522, loss = 0.35189384\n",
      "Iteration 523, loss = 0.35102813\n",
      "Iteration 524, loss = 0.35020213\n",
      "Iteration 525, loss = 0.34931831\n",
      "Iteration 526, loss = 0.34844583\n",
      "Iteration 527, loss = 0.34764368\n",
      "Iteration 528, loss = 0.34677611\n",
      "Iteration 529, loss = 0.34592407\n",
      "Iteration 530, loss = 0.34506021\n",
      "Iteration 531, loss = 0.34423151\n",
      "Iteration 532, loss = 0.34341124\n",
      "Iteration 533, loss = 0.34255431\n",
      "Iteration 534, loss = 0.34174208\n",
      "Iteration 535, loss = 0.34090834\n",
      "Iteration 536, loss = 0.34009769\n",
      "Iteration 537, loss = 0.33924775\n",
      "Iteration 538, loss = 0.33842449\n",
      "Iteration 539, loss = 0.33763568\n",
      "Iteration 540, loss = 0.33679079\n",
      "Iteration 541, loss = 0.33600000\n",
      "Iteration 542, loss = 0.33517948\n",
      "Iteration 543, loss = 0.33437528\n",
      "Iteration 544, loss = 0.33357896\n",
      "Iteration 545, loss = 0.33278234\n",
      "Iteration 546, loss = 0.33196291\n",
      "Iteration 547, loss = 0.33118388\n",
      "Iteration 548, loss = 0.33038418\n",
      "Iteration 549, loss = 0.32959865\n",
      "Iteration 550, loss = 0.32879654\n",
      "Iteration 551, loss = 0.32801006\n",
      "Iteration 552, loss = 0.32722544\n",
      "Iteration 553, loss = 0.32642832\n",
      "Iteration 554, loss = 0.32566114\n",
      "Iteration 555, loss = 0.32489193\n",
      "Iteration 556, loss = 0.32408845\n",
      "Iteration 557, loss = 0.32332607\n",
      "Iteration 558, loss = 0.32259200\n",
      "Iteration 559, loss = 0.32177724\n",
      "Iteration 560, loss = 0.32104953\n",
      "Iteration 561, loss = 0.32027891\n",
      "Iteration 562, loss = 0.31950093\n",
      "Iteration 563, loss = 0.31875973\n",
      "Iteration 564, loss = 0.31799729\n",
      "Iteration 565, loss = 0.31724580\n",
      "Iteration 566, loss = 0.31648367\n",
      "Iteration 567, loss = 0.31574336\n",
      "Iteration 568, loss = 0.31500471\n",
      "Iteration 569, loss = 0.31425671\n",
      "Iteration 570, loss = 0.31348027\n",
      "Iteration 571, loss = 0.31274784\n",
      "Iteration 572, loss = 0.31200864\n",
      "Iteration 573, loss = 0.31126956\n",
      "Iteration 574, loss = 0.31051260\n",
      "Iteration 575, loss = 0.30981636\n",
      "Iteration 576, loss = 0.30908418\n",
      "Iteration 577, loss = 0.30834351\n",
      "Iteration 578, loss = 0.30762273\n",
      "Iteration 579, loss = 0.30690068\n",
      "Iteration 580, loss = 0.30616800\n",
      "Iteration 581, loss = 0.30544825\n",
      "Iteration 582, loss = 0.30471980\n",
      "Iteration 583, loss = 0.30400768\n",
      "Iteration 584, loss = 0.30329858\n",
      "Iteration 585, loss = 0.30258843\n",
      "Iteration 586, loss = 0.30185941\n",
      "Iteration 587, loss = 0.30117385\n",
      "Iteration 588, loss = 0.30043970\n",
      "Iteration 589, loss = 0.29975853\n",
      "Iteration 590, loss = 0.29904539\n",
      "Iteration 591, loss = 0.29835138\n",
      "Iteration 592, loss = 0.29765030\n",
      "Iteration 593, loss = 0.29696664\n",
      "Iteration 594, loss = 0.29627154\n",
      "Iteration 595, loss = 0.29554739\n",
      "Iteration 596, loss = 0.29489027\n",
      "Iteration 597, loss = 0.29418622\n",
      "Iteration 598, loss = 0.29349716\n",
      "Iteration 599, loss = 0.29283278\n",
      "Iteration 600, loss = 0.29214499\n",
      "Iteration 601, loss = 0.29146197\n",
      "Iteration 602, loss = 0.29078941\n",
      "Iteration 603, loss = 0.29010075\n",
      "Iteration 604, loss = 0.28944292\n",
      "Iteration 605, loss = 0.28875755\n",
      "Iteration 606, loss = 0.28808680\n",
      "Iteration 607, loss = 0.28742946\n",
      "Iteration 608, loss = 0.28678020\n",
      "Iteration 609, loss = 0.28608881\n",
      "Iteration 610, loss = 0.28542556\n",
      "Iteration 611, loss = 0.28477085\n",
      "Iteration 612, loss = 0.28412536\n",
      "Iteration 613, loss = 0.28344880\n",
      "Iteration 614, loss = 0.28278436\n",
      "Iteration 615, loss = 0.28215333\n",
      "Iteration 616, loss = 0.28149532\n",
      "Iteration 617, loss = 0.28084992\n",
      "Iteration 618, loss = 0.28020689\n",
      "Iteration 619, loss = 0.27957178\n",
      "Iteration 620, loss = 0.27890898\n",
      "Iteration 621, loss = 0.27828121\n",
      "Iteration 622, loss = 0.27762602\n",
      "Iteration 623, loss = 0.27699865\n",
      "Iteration 624, loss = 0.27635065\n",
      "Iteration 625, loss = 0.27574319\n",
      "Iteration 626, loss = 0.27510509\n",
      "Iteration 627, loss = 0.27447103\n",
      "Iteration 628, loss = 0.27383311\n",
      "Iteration 629, loss = 0.27322995\n",
      "Iteration 630, loss = 0.27258602\n",
      "Iteration 631, loss = 0.27198498\n",
      "Iteration 632, loss = 0.27135293\n",
      "Iteration 633, loss = 0.27073438\n",
      "Iteration 634, loss = 0.27013827\n",
      "Iteration 635, loss = 0.26951402\n",
      "Iteration 636, loss = 0.26889687\n",
      "Iteration 637, loss = 0.26829998\n",
      "Iteration 638, loss = 0.26767639\n",
      "Iteration 639, loss = 0.26708345\n",
      "Iteration 640, loss = 0.26646730\n",
      "Iteration 641, loss = 0.26586339\n",
      "Iteration 642, loss = 0.26526872\n",
      "Iteration 643, loss = 0.26464574\n",
      "Iteration 644, loss = 0.26406233\n",
      "Iteration 645, loss = 0.26346334\n",
      "Iteration 646, loss = 0.26286108\n",
      "Iteration 647, loss = 0.26227398\n",
      "Iteration 648, loss = 0.26168964\n",
      "Iteration 649, loss = 0.26107509\n",
      "Iteration 650, loss = 0.26049802\n",
      "Iteration 651, loss = 0.25991061\n",
      "Iteration 652, loss = 0.25934545\n",
      "Iteration 653, loss = 0.25873212\n",
      "Iteration 654, loss = 0.25815112\n",
      "Iteration 655, loss = 0.25758567\n",
      "Iteration 656, loss = 0.25699790\n",
      "Iteration 657, loss = 0.25641585\n",
      "Iteration 658, loss = 0.25585852\n",
      "Iteration 659, loss = 0.25528360\n",
      "Iteration 660, loss = 0.25471572\n",
      "Iteration 661, loss = 0.25414245\n",
      "Iteration 662, loss = 0.25359871\n",
      "Iteration 663, loss = 0.25301576\n",
      "Iteration 664, loss = 0.25244987\n",
      "Iteration 665, loss = 0.25188783\n",
      "Iteration 666, loss = 0.25133944\n",
      "Iteration 667, loss = 0.25077135\n",
      "Iteration 668, loss = 0.25022355\n",
      "Iteration 669, loss = 0.24966594\n",
      "Iteration 670, loss = 0.24910039\n",
      "Iteration 671, loss = 0.24854546\n",
      "Iteration 672, loss = 0.24800556\n",
      "Iteration 673, loss = 0.24744195\n",
      "Iteration 674, loss = 0.24690633\n",
      "Iteration 675, loss = 0.24635920\n",
      "Iteration 676, loss = 0.24579983\n",
      "Iteration 677, loss = 0.24525678\n",
      "Iteration 678, loss = 0.24472382\n",
      "Iteration 679, loss = 0.24416903\n",
      "Iteration 680, loss = 0.24363856\n",
      "Iteration 681, loss = 0.24311213\n",
      "Iteration 682, loss = 0.24256608\n",
      "Iteration 683, loss = 0.24202401\n",
      "Iteration 684, loss = 0.24150881\n",
      "Iteration 685, loss = 0.24097293\n",
      "Iteration 686, loss = 0.24043912\n",
      "Iteration 687, loss = 0.23991032\n",
      "Iteration 688, loss = 0.23937574\n",
      "Iteration 689, loss = 0.23887095\n",
      "Iteration 690, loss = 0.23832614\n",
      "Iteration 691, loss = 0.23782249\n",
      "Iteration 692, loss = 0.23730345\n",
      "Iteration 693, loss = 0.23676662\n",
      "Iteration 694, loss = 0.23626857\n",
      "Iteration 695, loss = 0.23574273\n",
      "Iteration 696, loss = 0.23523210\n",
      "Iteration 697, loss = 0.23471707\n",
      "Iteration 698, loss = 0.23420579\n",
      "Iteration 699, loss = 0.23370562\n",
      "Iteration 700, loss = 0.23320011\n",
      "Iteration 701, loss = 0.23268332\n",
      "Iteration 702, loss = 0.23216935\n",
      "Iteration 703, loss = 0.23167645\n",
      "Iteration 704, loss = 0.23117500\n",
      "Iteration 705, loss = 0.23067472\n",
      "Iteration 706, loss = 0.23018076\n",
      "Iteration 707, loss = 0.22966681\n",
      "Iteration 708, loss = 0.22917720\n",
      "Iteration 709, loss = 0.22867530\n",
      "Iteration 710, loss = 0.22818842\n",
      "Iteration 711, loss = 0.22769646\n",
      "Iteration 712, loss = 0.22719940\n",
      "Iteration 713, loss = 0.22672726\n",
      "Iteration 714, loss = 0.22623630\n",
      "Iteration 715, loss = 0.22573533\n",
      "Iteration 716, loss = 0.22525817\n",
      "Iteration 717, loss = 0.22476788\n",
      "Iteration 718, loss = 0.22429491\n",
      "Iteration 719, loss = 0.22380210\n",
      "Iteration 720, loss = 0.22330931\n",
      "Iteration 721, loss = 0.22283741\n",
      "Iteration 722, loss = 0.22236057\n",
      "Iteration 723, loss = 0.22189647\n",
      "Iteration 724, loss = 0.22141278\n",
      "Iteration 725, loss = 0.22093011\n",
      "Iteration 726, loss = 0.22048125\n",
      "Iteration 727, loss = 0.22000412\n",
      "Iteration 728, loss = 0.21952929\n",
      "Iteration 729, loss = 0.21906677\n",
      "Iteration 730, loss = 0.21860127\n",
      "Iteration 731, loss = 0.21814766\n",
      "Iteration 732, loss = 0.21766532\n",
      "Iteration 733, loss = 0.21721860\n",
      "Iteration 734, loss = 0.21674617\n",
      "Iteration 735, loss = 0.21629361\n",
      "Iteration 736, loss = 0.21582450\n",
      "Iteration 737, loss = 0.21537740\n",
      "Iteration 738, loss = 0.21490951\n",
      "Iteration 739, loss = 0.21447638\n",
      "Iteration 740, loss = 0.21402216\n",
      "Iteration 741, loss = 0.21355793\n",
      "Iteration 742, loss = 0.21311047\n",
      "Iteration 743, loss = 0.21264858\n",
      "Iteration 744, loss = 0.21221971\n",
      "Iteration 745, loss = 0.21175831\n",
      "Iteration 746, loss = 0.21131234\n",
      "Iteration 747, loss = 0.21086647\n",
      "Iteration 748, loss = 0.21042390\n",
      "Iteration 749, loss = 0.20997798\n",
      "Iteration 750, loss = 0.20953599\n",
      "Iteration 751, loss = 0.20911331\n",
      "Iteration 752, loss = 0.20865042\n",
      "Iteration 753, loss = 0.20821277\n",
      "Iteration 754, loss = 0.20777201\n",
      "Iteration 755, loss = 0.20735649\n",
      "Iteration 756, loss = 0.20690978\n",
      "Iteration 757, loss = 0.20648202\n",
      "Iteration 758, loss = 0.20605027\n",
      "Iteration 759, loss = 0.20563234\n",
      "Iteration 760, loss = 0.20520309\n",
      "Iteration 761, loss = 0.20477031\n",
      "Iteration 762, loss = 0.20434034\n",
      "Iteration 763, loss = 0.20392100\n",
      "Iteration 764, loss = 0.20350259\n",
      "Iteration 765, loss = 0.20308455\n",
      "Iteration 766, loss = 0.20266099\n",
      "Iteration 767, loss = 0.20225967\n",
      "Iteration 768, loss = 0.20181703\n",
      "Iteration 769, loss = 0.20142117\n",
      "Iteration 770, loss = 0.20099870\n",
      "Iteration 771, loss = 0.20057934\n",
      "Iteration 772, loss = 0.20016973\n",
      "Iteration 773, loss = 0.19975584\n",
      "Iteration 774, loss = 0.19935186\n",
      "Iteration 775, loss = 0.19894731\n",
      "Iteration 776, loss = 0.19854038\n",
      "Iteration 777, loss = 0.19814401\n",
      "Iteration 778, loss = 0.19772296\n",
      "Iteration 779, loss = 0.19732698\n",
      "Iteration 780, loss = 0.19693505\n",
      "Iteration 781, loss = 0.19652227\n",
      "Iteration 782, loss = 0.19612218\n",
      "Iteration 783, loss = 0.19572089\n",
      "Iteration 784, loss = 0.19532204\n",
      "Iteration 785, loss = 0.19491244\n",
      "Iteration 786, loss = 0.19452963\n",
      "Iteration 787, loss = 0.19412449\n",
      "Iteration 788, loss = 0.19373751\n",
      "Iteration 789, loss = 0.19333770\n",
      "Iteration 790, loss = 0.19296073\n",
      "Iteration 791, loss = 0.19256589\n",
      "Iteration 792, loss = 0.19214246\n",
      "Iteration 793, loss = 0.19177386\n",
      "Iteration 794, loss = 0.19138315\n",
      "Iteration 795, loss = 0.19099572\n",
      "Iteration 796, loss = 0.19061295\n",
      "Iteration 797, loss = 0.19021964\n",
      "Iteration 798, loss = 0.18984560\n",
      "Iteration 799, loss = 0.18944252\n",
      "Iteration 800, loss = 0.18904861\n",
      "Iteration 801, loss = 0.18867088\n",
      "Iteration 802, loss = 0.18828970\n",
      "Iteration 803, loss = 0.18791866\n",
      "Iteration 804, loss = 0.18752084\n",
      "Iteration 805, loss = 0.18714741\n",
      "Iteration 806, loss = 0.18677985\n",
      "Iteration 807, loss = 0.18639225\n",
      "Iteration 808, loss = 0.18600914\n",
      "Iteration 809, loss = 0.18564872\n",
      "Iteration 810, loss = 0.18526482\n",
      "Iteration 811, loss = 0.18489651\n",
      "Iteration 812, loss = 0.18453140\n",
      "Iteration 813, loss = 0.18416276\n",
      "Iteration 814, loss = 0.18378377\n",
      "Iteration 815, loss = 0.18342368\n",
      "Iteration 816, loss = 0.18306478\n",
      "Iteration 817, loss = 0.18269009\n",
      "Iteration 818, loss = 0.18233257\n",
      "Iteration 819, loss = 0.18197509\n",
      "Iteration 820, loss = 0.18160493\n",
      "Iteration 821, loss = 0.18125209\n",
      "Iteration 822, loss = 0.18088913\n",
      "Iteration 823, loss = 0.18052726\n",
      "Iteration 824, loss = 0.18018532\n",
      "Iteration 825, loss = 0.17981220\n",
      "Iteration 826, loss = 0.17946166\n",
      "Iteration 827, loss = 0.17911563\n",
      "Iteration 828, loss = 0.17875132\n",
      "Iteration 829, loss = 0.17838869\n",
      "Iteration 830, loss = 0.17804459\n",
      "Iteration 831, loss = 0.17769559\n",
      "Iteration 832, loss = 0.17733901\n",
      "Iteration 833, loss = 0.17699751\n",
      "Iteration 834, loss = 0.17664657\n",
      "Iteration 835, loss = 0.17629972\n",
      "Iteration 836, loss = 0.17594509\n",
      "Iteration 837, loss = 0.17559734\n",
      "Iteration 838, loss = 0.17525597\n",
      "Iteration 839, loss = 0.17492535\n",
      "Iteration 840, loss = 0.17457029\n",
      "Iteration 841, loss = 0.17422903\n",
      "Iteration 842, loss = 0.17389080\n",
      "Iteration 843, loss = 0.17354590\n",
      "Iteration 844, loss = 0.17321729\n",
      "Iteration 845, loss = 0.17287793\n",
      "Iteration 846, loss = 0.17253813\n",
      "Iteration 847, loss = 0.17219158\n",
      "Iteration 848, loss = 0.17185413\n",
      "Iteration 849, loss = 0.17152181\n",
      "Iteration 850, loss = 0.17119065\n",
      "Iteration 851, loss = 0.17084747\n",
      "Iteration 852, loss = 0.17053042\n",
      "Iteration 853, loss = 0.17018537\n",
      "Iteration 854, loss = 0.16984888\n",
      "Iteration 855, loss = 0.16952168\n",
      "Iteration 856, loss = 0.16919404\n",
      "Iteration 857, loss = 0.16886039\n",
      "Iteration 858, loss = 0.16853779\n",
      "Iteration 859, loss = 0.16820723\n",
      "Iteration 860, loss = 0.16788621\n",
      "Iteration 861, loss = 0.16756959\n",
      "Iteration 862, loss = 0.16722980\n",
      "Iteration 863, loss = 0.16691558\n",
      "Iteration 864, loss = 0.16658683\n",
      "Iteration 865, loss = 0.16626918\n",
      "Iteration 866, loss = 0.16595923\n",
      "Iteration 867, loss = 0.16563686\n",
      "Iteration 868, loss = 0.16531759\n",
      "Iteration 869, loss = 0.16500729\n",
      "Iteration 870, loss = 0.16469212\n",
      "Iteration 871, loss = 0.16437916\n",
      "Iteration 872, loss = 0.16406458\n",
      "Iteration 873, loss = 0.16374947\n",
      "Iteration 874, loss = 0.16343507\n",
      "Iteration 875, loss = 0.16312277\n",
      "Iteration 876, loss = 0.16282590\n",
      "Iteration 877, loss = 0.16251094\n",
      "Iteration 878, loss = 0.16219013\n",
      "Iteration 879, loss = 0.16189102\n",
      "Iteration 880, loss = 0.16157889\n",
      "Iteration 881, loss = 0.16127612\n",
      "Iteration 882, loss = 0.16096251\n",
      "Iteration 883, loss = 0.16066233\n",
      "Iteration 884, loss = 0.16035574\n",
      "Iteration 885, loss = 0.16003776\n",
      "Iteration 886, loss = 0.15974129\n",
      "Iteration 887, loss = 0.15943614\n",
      "Iteration 888, loss = 0.15913202\n",
      "Iteration 889, loss = 0.15883855\n",
      "Iteration 890, loss = 0.15853525\n",
      "Iteration 891, loss = 0.15822945\n",
      "Iteration 892, loss = 0.15793128\n",
      "Iteration 893, loss = 0.15763591\n",
      "Iteration 894, loss = 0.15734106\n",
      "Iteration 895, loss = 0.15704520\n",
      "Iteration 896, loss = 0.15674478\n",
      "Iteration 897, loss = 0.15644465\n",
      "Iteration 898, loss = 0.15616412\n",
      "Iteration 899, loss = 0.15587160\n",
      "Iteration 900, loss = 0.15557156\n",
      "Iteration 901, loss = 0.15527986\n",
      "Iteration 902, loss = 0.15498035\n",
      "Iteration 903, loss = 0.15470749\n",
      "Iteration 904, loss = 0.15441276\n",
      "Iteration 905, loss = 0.15413317\n",
      "Iteration 906, loss = 0.15382491\n",
      "Iteration 907, loss = 0.15354845\n",
      "Iteration 908, loss = 0.15326182\n",
      "Iteration 909, loss = 0.15297181\n",
      "Iteration 910, loss = 0.15269115\n",
      "Iteration 911, loss = 0.15240554\n",
      "Iteration 912, loss = 0.15213062\n",
      "Iteration 913, loss = 0.15183991\n",
      "Iteration 914, loss = 0.15155529\n",
      "Iteration 915, loss = 0.15128035\n",
      "Iteration 916, loss = 0.15100268\n",
      "Iteration 917, loss = 0.15071950\n",
      "Iteration 918, loss = 0.15043638\n",
      "Iteration 919, loss = 0.15016964\n",
      "Iteration 920, loss = 0.14990188\n",
      "Iteration 921, loss = 0.14961112\n",
      "Iteration 922, loss = 0.14933650\n",
      "Iteration 923, loss = 0.14906314\n",
      "Iteration 924, loss = 0.14879187\n",
      "Iteration 925, loss = 0.14851469\n",
      "Iteration 926, loss = 0.14824298\n",
      "Iteration 927, loss = 0.14796834\n",
      "Iteration 928, loss = 0.14769694\n",
      "Iteration 929, loss = 0.14743252\n",
      "Iteration 930, loss = 0.14717164\n",
      "Iteration 931, loss = 0.14688729\n",
      "Iteration 932, loss = 0.14661852\n",
      "Iteration 933, loss = 0.14635649\n",
      "Iteration 934, loss = 0.14610158\n",
      "Iteration 935, loss = 0.14584036\n",
      "Iteration 936, loss = 0.14556953\n",
      "Iteration 937, loss = 0.14528970\n",
      "Iteration 938, loss = 0.14503908\n",
      "Iteration 939, loss = 0.14477795\n",
      "Iteration 940, loss = 0.14451479\n",
      "Iteration 941, loss = 0.14425786\n",
      "Iteration 942, loss = 0.14399469\n",
      "Iteration 943, loss = 0.14373973\n",
      "Iteration 944, loss = 0.14348182\n",
      "Iteration 945, loss = 0.14321660\n",
      "Iteration 946, loss = 0.14297016\n",
      "Iteration 947, loss = 0.14269802\n",
      "Iteration 948, loss = 0.14244951\n",
      "Iteration 949, loss = 0.14219665\n",
      "Iteration 950, loss = 0.14193062\n",
      "Iteration 951, loss = 0.14167436\n",
      "Iteration 952, loss = 0.14142862\n",
      "Iteration 953, loss = 0.14117587\n",
      "Iteration 954, loss = 0.14090952\n",
      "Iteration 955, loss = 0.14067527\n",
      "Iteration 956, loss = 0.14041642\n",
      "Iteration 957, loss = 0.14016525\n",
      "Iteration 958, loss = 0.13991118\n",
      "Iteration 959, loss = 0.13965911\n",
      "Iteration 960, loss = 0.13941415\n",
      "Iteration 961, loss = 0.13916668\n",
      "Iteration 962, loss = 0.13891226\n",
      "Iteration 963, loss = 0.13867137\n",
      "Iteration 964, loss = 0.13842807\n",
      "Iteration 965, loss = 0.13817029\n",
      "Iteration 966, loss = 0.13793259\n",
      "Iteration 967, loss = 0.13767611\n",
      "Iteration 968, loss = 0.13743105\n",
      "Iteration 969, loss = 0.13719310\n",
      "Iteration 970, loss = 0.13694381\n",
      "Iteration 971, loss = 0.13670094\n",
      "Iteration 972, loss = 0.13645908\n",
      "Iteration 973, loss = 0.13621561\n",
      "Iteration 974, loss = 0.13597842\n",
      "Iteration 975, loss = 0.13573278\n",
      "Iteration 976, loss = 0.13549413\n",
      "Iteration 977, loss = 0.13524916\n",
      "Iteration 978, loss = 0.13502126\n",
      "Iteration 979, loss = 0.13476719\n",
      "Iteration 980, loss = 0.13453685\n",
      "Iteration 981, loss = 0.13430023\n",
      "Iteration 982, loss = 0.13406654\n",
      "Iteration 983, loss = 0.13383050\n",
      "Iteration 984, loss = 0.13359335\n",
      "Iteration 985, loss = 0.13336024\n",
      "Iteration 986, loss = 0.13311356\n",
      "Iteration 987, loss = 0.13288158\n",
      "Iteration 988, loss = 0.13265553\n",
      "Iteration 989, loss = 0.13241617\n",
      "Iteration 990, loss = 0.13218064\n",
      "Iteration 991, loss = 0.13195600\n",
      "Iteration 992, loss = 0.13172521\n",
      "Iteration 993, loss = 0.13148876\n",
      "Iteration 994, loss = 0.13125595\n",
      "Iteration 995, loss = 0.13102988\n",
      "Iteration 996, loss = 0.13080397\n",
      "Iteration 997, loss = 0.13056481\n",
      "Iteration 998, loss = 0.13034443\n",
      "Iteration 999, loss = 0.13011223\n",
      "Iteration 1000, loss = 0.12989287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\syeda\\OneDrive\\Documents\\American Bureau of Shipping\\projects\\nlp_risk_prediction\\nlp_env\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;Group&#x27;, TfidfVectorizer(),\n",
       "                                                  &#x27;Group&#x27;),\n",
       "                                                 (&#x27;Symptom&#x27;, TfidfVectorizer(),\n",
       "                                                  &#x27;Symptom&#x27;),\n",
       "                                                 (&#x27;Error_Cause&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;Error_Cause&#x27;),\n",
       "                                                 (&#x27;Cause_Details&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;Cause_Details&#x27;),\n",
       "                                                 (&#x27;Error_Class&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;Error_Class&#x27;),\n",
       "                                                 (&#x27;Discovery&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;Discovery&#x27;),\n",
       "                                                 (&#x27;Completion_Note&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;Completion_Note&#x27;),\n",
       "                                                 (&#x27;Action_Taken&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;Action_Taken&#x27;),\n",
       "                                                 (&#x27;Work_Description&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;Work_Description&#x27;),\n",
       "                                                 (&#x27;Directive&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;Directive&#x27;)])),\n",
       "                (&#x27;scaler&#x27;, MaxAbsScaler()),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 MLPClassifier(activation=&#x27;tanh&#x27;, hidden_layer_sizes=(100, 100),\n",
       "                               max_iter=1000, solver=&#x27;sgd&#x27;, verbose=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;Group&#x27;, TfidfVectorizer(),\n",
       "                                                  &#x27;Group&#x27;),\n",
       "                                                 (&#x27;Symptom&#x27;, TfidfVectorizer(),\n",
       "                                                  &#x27;Symptom&#x27;),\n",
       "                                                 (&#x27;Error_Cause&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;Error_Cause&#x27;),\n",
       "                                                 (&#x27;Cause_Details&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;Cause_Details&#x27;),\n",
       "                                                 (&#x27;Error_Class&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;Error_Class&#x27;),\n",
       "                                                 (&#x27;Discovery&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;Discovery&#x27;),\n",
       "                                                 (&#x27;Completion_Note&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;Completion_Note&#x27;),\n",
       "                                                 (&#x27;Action_Taken&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;Action_Taken&#x27;),\n",
       "                                                 (&#x27;Work_Description&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;Work_Description&#x27;),\n",
       "                                                 (&#x27;Directive&#x27;,\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  &#x27;Directive&#x27;)])),\n",
       "                (&#x27;scaler&#x27;, MaxAbsScaler()),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 MLPClassifier(activation=&#x27;tanh&#x27;, hidden_layer_sizes=(100, 100),\n",
       "                               max_iter=1000, solver=&#x27;sgd&#x27;, verbose=True))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;preprocessor: ColumnTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.compose.ColumnTransformer.html\">?<span>Documentation for preprocessor: ColumnTransformer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>ColumnTransformer(transformers=[(&#x27;Group&#x27;, TfidfVectorizer(), &#x27;Group&#x27;),\n",
       "                                (&#x27;Symptom&#x27;, TfidfVectorizer(), &#x27;Symptom&#x27;),\n",
       "                                (&#x27;Error_Cause&#x27;, TfidfVectorizer(),\n",
       "                                 &#x27;Error_Cause&#x27;),\n",
       "                                (&#x27;Cause_Details&#x27;, TfidfVectorizer(),\n",
       "                                 &#x27;Cause_Details&#x27;),\n",
       "                                (&#x27;Error_Class&#x27;, TfidfVectorizer(),\n",
       "                                 &#x27;Error_Class&#x27;),\n",
       "                                (&#x27;Discovery&#x27;, TfidfVectorizer(), &#x27;Discovery&#x27;),\n",
       "                                (&#x27;Completion_Note&#x27;, TfidfVectorizer(),\n",
       "                                 &#x27;Completion_Note&#x27;),\n",
       "                                (&#x27;Action_Taken&#x27;, TfidfVectorizer(),\n",
       "                                 &#x27;Action_Taken&#x27;),\n",
       "                                (&#x27;Work_Description&#x27;, TfidfVectorizer(),\n",
       "                                 &#x27;Work_Description&#x27;),\n",
       "                                (&#x27;Directive&#x27;, TfidfVectorizer(), &#x27;Directive&#x27;)])</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Group</label><div class=\"sk-toggleable__content fitted\"><pre>Group</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" ><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer()</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" ><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Symptom</label><div class=\"sk-toggleable__content fitted\"><pre>Symptom</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-30\" type=\"checkbox\" ><label for=\"sk-estimator-id-30\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer()</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-31\" type=\"checkbox\" ><label for=\"sk-estimator-id-31\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Error_Cause</label><div class=\"sk-toggleable__content fitted\"><pre>Error_Cause</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-32\" type=\"checkbox\" ><label for=\"sk-estimator-id-32\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer()</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-33\" type=\"checkbox\" ><label for=\"sk-estimator-id-33\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Cause_Details</label><div class=\"sk-toggleable__content fitted\"><pre>Cause_Details</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-34\" type=\"checkbox\" ><label for=\"sk-estimator-id-34\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer()</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-35\" type=\"checkbox\" ><label for=\"sk-estimator-id-35\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Error_Class</label><div class=\"sk-toggleable__content fitted\"><pre>Error_Class</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-36\" type=\"checkbox\" ><label for=\"sk-estimator-id-36\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer()</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-37\" type=\"checkbox\" ><label for=\"sk-estimator-id-37\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Discovery</label><div class=\"sk-toggleable__content fitted\"><pre>Discovery</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-38\" type=\"checkbox\" ><label for=\"sk-estimator-id-38\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer()</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-39\" type=\"checkbox\" ><label for=\"sk-estimator-id-39\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Completion_Note</label><div class=\"sk-toggleable__content fitted\"><pre>Completion_Note</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-40\" type=\"checkbox\" ><label for=\"sk-estimator-id-40\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer()</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-41\" type=\"checkbox\" ><label for=\"sk-estimator-id-41\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Action_Taken</label><div class=\"sk-toggleable__content fitted\"><pre>Action_Taken</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-42\" type=\"checkbox\" ><label for=\"sk-estimator-id-42\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer()</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-43\" type=\"checkbox\" ><label for=\"sk-estimator-id-43\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Work_Description</label><div class=\"sk-toggleable__content fitted\"><pre>Work_Description</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-44\" type=\"checkbox\" ><label for=\"sk-estimator-id-44\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer()</pre></div> </div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-45\" type=\"checkbox\" ><label for=\"sk-estimator-id-45\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">Directive</label><div class=\"sk-toggleable__content fitted\"><pre>Directive</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-46\" type=\"checkbox\" ><label for=\"sk-estimator-id-46\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer()</pre></div> </div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-47\" type=\"checkbox\" ><label for=\"sk-estimator-id-47\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MaxAbsScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.MaxAbsScaler.html\">?<span>Documentation for MaxAbsScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MaxAbsScaler()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-48\" type=\"checkbox\" ><label for=\"sk-estimator-id-48\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, hidden_layer_sizes=(100, 100), max_iter=1000,\n",
       "              solver=&#x27;sgd&#x27;, verbose=True)</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('Group', TfidfVectorizer(),\n",
       "                                                  'Group'),\n",
       "                                                 ('Symptom', TfidfVectorizer(),\n",
       "                                                  'Symptom'),\n",
       "                                                 ('Error_Cause',\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  'Error_Cause'),\n",
       "                                                 ('Cause_Details',\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  'Cause_Details'),\n",
       "                                                 ('Error_Class',\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  'Error_Class'),\n",
       "                                                 ('Discovery',\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  'Discovery'),\n",
       "                                                 ('Completion_Note',\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  'Completion_Note'),\n",
       "                                                 ('Action_Taken',\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  'Action_Taken'),\n",
       "                                                 ('Work_Description',\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  'Work_Description'),\n",
       "                                                 ('Directive',\n",
       "                                                  TfidfVectorizer(),\n",
       "                                                  'Directive')])),\n",
       "                ('scaler', MaxAbsScaler()),\n",
       "                ('classifier',\n",
       "                 MLPClassifier(activation='tanh', hidden_layer_sizes=(100, 100),\n",
       "                               max_iter=1000, solver='sgd', verbose=True))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the TF-IDF vectorizer for text columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (col, TfidfVectorizer(), col) for col in feature_columns\n",
    "    ], remainder='drop'  # Dropping non-specified columns, though all columns are specified here\n",
    ")\n",
    "\n",
    "params = {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'max_iter': 1000, 'solver': 'sgd', 'verbose': True}\n",
    "\n",
    "# Create a pipeline with preprocessor, scaler, and MLPClassifier with fixed hyperparameters\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('scaler', MaxAbsScaler()),  # Apply scaler after TF-IDF transformation\n",
    "    ('classifier', MLPClassifier(**params))\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df[feature_columns]\n",
    "y = df[target_column]\n",
    "\n",
    "# Check if the length of X and y are consistent\n",
    "assert len(X) == len(y), \"Mismatch in the number of samples between X and y\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training samples: \", X_train.shape, \"Testing samples: \", X_test.shape)\n",
    "print(\"Training labels: \", y_train.shape, \"Testing labels: \", y_test.shape)\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.8102189781021898\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.96      1.00      0.98        24\n",
      "           6       0.00      0.00      0.00         2\n",
      "           7       0.64      0.64      0.64        11\n",
      "           8       0.00      0.00      0.00         1\n",
      "           9       0.93      0.93      0.93        29\n",
      "          10       0.00      0.00      0.00         1\n",
      "          11       0.00      0.00      0.00         1\n",
      "          12       0.67      1.00      0.80         4\n",
      "          14       0.00      0.00      0.00         1\n",
      "          15       1.00      0.50      0.67         2\n",
      "          16       1.00      1.00      1.00         1\n",
      "          17       0.60      1.00      0.75         3\n",
      "          18       0.00      0.00      0.00         1\n",
      "          19       0.93      0.93      0.93        28\n",
      "          20       0.00      0.00      0.00         1\n",
      "          21       0.00      0.00      0.00         2\n",
      "          22       0.64      1.00      0.78         7\n",
      "          23       0.00      0.00      0.00         1\n",
      "          24       1.00      1.00      1.00         1\n",
      "          25       0.43      0.75      0.55         4\n",
      "          26       0.83      1.00      0.91         5\n",
      "          27       0.50      1.00      0.67         1\n",
      "          28       0.00      0.00      0.00         2\n",
      "          30       1.00      0.50      0.67         2\n",
      "          33       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.81       137\n",
      "   macro avg       0.43      0.47      0.43       137\n",
      "weighted avg       0.76      0.81      0.78       137\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\syeda\\OneDrive\\Documents\\American Bureau of Shipping\\projects\\nlp_risk_prediction\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\syeda\\OneDrive\\Documents\\American Bureau of Shipping\\projects\\nlp_risk_prediction\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\syeda\\OneDrive\\Documents\\American Bureau of Shipping\\projects\\nlp_risk_prediction\\nlp_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAABVt0lEQVR4nO3deXhTZfo+8Dtpk7Rpm+77xlKgLbSlBYECAsomMAijgwwqi6P4VeE3Mjg6Mo4OuOGG6IiijiNVR0R0BBXZaqEgO4UWaJGy0wJN9zZd07Q5vz/aBmILdEnOSdP7c125ICdvTp48Frh9z3vOkQmCIICIiIjITsilLoCIiIjIkhhuiIiIyK4w3BAREZFdYbghIiIiu8JwQ0RERHaF4YaIiIjsCsMNERER2RWGGyIiIrIrDDdERERkVxhuiIiIyK4w3BDRTSUlJUEmkyEtLU3qUtokIyMDDz74IEJDQ6FSqeDl5YVx48ZhzZo1aGhokLo8IhKBo9QFEBFZyieffILHHnsM/v7+mD17Nvr06YOKigqkpKTg4YcfRl5eHv7+979LXSYRWRnDDRHZhQMHDuCxxx5DYmIiNm/eDDc3N9NrixYtQlpaGjIzMy3yWVVVVXBxcbHIvojI8nhYiogsIj09HZMmTYJGo4GrqyvGjh2LAwcOmI0xGAxYtmwZ+vTpAycnJ3h7e2PkyJFITk42jdFqtXjooYcQEhIClUqFwMBATJs2DRcvXrzp5y9btgwymQxffvmlWbBpNnjwYMybNw8AkJqaCplMhtTUVLMxFy9ehEwmQ1JSkmnbvHnz4OrqinPnzmHy5Mlwc3PDAw88gIULF8LV1RXV1dUtPmvWrFkICAgwOwy2ZcsW3H777XBxcYGbmxumTJmCrKysm34nIuoYhhsi6rSsrCzcfvvtOHbsGJ555hk8//zzuHDhAsaMGYODBw+axi1duhTLli3DHXfcgVWrVuG5555DWFgYjh49ahpz7733YsOGDXjooYfwwQcf4M9//jMqKiqQk5Nzw8+vrq5GSkoKRo0ahbCwMIt/v/r6ekycOBF+fn546623cO+992LmzJmoqqrCTz/91KKWH3/8EX/4wx/g4OAAAPjiiy8wZcoUuLq64vXXX8fzzz+PkydPYuTIkbcMbUTUAQIR0U2sWbNGACAcPnz4hmOmT58uKJVK4dy5c6ZtV69eFdzc3IRRo0aZtsXFxQlTpky54X5KS0sFAMKbb77ZrhqPHTsmABCefPLJNo3fuXOnAEDYuXOn2fYLFy4IAIQ1a9aYts2dO1cAIDz77LNmY41GoxAcHCzce++9ZtvXr18vABB2794tCIIgVFRUCB4eHsL8+fPNxmm1WsHd3b3FdiLqPM7cEFGnNDQ0YPv27Zg+fTp69epl2h4YGIj7778fe/bsgU6nAwB4eHggKysLZ86caXVfzs7OUCqVSE1NRWlpaZtraN5/a4ejLOXxxx83ey6TyTBjxgxs3rwZlZWVpu1ff/01goODMXLkSABAcnIyysrKMGvWLBQVFZkeDg4OGDp0KHbu3Gm1mom6K4YbIuqUwsJCVFdXo1+/fi1ei4qKgtFoRG5uLgDgxRdfRFlZGfr27YuYmBg8/fTTOH78uGm8SqXC66+/ji1btsDf3x+jRo3CG2+8Aa1We9MaNBoNAKCiosKC3+waR0dHhISEtNg+c+ZM1NTU4IcffgAAVFZWYvPmzZgxYwZkMhkAmILcnXfeCV9fX7PH9u3bUVBQYJWaibozhhsiEs2oUaNw7tw5fPrppxgwYAA++eQTJCQk4JNPPjGNWbRoEU6fPo3ly5fDyckJzz//PKKiopCenn7D/UZERMDR0REnTpxoUx3NweO3bnQdHJVKBbm85V+Xw4YNQ48ePbB+/XoAwI8//oiamhrMnDnTNMZoNAJoXHeTnJzc4vH999+3qWYiajuGGyLqFF9fX6jVamRnZ7d47dSpU5DL5QgNDTVt8/LywkMPPYSvvvoKubm5iI2NxdKlS83e17t3bzz11FPYvn07MjMzUVdXhxUrVtywBrVajTvvvBO7d+82zRLdjKenJwCgrKzMbPulS5du+d7fuu+++7B161bodDp8/fXX6NGjB4YNG2b2XQDAz88P48aNa/EYM2ZMuz+TiG6O4YaIOsXBwQETJkzA999/b3bmT35+PtauXYuRI0eaDhsVFxebvdfV1RURERHQ6/UAGs80qq2tNRvTu3dvuLm5mcbcyD//+U8IgoDZs2ebrYFpduTIEXz22WcAgPDwcDg4OGD37t1mYz744IO2fenrzJw5E3q9Hp999hm2bt2K++67z+z1iRMnQqPR4NVXX4XBYGjx/sLCwnZ/JhHdHC/iR0Rt8umnn2Lr1q0ttj/55JN4+eWXkZycjJEjR+KJJ56Ao6MjPvroI+j1erzxxhumsdHR0RgzZgwGDRoELy8vpKWl4dtvv8XChQsBAKdPn8bYsWNx3333ITo6Go6OjtiwYQPy8/Pxxz/+8ab1DR8+HO+//z6eeOIJREZGml2hODU1FT/88ANefvllAIC7uztmzJiB9957DzKZDL1798amTZs6tP4lISEBEREReO6556DX680OSQGN64FWr16N2bNnIyEhAX/84x/h6+uLnJwc/PTTTxgxYgRWrVrV7s8lopuQ+nQtIrJtzaeC3+iRm5srCIIgHD16VJg4caLg6uoqqNVq4Y477hD27dtntq+XX35ZGDJkiODh4SE4OzsLkZGRwiuvvCLU1dUJgiAIRUVFwoIFC4TIyEjBxcVFcHd3F4YOHSqsX7++zfUeOXJEuP/++4WgoCBBoVAInp6ewtixY4XPPvtMaGhoMI0rLCwU7r33XkGtVguenp7C//3f/wmZmZmtngru4uJy08987rnnBABCRETEDcfs3LlTmDhxouDu7i44OTkJvXv3FubNmyekpaW1+bsRUdvIBEEQJEtWRERERBbGNTdERERkVxhuiIiIyK4w3BAREZFdYbghIiIiu8JwQ0RERHaF4YaIiIjsSre7iJ/RaMTVq1fh5uZ2w/vLEBERkW0RBAEVFRUICgpq9V5v1+t24ebq1atm97khIiKiriM3NxchISE3HdPtwo2bmxuAxuY03+/GUgwGA7Zv344JEyZAoVBYdN90DfssDvZZPOy1ONhncVirzzqdDqGhoaZ/x2+m24Wb5kNRGo3GKuFGrVZDo9HwD44Vsc/iYJ/Fw16Lg30Wh7X73JYlJVxQTERERHaF4YaIiIjsCsMNERER2RWGGyIiIrIrDDdERERkVyQNN6tXr0ZsbKzpzKXExERs2bLlhuOTkpIgk8nMHk5OTiJWTERERLZO0lPBQ0JC8Nprr6FPnz4QBAGfffYZpk2bhvT0dPTv37/V92g0GmRnZ5ue8yrDREREdD1Jw83UqVPNnr/yyitYvXo1Dhw4cMNwI5PJEBAQIEZ5RERE1AXZzEX8Ghoa8M0336CqqgqJiYk3HFdZWYnw8HAYjUYkJCTg1VdfvWEQAgC9Xg+9Xm96rtPpADReZMhgMFjuCzTt8/pfyTrYZ3Gwz+Jhr8XBPovDWn1uz/5kgiAIFv30djpx4gQSExNRW1sLV1dXrF27FpMnT2517P79+3HmzBnExsaivLwcb731Fnbv3o2srKwb3mdi6dKlWLZsWYvta9euhVqttuh3ISIiIuuorq7G/fffj/Ly8lveYUDycFNXV4ecnByUl5fj22+/xSeffIJdu3YhOjr6lu81GAyIiorCrFmz8NJLL7U6prWZm9DQUBQVFVnl9gvJyckYP348L+1tReyzONhn8bDX4mCfxWGtPut0Ovj4+LQp3Eh+WEqpVCIiIgIAMGjQIBw+fBjvvvsuPvroo1u+V6FQID4+HmfPnr3hGJVKBZVK1ep7rfXDbc190zXsszjYZ/Gw1+Jgn8Vh6T63Z182d50bo9FoNtNyMw0NDThx4gQCAwOtXFXbFFfV4UqV1FUQERF1b5LO3CxZsgSTJk1CWFgYKioqsHbtWqSmpmLbtm0AgDlz5iA4OBjLly8HALz44osYNmwYIiIiUFZWhjfffBOXLl3CI488IuXXAABszdTiiS+PINTFAfOlLoaIiKgbkzTcFBQUYM6cOcjLy4O7uztiY2Oxbds2jB8/HgCQk5MDufza5FJpaSnmz58PrVYLT09PDBo0CPv27WvT+hxrSwjzgADgUqUMWl0tQr055UlERCQFScPNf/7zn5u+npqaavZ85cqVWLlypRUr6jg/jRNigjU4flmHg+dLEOrtJnVJRERE3ZLNrbnpymKD3QEAv2orJK6EiIio+2K4saDIgMbZmuz8SokrISIi6r4Ybiyon78rAOAUZ26IiIgkw3BjQX39XSGDgKLKOhRWtO10diIiIrIshhsLUisd4e3U+PszBZy9ISIikgLDjYX5OjXezSK3pFriSoiIiLonhhsL826608OlYoYbIiIiKTDcWJhP08xNDmduiIiIJMFwY2HNa24YboiIiKTBcGNhPirO3BAREUmJ4cbCmmduyqoNKK8xSFsMERFRN8RwY2EqB8DHVQkAyOGiYiIiItEx3FhBqKczAB6aIiIikgLDjRWEeakBAJdKqiSuhIiIqPthuLGC5pkbXsiPiIhIfAw3VhDcFG4ul9ZIXAkREVH3w3BjBcEejadMXSljuCEiIhIbw40VBHs0ztxcKa2BIAgSV0NERNS9MNxYQYDGCTIZoK83oqiyTupyiIiIuhWGGytQOsoRoGk8NHW5lIuKiYiIxMRwYyWmQ1Ncd0NERCQqhhsrCeEZU0RERJJguLGS5tPBrzDcEBERiYrhxkpCPBuvUsw1N0REROJiuLESrrkhIiKSBsONlYR48lo3REREUmC4sZKgppmbqroGlFUbJK6GiIio+2C4sRInhQN83VQAeGiKiIhITAw3VtS87oaLiomIiMTDcGNFvDs4ERGR+BhurIgX8iMiIhIfw40VhfB0cCIiItEx3FjRtQv5MdwQERGJheHGiq7dgoELiomIiMTCcGNFzWdL6WrroavltW6IiIjEwHBjRS4qR3iqFQB4A00iIiKxMNxYGdfdEBERiYvhxspMN9DkuhsiIiJRMNxYGa91Q0REJC6GGysznTHFa90QERGJQtJws3r1asTGxkKj0UCj0SAxMRFbtmy56Xu++eYbREZGwsnJCTExMdi8ebNI1XYM19wQERGJS9JwExISgtdeew1HjhxBWloa7rzzTkybNg1ZWVmtjt+3bx9mzZqFhx9+GOnp6Zg+fTqmT5+OzMxMkStvu2BepZiIiEhUkoabqVOnYvLkyejTpw/69u2LV155Ba6urjhw4ECr4999913cddddePrppxEVFYWXXnoJCQkJWLVqlciVt13zYamSqjpU19VLXA0REZH9c5S6gGYNDQ345ptvUFVVhcTExFbH7N+/H4sXLzbbNnHiRGzcuPGG+9Xr9dDr9abnOp0OAGAwGGAwWPbCes37u36/akfAzckRFbX1uFhYgT5+rhb9zO6otT6T5bHP4mGvxcE+i8NafW7P/iQPNydOnEBiYiJqa2vh6uqKDRs2IDo6utWxWq0W/v7+Ztv8/f2h1WpvuP/ly5dj2bJlLbZv374darW6c8XfQHJystlzjdwBFZDh++RfEO0pWOUzu6Pf9pmsg30WD3stDvZZHJbuc3V12y+pInm46devHzIyMlBeXo5vv/0Wc+fOxa5du24YcNpryZIlZrM9Op0OoaGhmDBhAjQajUU+o5nBYEBycjLGjx8PhUJh2v5DaTqunCpEYJ8BmDwk1KKf2R3dqM9kWeyzeNhrcbDP4rBWn5uPvLSF5OFGqVQiIiICADBo0CAcPnwY7777Lj766KMWYwMCApCfn2+2LT8/HwEBATfcv0qlgkqlarFdoVBY7Yf7t/sO9XIBUAitro5/oCzImv8N6Rr2WTzstTjYZ3FYus/t2ZfNXefGaDSarZG5XmJiIlJSUsy2JScn33CNjq24diE/XqWYiIjI2iSduVmyZAkmTZqEsLAwVFRUYO3atUhNTcW2bdsAAHPmzEFwcDCWL18OAHjyyScxevRorFixAlOmTMG6deuQlpaGjz/+WMqvcUs8HZyIiEg8koabgoICzJkzB3l5eXB3d0dsbCy2bduG8ePHAwBycnIgl1+bXBo+fDjWrl2Lf/zjH/j73/+OPn36YOPGjRgwYIBUX6FNeCE/IiIi8Ugabv7zn//c9PXU1NQW22bMmIEZM2ZYqSLraL7WTWGFHrWGBjgpHCSuiIiIyH7Z3Jobe+SpVkCtbAw0V3loioiIyKoYbkQgk8m47oaIiEgkDDciuXbGFMMNERGRNTHciKR53c0VhhsiIiKrYrgRybUzpnitGyIiImtiuBEJ19wQERGJg+FGJFxzQ0REJA6GG5E0r7nJ19Wirt4ocTVERET2i+FGJL6uKqgc5TAKgLa8VupyiIiI7BbDjUiuv9bN5TIuKiYiIrIWhhsR8XRwIiIi62O4EREXFRMREVkfw42Imq91w9PBiYiIrIfhRkSmNTe8kB8REZHVMNyIyLTmhjM3REREVsNwI6LmNTd5ZbVoMAoSV0NERGSfGG5E5OfmBEe5DPVGAfk6XuuGiIjIGhhuROQglyHIg2dMERERWRPDjciaD03llnBRMRERkTUw3IgszKvxdPAchhsiIiKrYLgRWZg3ww0REZE1MdyILNzLBQDDDRERkbUw3Iis+bDUpWKGGyIiImtguBFZ82Gpoko9qvT1EldDRERkfxhuROburICHWgGAh6aIiIisgeFGAuE8NEVERGQ1DDcSCPNuXFTMa90QERFZHsONBEwzNyVVEldCRERkfxhuJMAzpoiIiKyH4UYCvJAfERGR9TDcSCC8KdxcKa1BfYNR4mqIiIjsC8ONBPzdnKB0lKPeKCCvvFbqcoiIiOwKw40E5HIZ190QERFZCcONRMJ4xhQREZFVMNxIpDnc5HDmhoiIyKIYbiQSzjOmiIiIrILhRiLN4YZrboiIiCyL4UYiYV6Nt2DIKamGIAgSV0NERGQ/GG4kEurlDJkMqNTXo7iqTupyiIiI7Iak4Wb58uW47bbb4ObmBj8/P0yfPh3Z2dk3fU9SUhJkMpnZw8nJSaSKLUfl6IAQT2cAwPlCnjFFRERkKZKGm127dmHBggU4cOAAkpOTYTAYMGHCBFRV3fwfe41Gg7y8PNPj0qVLIlVsWb18XAEA5wsrJa6EiIjIfjhK+eFbt241e56UlAQ/Pz8cOXIEo0aNuuH7ZDIZAgICrF2e1fXydcGu04U4X8SZGyIiIkuRNNz8Vnl5OQDAy8vrpuMqKysRHh4Oo9GIhIQEvPrqq+jfv3+rY/V6PfR6vem5TqcDABgMBhgMBgtVDtM+r//1VsK9Gg9Lnc2vsHgt9qy9faaOYZ/Fw16Lg30Wh7X63J79yQQbOVXHaDTi7rvvRllZGfbs2XPDcfv378eZM2cQGxuL8vJyvPXWW9i9ezeysrIQEhLSYvzSpUuxbNmyFtvXrl0LtVpt0e/QXqfLZXj/pAN8nQT8I75B0lqIiIhsWXV1Ne6//36Ul5dDo9HcdKzNhJvHH38cW7ZswZ49e1oNKTdiMBgQFRWFWbNm4aWXXmrxemszN6GhoSgqKrplc9rLYDAgOTkZ48ePh0KhuOX4vPJajHprNxzkMhx/fiyUjjx5rS3a22fqGPZZPOy1ONhncVirzzqdDj4+Pm0KNzZxWGrhwoXYtGkTdu/e3a5gAwAKhQLx8fE4e/Zsq6+rVCqoVKpW32etH+627jvU2xFqpQOq6xqQV2FAhJ+rVeqxV9b8b0jXsM/iYa/FwT6Lw9J9bs++JJ0qEAQBCxcuxIYNG7Bjxw707Nmz3ftoaGjAiRMnEBgYaIUKrUsmk6GnT+PF/HjGFBERkWVIGm4WLFiA//73v1i7di3c3Nyg1Wqh1WpRU1NjGjNnzhwsWbLE9PzFF1/E9u3bcf78eRw9ehQPPvggLl26hEceeUSKr9BpvXybTgfnGVNEREQWIelhqdWrVwMAxowZY7Z9zZo1mDdvHgAgJycHcvm1DFZaWor58+dDq9XC09MTgwYNwr59+xAdHS1W2RbVizM3REREFiVpuGnLWubU1FSz5ytXrsTKlSutVJH4evk2hxvO3BAREVkCT8+RWG8eliIiIrIohhuJNS8oLqmqQ1k1b6BJRETUWQw3EnNROSJA03jjz3M8NEVERNRpDDc2oLdf4+zNuQIuKiYiIuoshhsb0NffDQCQnV8hcSVERERdH8ONDWgON6cZboiIiDqN4cYGMNwQERFZDsONDejr33g6eL5Oj/Jqy94inoiIqLthuLEBbk4KBLk3njF1uoCzN0RERJ3BcGMj+gbw0BQREZElMNzYCNO6Gy3DDRERUWcw3NiIa4uKea0bIiKizmC4sRHNi4p5WIqIiKhzGG5sRISfK2QyoLiqDkWVeqnLISIi6rIYbmyEWumIHt6Nt2E4eVUncTVERERdF8ONDYkO0gAAshhuiIiIOozhxob0N4WbcokrISIi6roYbmxI/yB3ADwsRURE1BkMNzakeebmfFEVKvX1EldDRETUNTHc2BAfVxX8NSoAwK95nL0hIiLqCIYbG9N8aCrrCtfdEBERdQTDjY3pzzOmiIiIOoXhxsaYZm4YboiIiDqE4cbGDAhunLk5nV+BWkODxNUQERF1PQw3NibYwxk+rirUGwVkct0NERFRuzHc2BiZTIb4MA8AQHpOmaS1EBERdUUMNzaoOdxk5JZJWgcREVFXxHBjg+JDPQEA6TmlEldCRETU9TDc2KDYEHfIZcDV8lrk62qlLoeIiKhLYbixQS4qR/T1dwPAdTdERETtxXBjo+LDmg5N5fLQFBERUXsw3Ngo06JiztwQERG1C8ONjUpoCjfHL5ejvsEobTFERERdCMONjerl4wo3J0fUGBpwSlshdTlERERdBsONjZLLZRgc3rju5sD5YomrISIi6joYbmzY0F7eAICDF0okroSIiKjrYLixYcOaws2hCyUwGgWJqyEiIuoaGG5s2IAgDVyUDiivMXDdDRERURsx3NgwRwc5BvXwAgAcvMB1N0RERG3BcGPjhvVqCjfnue6GiIioLSQNN8uXL8dtt90GNzc3+Pn5Yfr06cjOzr7l+7755htERkbCyckJMTEx2Lx5swjVSmNoz+ZFxcVcd0NERNQGkoabXbt2YcGCBThw4ACSk5NhMBgwYcIEVFVV3fA9+/btw6xZs/Dwww8jPT0d06dPx/Tp05GZmSli5eKJDXGHs8IBpdUGnCmolLocIiIimydpuNm6dSvmzZuH/v37Iy4uDklJScjJycGRI0du+J53330Xd911F55++mlERUXhpZdeQkJCAlatWiVi5eJROMgxuEfj9W72nyuSuBoiIiLb5yh1AdcrLy8HAHh5ed1wzP79+7F48WKzbRMnTsTGjRtbHa/X66HX603PdTodAMBgMMBgMHSyYnPN+7P0fof28MQvZ4rwy5lCPDAkxKL77oqs1Wcyxz6Lh70WB/ssDmv1uT37s5lwYzQasWjRIowYMQIDBgy44TitVgt/f3+zbf7+/tBqta2OX758OZYtW9Zi+/bt26FWqztX9A0kJydbdoeVAOCIPWcK8OOmzXDgMnAAVugztYp9Fg97LQ72WRyW7nN1dXWbx9pMuFmwYAEyMzOxZ88ei+53yZIlZjM9Op0OoaGhmDBhAjQajUU/y2AwIDk5GePHj4dCobDYfo1GAf85l4rSagOCYhIxqOm2DN2VtfpM5thn8bDX4mCfxWGtPjcfeWkLmwg3CxcuxKZNm7B7926EhNz8sEtAQADy8/PNtuXn5yMgIKDV8SqVCiqVqsV2hUJhtR9ua+x7eIQPfjqeh/0XyjAsws+i++6qrPnfkK5hn8XDXouDfRaHpfvcnn1JeoBDEAQsXLgQGzZswI4dO9CzZ89bvicxMREpKSlm25KTk5GYmGitMm3C7RE+AIA9Z7momIiI6GYknblZsGAB1q5di++//x5ubm6mdTPu7u5wdnYGAMyZMwfBwcFYvnw5AODJJ5/E6NGjsWLFCkyZMgXr1q1DWloaPv74Y8m+hxhG9mkMNxm5ZdDVGqBx4v91EBERtUbSmZvVq1ejvLwcY8aMQWBgoOnx9ddfm8bk5OQgLy/P9Hz48OFYu3YtPv74Y8TFxeHbb7/Fxo0bb7oI2R6EeKrRy8cFDUYBe89w9oaIiOhGJJ25EYRbX3E3NTW1xbYZM2ZgxowZVqjIto3p54fzRRew41QBJsUESl0OERGRTeJJxV3I2KjGhcQ7swt5KwYiIqIbYLjpQm7r4QVXlSOKKvU4caVc6nKIiIhsUofCTW5uLi5fvmx6fujQISxatMjuF/VKTekox+1NC4tTThVIXA0REZFt6lC4uf/++7Fz504AjVcMHj9+PA4dOoTnnnsOL774okULJHN3RjYdmmK4ISIialWHwk1mZiaGDBkCAFi/fj0GDBiAffv24csvv0RSUpIl66PfGNPPDzIZcOJKOQp0tVKXQ0REZHM6FG4MBoPpqr8///wz7r77bgBAZGSk2WnbZHm+birEhngAAHZmc/aGiIjotzoUbvr3748PP/wQv/zyC5KTk3HXXXcBAK5evQpvb2+LFkgtjW06NJXyK8MNERHRb3Uo3Lz++uv46KOPMGbMGMyaNQtxcXEAgB9++MF0uIqsp3ndzS9nilBraJC4GiIiItvSoYv4jRkzBkVFRdDpdPD0vHaH6kcffRRqtdpixVHr+gdpEOzhjCtlNdh1uhAT+7d+01AiIqLuqEMzNzU1NdDr9aZgc+nSJbzzzjvIzs6Gnx/vWG1tMpkMdw1oDDRbM7USV0NERGRbOhRupk2bhs8//xwAUFZWhqFDh2LFihWYPn06Vq9ebdECqXWTYxrDzc8n86Gv56EpIiKiZh0KN0ePHsXtt98OAPj222/h7++PS5cu4fPPP8e//vUvixZIrYsP9YS/RoUKfT32nS2WuhwiIiKb0aFwU11dDTc3NwDA9u3bcc8990Aul2PYsGG4dOmSRQuk1snlMtNamy2ZPP2eiIioWYfCTUREBDZu3Ijc3Fxs27YNEyZMAAAUFBRAo9FYtEC6seZ1N9tP5sPQYJS4GiIiItvQoXDzwgsv4K9//St69OiBIUOGIDExEUDjLE58fLxFC6QbG9LDC94uSpRVG3DwfInU5RAREdmEDoWbP/zhD8jJyUFaWhq2bdtm2j527FisXLnSYsXRzTk6yDGhvz8AHpoiIiJq1qFwAwABAQGIj4/H1atXTXcIHzJkCCIjIy1WHN3aXQMCAQDbsvLRYBQkroaIiEh6HQo3RqMRL774Itzd3REeHo7w8HB4eHjgpZdegtHItR9iGt7bG+7OChRV6pF2kYemiIiIOhRunnvuOaxatQqvvfYa0tPTkZ6ejldffRXvvfcenn/+eUvXSDehcJBjXFTjoanNJ3hoioiIqEPh5rPPPsMnn3yCxx9/HLGxsYiNjcUTTzyBf//730hKSrJwiXQrv4trPDS16Xgez5oiIqJur0PhpqSkpNW1NZGRkSgp4aERsd0e4QNvFyWKq+qw52yR1OUQERFJqkPhJi4uDqtWrWqxfdWqVYiNje10UdQ+jg5yTI0LAgBsTL8icTVERETS6tBdwd944w1MmTIFP//8s+kaN/v370dubi42b95s0QKpbaYNDELSvovYnpWPKn09XFQd+k9LRETU5XVo5mb06NE4ffo0fv/736OsrAxlZWW45557kJWVhS+++MLSNVIbDAz1QA9vNWoMDUg+mS91OURERJLp8P/eBwUF4ZVXXjHbduzYMfznP//Bxx9/3OnCqH1kMhmmDQzGuylnsCH9CqbHB0tdEhERkSQ6fBE/sj3NgWbP2SIUVeolroaIiEgaDDd2pKePC+JCPdBgFLDp2FWpyyEiIpIEw42dmT6w8aypDRkMN0RE1D21a83NPffcc9PXy8rKOlMLWcDvYoPw8k+/4lhuGc4XVqKXr6vUJREREYmqXeHG3d39lq/PmTOnUwVR5/i6qTCqjw92ZhdifdplPDuJNzIlIqLupV3hZs2aNdaqgyxo5m1h2JldiG+PXMZTE/pC4cCjj0RE1H3wXz07NDbKDz6uShRV6rHzVIHU5RAREYmK4cYOKRzkuCchBACwPi1X4mqIiIjExXBjp+4bHAoA2JldiAJdrcTVEBERiYfhxk5F+LliULgnGowCvj16WepyiIiIRMNwY8dmNs3erD+cC0EQJK6GiIhIHAw3dmxKbCBclA64WFyNA+dLpC6HiIhIFAw3dsxF5Yi7Bzbeb+q/By5JXA0REZE4GG7s3JzEcADA1iwttOVcWExERPZP0nCze/duTJ06FUFBQZDJZNi4ceNNx6empkImk7V4aLVacQrugqICNRjS0wsNRgFrD3L2hoiI7J+k4aaqqgpxcXF4//332/W+7Oxs5OXlmR5+fn5WqtA+zE3sAQBYeygXdfVGaYshIiKysnbdfsHSJk2ahEmTJrX7fX5+fvDw8LB8QXZqQn9/+GtUyNfpsSUzD9Oa1uEQERHZI0nDTUcNHDgQer0eAwYMwNKlSzFixIgbjtXr9dDr9abnOp0OAGAwGGAwGCxaV/P+LL1fS5g5OAT/2nEOn+27iMn9u/ZMly332Z6wz+Jhr8XBPovDWn1uz/5kgo1cAEUmk2HDhg2YPn36DcdkZ2cjNTUVgwcPhl6vxyeffIIvvvgCBw8eREJCQqvvWbp0KZYtW9Zi+9q1a6FWqy1Vvs3T1QFLjzqgQZDhrzH1CHWVuiIiIqK2q66uxv3334/y8nJoNJqbju1S4aY1o0ePRlhYGL744otWX29t5iY0NBRFRUW3bE57GQwGJCcnY/z48VAoFBbdtyX8Zf1xbDqhxe8HBuKNe2OkLqfDbL3P9oJ9Fg97LQ72WRzW6rNOp4OPj0+bwk2XPCx1vSFDhmDPnj03fF2lUkGlUrXYrlAorPbDbc19d8ajo3tj0wktfjyuxTOTohDo7ix1SZ1iq322N+yzeNhrcbDP4rB0n9uzry5/nZuMjAwEBgZKXUaXEBvigaE9vVBvFLBm70WpyyEiIrIKSWduKisrcfbsWdPzCxcuICMjA15eXggLC8OSJUtw5coVfP755wCAd955Bz179kT//v1RW1uLTz75BDt27MD27dul+gpdzv+N7oWDF0qw9mAOFt4ZAY0T/++FiIjsi6ThJi0tDXfccYfp+eLFiwEAc+fORVJSEvLy8pCTk2N6va6uDk899RSuXLkCtVqN2NhY/Pzzz2b7oJsb09cPffxccaagEl8dzMH/je4tdUlEREQWJWm4GTNmzE3vVp2UlGT2/JlnnsEzzzxj5arsm1wuw/zbe+GZ/x3Hmr0X8dCInlA6dvmjk0RERCb8V60bmhYfBD83FbS6WmxIvyx1OURERBbFcNMNqRwd8OioXgCAVTvPwtDAWzIQEZH9YLjpph4YGg4fVyVyS2rwfcZVqcshIiKyGIabbspZ6YD5tzfO3ry/8yzqOXtDRER2guGmG3twWDg81QpcKKrCpuN5UpdDRERkEQw33ZiLyhGPNM3e/GvHGc7eEBGRXWC46ebmJDbO3pwvrMJ3R69IXQ4REVGnMdx0c25OCiy4IwIAsPLn06g1NEhcERERUecw3BAeHBaOIHcn5JXX4r8HLkldDhERUacw3BCcFA5YNL4vgMYzp3S1BokrIiIi6jiGGwIA3BMfjAg/V5RWG/DRrnNSl0NERNRhDDcEAHB0kOPpif0AAP/+5QJyS6olroiIiKhjGG7IZEK0P4b39kZdvRHLt/wqdTlEREQdwnBDJjKZDC9MjYZcBmw+ocWB88VSl0RERNRuDDdkJjJAgweGhgMAlv14Eg1GQeKKiIiI2ofhhlpYPL4v3J0V+DVPh3WHc6Quh4iIqF0YbqgFTxclFo3rAwB4Y2s2iir1EldERETUdgw31KrZw8IRHahBeY0Br/zExcVERNR1MNxQqxwd5Fh+TwxkMmBD+hXsOVMkdUlERERtwnBDNxQX6oG5iT0AAM9tPMH7ThERUZfAcEM39dSEvgjQOOFScTVWJp+WuhwiIqJbYrihm3JzUuCl6QMAAB//ch5HLpVIXBEREdHNMdzQLY2P9se9CSEQBOCp9cdQXVcvdUlEREQ3xHBDbfLC1GgEujvhYnE1XttySupyiIiIbojhhtrE3VmBN/4QCwD4fP8l/HKmUOKKiIiIWsdwQ212ex9fzB7WeGuGp9YfQzEv7kdERDaI4YbaZcnkSET4uaKgQo+nvjkGI+89RURENobhhtpFrXTEqvvjoXKUIzW7EJ/uvSB1SURERGYYbqjdIgM0eGFqNADg9a2nkJ5TKnFFRERE1zDcUIfcPyQMU2ICYWgQ8Ph/j6KwgutviIjINjDcUIfIZDK8dm8Mevu6QKurxYK1R2FoMEpdFhEREcMNdZybkwIfzxkMV5UjDl0owaubefdwIiKSHsMNdUpvX1e8fV8cAGDN3ov49shliSsiIqLujuGGOm1C/wD8+c4IAMCz/zuOvWeLJK6IiIi6M4YbsohF4/pialwQ6o0CHvviCLK1FVKXRERE3RTDDVmEXC7DWzNiMaSHFyr09XhozSHk62qlLouIiLohhhuyGJWjAz6eMwi9fF1wtbwWf0o6jEo97yBORETiYrghi/JQK5E0bwh8XJXIuqrDgi+Poq6ep4gTEZF4GG7I4sK81fjP3NvgpJBj1+lCLPo6HfW8Bg4REYmE4YasIi7UAx/PHgylgxybT2jxzLfHeZNNIiIShaThZvfu3Zg6dSqCgoIgk8mwcePGW74nNTUVCQkJUKlUiIiIQFJSktXrpI4Z1dcXq+6Ph4Nchu/Sr+D57zMhCAw4RERkXZKGm6qqKsTFxeH9999v0/gLFy5gypQpuOOOO5CRkYFFixbhkUcewbZt26xcKXXUhP4BWDlzIGQy4MuDOVj6QxZncIiIyKocpfzwSZMmYdKkSW0e/+GHH6Jnz55YsWIFACAqKgp79uzBypUrMXHiRGuVSZ10d1wQag0N+Nv/juOz/ZdQY2jA8nti4SCXSV0aERHZIUnDTXvt378f48aNM9s2ceJELFq06Ibv0ev10Ouv3bFap9MBAAwGAwwGg0Xra96fpfdrD34fFwCZYMTfvsvE+rTLqNLX4817B0Dh0P7JQ/ZZHOyzeNhrcbDP4rBWn9uzvy4VbrRaLfz9/c22+fv7Q6fToaamBs7Ozi3es3z5cixbtqzF9u3bt0OtVlulzuTkZKvst6tTApjbR4bPz8jx0wktLl2+inl9jVB08OAo+ywO9lk87LU42GdxWLrP1dXVbR7bpcJNRyxZsgSLFy82PdfpdAgNDcWECROg0Wgs+lkGgwHJyckYP348FAqFRfdtLyYDGH66EAu+OobMUmBDkS9WzYqDq6rtP4rsszjYZ/Gw1+Jgn8VhrT43H3lpiy4VbgICApCfn2+2LT8/HxqNptVZGwBQqVRQqVQttisUCqv9cFtz3/ZgfP8gJM1T4pHP07D3XDEe/DQNa+bdBj+NU7v2wz6Lg30WD3stDvZZHJbuc3v21aWuc5OYmIiUlBSzbcnJyUhMTJSoIuqo4RE+WDt/GLxdGq9k/PsP9uFsAW+2SUREnSdpuKmsrERGRgYyMjIANJ7qnZGRgZycHACNh5TmzJljGv/YY4/h/PnzeOaZZ3Dq1Cl88MEHWL9+Pf7yl79IUT510sBQD3z3xHD09HHBlbIa3PPBPhw8Xyx1WURE1MVJGm7S0tIQHx+P+Ph4AMDixYsRHx+PF154AQCQl5dnCjoA0LNnT/z0009ITk5GXFwcVqxYgU8++YSngXdh4d4u+N/jw5EQ5gFdbT1m/+cQ1qflSl0WERF1YZKuuRkzZsxNr1jb2tWHx4wZg/T0dCtWRWLzclFi7fxh+MvXGdiS2Xirhl/zdHhuchQcO3CqOBERdW/8l4NsgpPCAe/fn4BF4/oAANbsvYi5aw6htKpO4sqIiKirYbghmyGXy7BoXF98+OAgqJUO2Hu2GNPe34vMK+VSl0ZERF0Iww3ZnLsGBOC7J4Yj1MsZOSXVuOeDffjiwCXedJOIiNqE4YZsUmSABj8uHIlxUf6oazDi+Y2ZWLg2HbpaXjadiIhujuGGbJaHWol/zxmEf0yJgqNchp9O5GHqe3uQeaXtV6kkIqLuh+GGbJpMJsMjt/fCN48lItjDGZeKq3Hfvw8iNU8Go5GHqYiIqCWGG+oS4sM8sfnPt2N8tD8MDQI2XHTAnKQ0XC5t+43UiIioe2C4oS7DXa3Ax7MHYdnUKCjlAg5eKMVd7/yC9Wm5XGxMREQmDDfUpchkMtw/JBTPxDYgIcwDlfp6PPPtccz/PA0FFbVSl0dERDaA4Ya6JF9nYO3Dt+HZSZFQOsjx868FmLhyN77PuMJZHCKibo7hhrosB7kMj43ujR/+3whEBWpQWm3Ak+syMOfTQ7hUXCV1eUREJBGGG+ryIgM0+H7BCPx1Ql8oHeX45UwRJqzcjfd3nkVdvVHq8oiISGQMN2QXlI5yLLyzD7YtGoUREd7Q1xvx5rZsTH1vD45cKpG6PCIiEhHDDdmVnj4u+O/DQ7FyZhy8XJTIzq/Avav342/fHueCYyKiboLhhuyOTCbD7+NDkLJ4NGYODgUAfJ2WizveTMX7O8+i1tAgcYVERGRNDDdktzxdlHj9D7H49rFExIW4o6quAW9uy8bYFbvw47GrPKuKiMhOMdyQ3RvcwwsbnhiBlTPjEOjuhCtlNfh/X6Xj3tX7kJ5TKnV5RERkYQw31C3I5Y2HqnY8NQZ/GdcXzgoHHM0pw+8/2Icn16XjSlmN1CUSEZGFMNxQt+KsdMCT4/pg51/H4N6EEADA9xlXccdbqVj2YxYKK/QSV0hERJ3FcEPdUoC7E1bcF4cfF47E0J5eqKs3Ys3eixj1xk68vvUUyqrrpC6RiIg6iOGGurWYEHese3QYPv/TEMSFuKPG0IDVqedw++s78e7PZ1BRa5C6RCIiaieGG+r2ZDIZRvX1xcYFI/DvOYMRGeCGCn09Vv58Gre/sROrU8+hUl8vdZlERNRGDDdETWQyGcZH+2Pzn2/He7Pi0cvXBWXVBry+9RSGL0/Biu3ZKK7kmhwiIlvHcEP0G3K5DFPjgrB90Si8NSMOvXxcoKutx3s7zmLE6zuw9IcsXC6tlrpMIiK6AYYbohtwdJDjD4NCkLx4NFY/kICYYHfUGoxI2ncRY95MxeL1GTidXyF1mURE9BuOUhdAZOsc5DJMignEXQMCsPdsMVbvOou9Z4vx3dEr+O7oFYyP9sejo3phcLgnZDKZ1OUSEXV7DDdEbSSTyTCyjw9G9vFBRm4ZPkw9h20ntUg+mY/kk/mICXbHvOE98Lu4QKgcHaQul4io2+JhKaIOGBjqgQ9nD0LyX0Zh5uBQqBzlOHGlHE99cwwjXtuBt5NPo0DHu5ATEUmB4YaoEyL83PD6H2Kxf8lYPD2xHwI0TiiqrMO/Us5gxOs7sGhdOjJyy6Quk4ioW+FhKSIL8HJRYsEdEXh0VC9sy9Iiae9FpF0qxcaMq9iYcRUxwe6YNSQMdw8MgquKf+yIiKyJf8sSWZDCQY7fxQbhd7FBOHG5HEn7LuLHY1dx4ko5Tmw4gZd/Oom744Iwa0gYYkPcuQCZiMgKGG6IrCQmxB0r7ovDc1Oi8N3Ry/jqUA7OFVZh3eFcrDuci+hADWYNDcO0gUHQOCmkLpeIyG4w3BBZmZeLEo/c3gsPj+yJwxdL8dWhHPx0Ig8n83R4fmMmXv3pV0waEIB7EkKQ2NsbDnLO5hARdQbDDZFIZDIZhvT0wpCeXvjn1Gh8d/QKvjqUgzMFlfgu/Qq+S7+CQHcnTI8Pxr0JwYjwc5O6ZCKiLonhhkgCHmol/jSyJx4a0QNHc8rw3dHL+PHYVeSV12J16jmsTj2HuBB33JMQgrvjguDpopS6ZCKiLoPhhkhCMpkMg8I9MSjcEy9MjcaOXwvwv6OXkZpdiGOXy3Hscjle/ukkxvTzw+9iAzEuyh8uPNuKiOim+LckkY1QOTpgUkwgJsUEoqhSjx8yruK79MvIvKIzXQVZ5SjHnZF+mBIbiDsj/aBW8o8wEdFv8W9GIhvk46rCn0b2xJ9G9kS2tgI/HruKTcev4mJxNbZkarElUwtnhQPujPLD72ICcUekH5wUvOUDERHAcENk8/oFuKFfQD88NaEvsq7q8NOJPGw6fhW5JTX46XgefjqeB7XSAeOi/DElNhCj+/oy6BBRt2YTt194//330aNHDzg5OWHo0KE4dOjQDccmJSVBJpOZPZycnESslkgaMpkMA4Ld8be7IrH76Tvww8IR+L9RvRDs4Yzqugb8cOwq/u+LIxj88s9YuPYofjh2Fbpag9RlExGJTvKZm6+//hqLFy/Ghx9+iKFDh+Kdd97BxIkTkZ2dDT8/v1bfo9FokJ2dbXrOq7xSdyOTyRAb4oHYEA88OykSGbll2HQ8D5tP5CGvvBabjudh0/E8KBxkSOztgwnR/hgf7Q9/Df9HgIjsn+Th5u2338b8+fPx0EMPAQA+/PBD/PTTT/j000/x7LPPtvoemUyGgIAAMcskslkymQzxYZ6ID/PEc5OjkHG5DMkn87E9S4tzhVXYfboQu08X4h8bMzEw1APjo/0xIdofEX6u/B8DIrJLkoaburo6HDlyBEuWLDFtk8vlGDduHPbv33/D91VWViI8PBxGoxEJCQl49dVX0b9/fzFKJrJpcrkMCWGeSAjzxN/uisTZgsrGoHNSi/ScMmTkNj7e3JaNEE9n3Bnphzsi/ZDYy5vrdIjIbkgaboqKitDQ0AB/f3+z7f7+/jh16lSr7+nXrx8+/fRTxMbGory8HG+99RaGDx+OrKwshISEtBiv1+uh1+tNz3U6HQDAYDDAYLDseoTm/Vl6v2SOfW67cE8VHhkRhkdGhCFfV4sd2YX4+dcC7D9fgsulNfh8/yV8vv8SnBRyJPbywpi+vrijny8C3Z3YZxGx1+Jgn8VhrT63Z38yQRAEi356O1y9ehXBwcHYt28fEhMTTdufeeYZ7Nq1CwcPHrzlPgwGA6KiojBr1iy89NJLLV5funQpli1b1mL72rVroVarO/cFiLoofQNwulyGk6UyZJXJUF5nfngqUC0gykNAP3cBvdwEKDmpQ0QSq66uxv3334/y8nJoNJqbjpV05sbHxwcODg7Iz883256fn9/mNTUKhQLx8fE4e/Zsq68vWbIEixcvNj3X6XQIDQ3FhAkTbtmc9jIYDEhOTsb48eOhUPAuz9bCPluWIAg4pa1E6ulCpJ4uQkZuGfKqZcirlmHHVUDlKMfgcE+MjPDGyAhv9PPnWh1L48+0ONhncVirz81HXtpC0nCjVCoxaNAgpKSkYPr06QAAo9GIlJQULFy4sE37aGhowIkTJzB58uRWX1epVFCpVC22KxQKq/1wW3PfdA37bDmxYV6IDfPCn8f1Q2lVHXafKcSu7AKkZF1BeZ0Re88VY++5Yry+rfECg7f38cHICB/c3scHfjwDy2L4My0O9lkclu5ze/Yl+dlSixcvxty5czF48GAMGTIE77zzDqqqqkxnT82ZMwfBwcFYvnw5AODFF1/EsGHDEBERgbKyMrz55pu4dOkSHnnkESm/BpHd8HRRYtrAYEzu74efVDnoe9soHLhQhl/OFOLA+RIUVeqxIf0KNqRfAQD083fD8AhvDOvljaE9veCh5k0+iUhakoebmTNnorCwEC+88AK0Wi0GDhyIrVu3mhYZ5+TkQC6/dq3B0tJSzJ8/H1qtFp6enhg0aBD27duH6Ohoqb4Ckd2SyYA+fq6IDvbEn0b2hL6+AUcvNQadPWeLcOJKObLzK5CdX4E1ey8CACID3DCslzeG9fLCkJ7e8OIdzYlIZJKHGwBYuHDhDQ9Dpaammj1fuXIlVq5cKUJVRPRbKkcHJPb2RmJvbzwDoLSqDnvPFeHA+WIcOF+CswWVOKWtwCltBZL2XQTQOLMzrJcXhvXyxpCeXvB2bXmYmIjIkmwi3BBR1+TposTvYoPwu9ggAEBhhR6HLpTgwPliHLxQjNP5laaZnc/2XwIA9PV3xdCe3qaw4+vGsENElsVwQ0QW4+umwpTYQEyJDQQAFFdeCzsHzpcgO78Cp/MrcTq/El8caAw74d5qDArzREJ448UH+wW4wUHOs7GIqOMYbojIarxdVZgUE4hJMY1hp6SqDocuNAadA+eLcUpbgUvF1bhUXI3vmhYou6ocMTDUAwlhHkgIb7ythLszz2whorZjuCEi0Xi5KHHXgEDcNaAx7JTXGJCRW4Yjl0px9FIpMnLLUKmvx56zRdhztsj0vr7+ro23lQj3xKBwT/TyceG1dojohhhuiEgy7s4KjO7ri9F9fQEADUYBp/MrGsNOTmPguVhcbTqUte5wLgDAQ61ATLA7BoY23hk9LsSd19shIhOGGyKyGQ5yGaICNYgK1ODBYeEAgKJKPdJzrs3uHLtchrJqA345U4Rfzlyb3QnQOCEu1L0p7HggJsSdh7OIuimGGyKyaT6uKoyP9sf46MZrX9XVG5GtrcCxy2U4lluG45fLcaagAlpdLbRZtdiWde12LuHeavQP0iA6UIP+Qe6IDtLAz03FQ1pEdo7hhoi6FKWjHDEh7ogJcTfN7lTp65F5pRzHL5c3hp7LZcgtqTEtVt58Qmt6v4+rElGBGkQHNQWeQA16+rjwDC0iO8JwQ0RdnovKEUN7eWNoL2/TttKqOpzM0yHrajlOXtUh66oO5worUVRZ1+KQlrPCAZGBbk2zPI0zPJEBbnBS8HboRF0Rww0R2SVPFyVGRPhgRISPaVutoQGntBVNYaccJ/N0OJVXgRpDA9JzypCeU2YaK5cBvX1dGwNPUOM6oH7+bvDlYS0im8dwQ0TdhpPCAQNDPTAw1MO0rcEo4EJRldksz8mrOhRX1eFMQSXOFFRiY8ZV03gPtQJ9/d3Q198V/fzd0MffDf383eDJe2gR2QyGGyLq1hzkMkT4uSLCzxV3xzXeRkIQBBRU6E0zPFlXdcjWVuBicRXKqg04dKEEhy6UmO3H102Fvv6uTcGnMfz09nXlXdKJJMBwQ0T0GzKZDP4aJ/hrnHBHpJ9pe62hAWcLKnGmoALZ2kqczq/A6fwKXC6tQWGFHoUVeuw9W2y2L28XJXr7uqK3n0vTr66I8HVFkIczFzETWQnDDRFRGzkpHDAg2B0Dgt3Ntlfq63EmvwJnmm4Uejq/AucKKnG1vBbFVXUorirBoYvmMz0qRzl6+rigt58reno5Q1ckQ/hVHfoGukOt5F/NRJ3BP0FERJ3kqnJEfFjjfbCuV6Wvx4WiKpwrrMS5gkqcK2z8/fmiKujrjTilrcApbUXTaAd8fuYAAMBfo0K4twt6eKubfnVBuLca4d5quDnxwoREt8JwQ0RkJS4qx1ZnehqMAi6XVjeFniqcydch7fRllDYoUVptQL5Oj3ydvsW6HqDxMFe4t7op8FwLPT28XeChVvBMLiIw3BARic5BLmsKJi64MxIwGAzYvPkSJk++A1UGAZeKq3GxuMrs10vFVSiqrGs6zFWHo9edtt5M4+SIHj5NocdL3RR8XBDs6YwAjRPX+FC3wXBDRGRDPNRKeKiViLvudPVmlfp6XLo+9BRdCz9aXS10tfU4frnxSs2/5SCXIdDdCcEezgj2dEaIhzNCPNUI9nRGsIczAj2coHLkRQvJPjDcEBF1Ea4qR/QPckf/IPcWr9XUNSCnpNo8/BRXI6ekGnnlNTA0CLhcWoPLpTXAhZb7lskAX1cVQjydEeypvhaCmoJQsKczFzpTl8GfVCIiO+CsdEC/ADf0C3Br8VqDUUBBRS2ulNbgSlmNKeRcKavBldJqXCmrQa3BiIIKPQoq9K0e8gIAT7WiadanccYnyKPxcFeAuwr+Gif4uTlB6Si38jclujWGGyIiO9d4SMoZge7OGNzK64IgoKSq7rrAc30Iagw/FbX1KK02oLTagMwruht+lo+rEv4aJwRonODv3vjrb3+vcXbkwmeyKoYbIqJuTiaTwdtVBW9XVatrfQBAV2vAleYZn6bAc7WsFlpdLbTltSioqIWhQUBRZR2KKuuQdfXGAchJIW8MPBonBDSFnubfN148UQVfNxXXAFGHMdwQEdEtaZwU0AQqEBWoafV1o1FAaXUdtLpa5OtqoS3XN/6+vPbaNl0tyqoNqDUYcbG4GheLq2/6me7OCvi6qeDr2hh2/NwafzV7uKrgqVZCzjPB6DoMN0RE1Gly+bXZn9YWPDerNTQ0hZ9asyDUHH6unwUqrzGgvMaAswWVN/1sB7kMXi5KeLso4eumgreLEj5NtXi7KuHb9Ku7Sg6D0dLfnGwRww0REYnGSeFgusbPjQhCY7Bpvl9XQdOvhZX667bVorBCj9JqAxqMgmn7tSs+34gjlmbsgI9rcwBSwstFCU91468eaiW8XBRmzzVOXCPU1TDcEBGRTZHJZKbr/fTxb3n21/Xq6o0oqapDUaUeRZV6FFc2/r7YtK0OxU3bi6v0MDQIqNTXo1Jff8vDYs0c5TKz0OOpVsLTxTwEmbaplfB0UcBVxUAkJYYbIiLqspSO8sZFye5OtxxbV1eH//24BQmJo1FW29B4tedKPUqqDCitrkNpdR1Kqhp/LW3aVl3XgHqjYApPbaVwaApEaiXc1QponBRwd77+4djqdo2zAk4KLqTuLIYbIiLqFmQyGdSOQC9fFygUbbsBaa2hwSzsNIefkqo6lFUbWn1eY2iAoeHaobL2UjnKoXH+bRhqCj9Oji1eu/65WunAGSMw3BAREd2Qk8LBdI2gtqqpazCbCWpeGN380NXUQ/ebbeU1BuhqDRAEQF9v7HAwcpTLTEHH1ckRbk6OcFU5wlWlgNt1z92cml5XNW27frvKscvfh4zhhoiIyIKclQ5wVjZewbk9jEYBlXX1KK9uDkHmwedaEKo3/b7iujH1RgH1RsF0c9XOUCsdmsKOI1ydFNdCkMqxKTQ1bnN1coSLqjEkuagc4aJygMoBqDJ06uM7jeGGiIjIBsjlssbrCTkpENrO9wqCgOq6BrOZoMraxoXTFfp6VNRee15ZWw9dbT0q9YbG12sbt1Xo61FX33iufHVdA6rrGlDQgdkjAAh1ccCMaR16q0Uw3BAREXVxMpmsaebEsd0zRtfT1zdcC0W1TcFH3xiEzJ7XNgWmpjPPqvQN1/2+Hk4O0k7dMNwQERERAEDl6ACVqwO8XVUd3ofBYMBPP222YFXtx9u3EhERkUVJfcIWww0RERHZFYYbIiIisisMN0RERGRXGG6IiIjIrjDcEBERkV1huCEiIiK7YhPh5v3330ePHj3g5OSEoUOH4tChQzcd/8033yAyMhJOTk6IiYnB5s3Snk9PREREtkPycPP1119j8eLF+Oc//4mjR48iLi4OEydOREFBQavj9+3bh1mzZuHhhx9Geno6pk+fjunTpyMzM1PkyomIiMgWSR5u3n77bcyfPx8PPfQQoqOj8eGHH0KtVuPTTz9tdfy7776Lu+66C08//TSioqLw0ksvISEhAatWrRK5ciIiIrJFkt5+oa6uDkeOHMGSJUtM2+RyOcaNG4f9+/e3+p79+/dj8eLFZtsmTpyIjRs3tjper9dDr7924y+dTgeg8fLQBoNl733RvD9L75fMsc/iYJ/Fw16Lg30Wh7X63J79SRpuioqK0NDQAH9/f7Pt/v7+OHXqVKvv0Wq1rY7XarWtjl++fDmWLVvWYvv27duhVqs7WPnNJScnW2W/ZI59Fgf7LB72Whzsszgs3efq6uo2j7X7G2cuWbLEbKZHp9MhNDQUEyZMgEajsehnGQwGJCcnY/z48VAoFBbdN13DPouDfRYPey0O9lkc1upz85GXtpA03Pj4+MDBwQH5+flm2/Pz8xEQENDqewICAto1XqVSQaVqeXdThUJhtR9ua+6brmGfxcE+i4e9Fgf7LA5L97k9+5I03CiVSgwaNAgpKSmYPn06AMBoNCIlJQULFy5s9T2JiYlISUnBokWLTNuSk5ORmJjYps8UBAFA+xJgWxkMBlRXV0On0/EPjhWxz+Jgn8XDXouDfRaHtfrc/O9287/jNyVIbN26dYJKpRKSkpKEkydPCo8++qjg4eEhaLVaQRAEYfbs2cKzzz5rGr93717B0dFReOutt4Rff/1V+Oc//ykoFArhxIkTbfq83NxcAQAffPDBBx988NEFH7m5ubf8t17yNTczZ85EYWEhXnjhBWi1WgwcOBBbt241LRrOycmBXH7tjPXhw4dj7dq1+Mc//oG///3v6NOnDzZu3IgBAwa06fOCgoKQm5sLNzc3yGQyi36X5vU8ubm5Fl/PQ9ewz+Jgn8XDXouDfRaHtfosCAIqKioQFBR0y7EyQWjL/A61hU6ng7u7O8rLy/kHx4rYZ3Gwz+Jhr8XBPovDFvos+UX8iIiIiCyJ4YaIiIjsCsONBalUKvzzn/9s9dRzshz2WRzss3jYa3Gwz+KwhT5zzQ0RERHZFc7cEBERkV1huCEiIiK7wnBDREREdoXhhoiIiOwKw42FvP/+++jRowecnJwwdOhQHDp0SOqSupTly5fjtttug5ubG/z8/DB9+nRkZ2ebjamtrcWCBQvg7e0NV1dX3HvvvS1uopqTk4MpU6ZArVbDz88PTz/9NOrr68X8Kl3Ka6+9BplMZnavNvbZMq5cuYIHH3wQ3t7ecHZ2RkxMDNLS0kyvC4KAF154AYGBgXB2dsa4ceNw5swZs32UlJTggQcegEajgYeHBx5++GFUVlaK/VVsWkNDA55//nn07NkTzs7O6N27N1566SWz+w+x1+23e/duTJ06FUFBQZDJZNi4caPZ65bq6fHjx3H77bfDyckJoaGheOONNyzzBdp1Iyhq1bp16wSlUil8+umnQlZWljB//nzBw8NDyM/Pl7q0LmPixInCmjVrhMzMTCEjI0OYPHmyEBYWJlRWVprGPPbYY0JoaKiQkpIipKWlCcOGDROGDx9uer2+vl4YMGCAMG7cOCE9PV3YvHmz4OPjIyxZskSKr2TzDh06JPTo0UOIjY0VnnzySdN29rnzSkpKhPDwcGHevHnCwYMHhfPnzwvbtm0Tzp49axrz2muvCe7u7sLGjRuFY8eOCXfffbfQs2dPoaamxjTmrrvuEuLi4oQDBw4Iv/zyixARESHMmjVLiq9ks1555RXB29tb2LRpk3DhwgXhm2++EVxdXYV3333XNIa9br/NmzcLzz33nPDdd98JAIQNGzaYvW6JnpaXlwv+/v7CAw88IGRmZgpfffWV4OzsLHz00Uedrp/hxgKGDBkiLFiwwPS8oaFBCAoKEpYvXy5hVV1bQUGBAEDYtWuXIAiCUFZWJigUCuGbb74xjfn1118FAML+/fsFQWj8wyiXy003XRUEQVi9erWg0WgEvV4v7hewcRUVFUKfPn2E5ORkYfTo0aZwwz5bxt/+9jdh5MiRN3zdaDQKAQEBwptvvmnaVlZWJqhUKuGrr74SBEEQTp48KQAQDh8+bBqzZcsWQSaTCVeuXLFe8V3MlClThD/96U9m2+655x7hgQceEASBvbaE34YbS/X0gw8+EDw9Pc3+3vjb3/4m9OvXr9M187BUJ9XV1eHIkSMYN26caZtcLse4ceOwf/9+CSvr2srLywEAXl5eAIAjR47AYDCY9TkyMhJhYWGmPu/fvx8xMTGmm64CwMSJE6HT6ZCVlSVi9bZvwYIFmDJlilk/AfbZUn744QcMHjwYM2bMgJ+fH+Lj4/Hvf//b9PqFCxeg1WrN+uzu7o6hQ4ea9dnDwwODBw82jRk3bhzkcjkOHjwo3pexccOHD0dKSgpOnz4NADh27Bj27NmDSZMmAWCvrcFSPd2/fz9GjRoFpVJpGjNx4kRkZ2ejtLS0UzVKflfwrq6oqAgNDQ1mf9EDgL+/P06dOiVRVV2b0WjEokWLMGLECNPd3rVaLZRKJTw8PMzG+vv7Q6vVmsa09t+h+TVqtG7dOhw9ehSHDx9u8Rr7bBnnz5/H6tWrsXjxYvz973/H4cOH8ec//xlKpRJz58419am1Pl7fZz8/P7PXHR0d4eXlxT5f59lnn4VOp0NkZCQcHBzQ0NCAV155BQ888AAAsNdWYKmearVa9OzZs8U+ml/z9PTscI0MN2RzFixYgMzMTOzZs0fqUuxObm4unnzySSQnJ8PJyUnqcuyW0WjE4MGD8eqrrwIA4uPjkZmZiQ8//BBz586VuDr7sn79enz55ZdYu3Yt+vfvj4yMDCxatAhBQUHsdTfGw1Kd5OPjAwcHhxZnk+Tn5yMgIECiqrquhQsXYtOmTdi5cydCQkJM2wMCAlBXV4eysjKz8df3OSAgoNX/Ds2vUeNhp4KCAiQkJMDR0RGOjo7YtWsX/vWvf8HR0RH+/v7sswUEBgYiOjrabFtUVBRycnIAXOvTzf7eCAgIQEFBgdnr9fX1KCkpYZ+v8/TTT+PZZ5/FH//4R8TExGD27Nn4y1/+guXLlwNgr63BUj215t8lDDedpFQqMWjQIKSkpJi2GY1GpKSkIDExUcLKuhZBELBw4UJs2LABO3bsaDFVOWjQICgUCrM+Z2dnIycnx9TnxMREnDhxwuwPVHJyMjQaTYt/aLqrsWPH4sSJE8jIyDA9Bg8ejAceeMD0e/a580aMGNHiUganT59GeHg4AKBnz54ICAgw67NOp8PBgwfN+lxWVoYjR46YxuzYsQNGoxFDhw4V4Vt0DdXV1ZDLzf8pc3BwgNFoBMBeW4OlepqYmIjdu3fDYDCYxiQnJ6Nfv36dOiQFgKeCW8K6desElUolJCUlCSdPnhQeffRRwcPDw+xsErq5xx9/XHB3dxdSU1OFvLw806O6uto05rHHHhPCwsKEHTt2CGlpaUJiYqKQmJhoer35FOUJEyYIGRkZwtatWwVfX1+eonwL158tJQjssyUcOnRIcHR0FF555RXhzJkzwpdffimo1Wrhv//9r2nMa6+9Jnh4eAjff/+9cPz4cWHatGmtnkobHx8vHDx4UNizZ4/Qp0+fbn16cmvmzp0rBAcHm04F/+677wQfHx/hmWeeMY1hr9uvoqJCSE9PF9LT0wUAwttvvy2kp6cLly5dEgTBMj0tKysT/P39hdmzZwuZmZnCunXrBLVazVPBbcl7770nhIWFCUqlUhgyZIhw4MABqUvqUgC0+lizZo1pTE1NjfDEE08Inp6eglqtFn7/+98LeXl5Zvu5ePGiMGnSJMHZ2Vnw8fERnnrqKcFgMIj8bbqW34Yb9tkyfvzxR2HAgAGCSqUSIiMjhY8//tjsdaPRKDz//POCv7+/oFKphLFjxwrZ2dlmY4qLi4VZs2YJrq6ugkajER566CGhoqJCzK9h83Q6nfDkk08KYWFhgpOTk9CrVy/hueeeMzu9mL1uv507d7b6d/LcuXMFQbBcT48dOyaMHDlSUKlUQnBwsPDaa69ZpH6ZIFx3GUciIiKiLo5rboiIiMiuMNwQERGRXWG4ISIiIrvCcENERER2heGGiIiI7ArDDREREdkVhhsiIiKyKww3RNQt9OjRA++8847UZRCRCBhuiMji5s2bh+nTpwMAxowZg0WLFon22UlJSfDw8Gix/fDhw3j00UdFq4OIpOModQFERG1RV1cHpVLZ4ff7+vpasBoismWcuSEiq5k3bx527dqFd999FzKZDDKZDBcvXgQAZGZmYtKkSXB1dYW/vz9mz56NoqIi03vHjBmDhQsXYtGiRfDx8cHEiRMBAG+//TZiYmLg4uKC0NBQPPHEE6isrAQApKam4qGHHkJ5ebnp85YuXQqg5WGpnJwcTJs2Da6urtBoNLjvvvuQn59ven3p0qUYOHAgvvjiC/To0QPu7u744x//iIqKCtOYb7/9FjExMXB2doa3tzfGjRuHqqoqK3WTiNqK4YaIrObdd99FYmIi5s+fj7y8POTl5SE0NBRlZWW48847ER8fj7S0NGzduhX5+fm47777zN7/2WefQalUYu/evfjwww8BAHK5HP/617+QlZWFzz77DDt27MAzzzwDABg+fDjeeecdaDQa0+f99a9/bVGX0WjEtGnTUFJSgl27diE5ORnnz5/HzJkzzcadO3cOGzduxKZNm7Bp0ybs2rULr732GgAgLy8Ps2bNwp/+9Cf8+uuvSE1NxT333APero9IejwsRURW4+7uDqVSCbVajYCAANP2VatWIT4+Hq+++qpp26efforQ0FCcPn0affv2BQD06dMHb7zxhtk+r1+/06NHD7z88st47LHH8MEHH0CpVMLd3R0ymczs834rJSUFJ06cwIULFxAaGgoA+Pzzz9G/f38cPnwYt912G4DGEJSUlAQ3NzcAwOzZs5GSkoJXXnkFeXl5qK+vxz333IPw8HAAQExMTCe6RUSWwpkbIhLdsWPHsHPnTri6upoekZGRABpnS5oNGjSoxXt//vlnjB07FsHBwXBzc8Ps2bNRXFyM6urqNn/+r7/+itDQUFOwAYDo6Gh4eHjg119/NW3r0aOHKdgAQGBgIAoKCgAAcXFxGDt2LGJiYjBjxgz8+9//RmlpadubQERWw3BDRKKrrKzE1KlTkZGRYfY4c+YMRo0aZRrn4uJi9r6LFy/id7/7HWJjY/G///0PR44cwfvvvw+gccGxpSkUCrPnMpkMRqMRAODg4IDk5GRs2bIF0dHReO+999CvXz9cuHDB4nUQUfsw3BCRVSmVSjQ0NJhtS0hIQFZWFnr06IGIiAizx28DzfWOHDkCo9GIFStWYNiwYejbty+uXr16y8/7raioKOTm5iI3N9e07eTJkygrK0N0dHSbv5tMJsOIESOwbNkypKenQ6lUYsOGDW1+PxFZB8MNEVlVjx49cPDgQVy8eBFFRUUwGo1YsGABSkpKMGvWLBw+fBjnzp3Dtm3b8NBDD900mERERMBgMOC9997D+fPn8cUXX5gWGl//eZWVlUhJSUFRUVGrh6vGjRuHmJgYPPDAAzh69CgOHTqEOXPmYPTo0Rg8eHCbvtfBgwfx6quvIi0tDTk5Ofjuu+9QWFiIqKio9jWIiCyO4YaIrOqvf/0rHBwcEB0dDV9fX+Tk5CAoKAh79+5FQ0MDJkyYgJiYGCxatAgeHh6Qy2/811JcXBzefvttvP766xgwYAC+/PJLLF++3GzM8OHD8dhjj2HmzJnw9fVtsSAZaJxx+f777+Hp6YlRo0Zh3Lhx6NWrF77++us2fy+NRoPdu3dj8uTJ6Nu3L/7xj39gxYoVmDRpUtubQ0RWIRN43iIRERHZEc7cEBERkV1huCEiIiK7wnBDREREdoXhhoiIiOwKww0RERHZFYYbIiIisisMN0RERGRXGG6IiIjIrjDcEBERkV1huCEiIiK7wnBDREREdoXhhoiIiOzK/wceakKQUzxSSQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test accuracy: \", test_accuracy)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot the loss curve\n",
    "classifier = pipeline.named_steps['classifier']\n",
    "plt.plot(classifier.loss_curve_)\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
